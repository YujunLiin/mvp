{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a work-through document for environment setup and PPO training/testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check out __GETTING_STARTED.md__ file without clone the repository from the paper github website. Follow the __installation instructions__ part except at step __Clone this repo__ clone our own repository from Gitlab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isaac gym test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After download Isaac Gym and install it, go to folder _isaacgym/python/example_, run the following line of code in __terminal__ to make sure it runs properly. \n",
    "\n",
    "Should be able to see a simulation window pop up and the robots move around with their joints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python joint_monkey.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE environment test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run this line of code in terminal under the directory __project1__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python tools/train_ppo.py task=FrankaPick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible error looks like this: __ImportError: libpython3.7m.so.1.0: cannot open shared object file: No such file or directory__\n",
    "\n",
    "First try run this code in terminal and run the task again:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> sudo apt install libpython3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it doesn't help, do the following to set up environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check if corresponding library is installed in terminal\n",
    "> locate libpython3.7m.so.1.0 \n",
    "\n",
    "There should be a path looks like __/home/usr_name/anaconda3/envs/mvp/lib/libpython3.7m.so.1.0__\n",
    "\n",
    "paste this path and run in terminal:\n",
    "> sudo gedit ~/.bashrc\n",
    "\n",
    "Paste this at the end of the file (remember to change the usr_name):\n",
    "> export LD_LIBRARY_PATH=/home/usr_name/anaconda3/envs/mvp/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "And then save the file and run this code in terminal:\n",
    "> source ~/.bashrc\n",
    "\n",
    "Lastly, try to run the training code again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is mainly from train_ppo.py, for better troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name to enable training with other tasks\n",
    "task_name='FrankaCabinet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_37' (/home/yujun/Documents/TUM/4th_semester/Praktikum/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /home/yujun/Documents/TUM/4th_semester/Praktikum/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yujun/anaconda3/envs/mvp/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.13.1+cu117\n",
      "Device count 1\n",
      "/home/yujun/Documents/TUM/4th_semester/Praktikum/isaacgym/python/isaacgym/_bindings/src/gymtorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/yujun/.cache/torch_extensions/py37_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/yujun/.cache/torch_extensions/py37_cu117/gymtorch/build.ninja...\n",
      "Building extension module gymtorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module gymtorch...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "import omegaconf\n",
    "import os\n",
    "\n",
    "from mvp.utils.hydra_utils import omegaconf_to_dict, print_dict, dump_cfg\n",
    "from mvp.utils.hydra_utils import set_np_formatting, set_seed\n",
    "from mvp.utils.hydra_utils import parse_sim_params, parse_task\n",
    "from mvp.utils.hydra_utils import process_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yujun/Documents/TUM/4th_semester/Praktikum/project1\n",
      "['assets', 'README.md', 'GETTING_STARTED.md', '.git', 'pixmc', 'TASKS.md', 'mvp', 'requirements_ubuntu2204.txt', 'experiment.ipynb', 'requirements.txt', '.gitignore', 'tools', 'pretrained', 'configs', 'mvp.egg-info', 'setup.py', 'ckpts']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yujun/anaconda3/envs/mvp/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  after removing the cwd from sys.path.\n",
      "/home/yujun/anaconda3/envs/mvp/lib/python3.7/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/yujun/anaconda3/envs/mvp/lib/python3.7/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/job_logging:\n",
      "Default list overrides requires 'override' keyword.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.\n",
      "\n",
      "  deprecation_warning(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': {'name': 'FrankaCabinet', 'env': {'numEnvs': 256, 'envSpacing': 1.5, 'episodeLength': 500, 'cabinet_pos_init': [0.05, 0.0, 0.45], 'cabinet_pos_delta': [0.075, 0.075, 0.05], 'numProps': 4, 'obs_type': 'oracle', 'dofVelocityScale': 0.1, 'actionScale': 7.5, 'handleDistRewardScale': 0.06, 'aroundHandleRewardScale': 1.0, 'openBonusRewardScale': 2.0, 'goalDistRewardScale': 6.0, 'openPoseRewardScale': 3.0, 'goalBonusRewardScale': 2.0, 'actionPenaltyScale': 0.01, 'asset': {'assetRoot': 'assets', 'assetFileNameFranka': 'urdf/franka_description/robots/franka_panda.urdf', 'assetFileNameCabinet': 'urdf/sektion_cabinet_model/urdf/sektion_cabinet_2.urdf'}}, 'sim': {'substeps': 1, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 12, 'num_velocity_iterations': 1, 'contact_offset': 0.005, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0, 'always_use_articulations': False}}, 'task': {'randomize': False}}, 'train': {'seed': 0, 'torch_deterministic': False, 'policy': {'pi_hid_sizes': [256, 128, 64], 'vf_hid_sizes': [256, 128, 64]}, 'learn': {'agent_name': 'franka_ppo', 'test': False, 'resume': 0, 'save_interval': 50, 'print_log': True, 'max_iterations': 1000, 'cliprange': 0.1, 'ent_coef': 0, 'nsteps': 32, 'noptepochs': 10, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.001, 'schedule': 'cos', 'gamma': 0.99, 'lam': 0.95, 'init_noise_std': 1.0, 'log_interval': 1}}, 'physics_engine': 'physx', 'pipeline': 'gpu', 'sim_device': 'cuda:0', 'rl_device': 'cuda:0', 'graphics_device_id': 0, 'num_gpus': 1, 'test': False, 'resume': 0, 'logdir': './ckpts/${task.name}', 'cptdir': '', 'headless': True}\n"
     ]
    }
   ],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "GlobalHydra.instance().clear()\n",
    "hydra.initialize(config_path=\"./configs/ppo\")\n",
    "#cfg=hydra.compose(config_name=\"config\",overrides=[\"task=FrankaReach\"])\n",
    "cfg=hydra.compose(config_name=\"config\",overrides=[f\"task={task_name}\"])\n",
    "#cfg=hydra.compose(config_name=\"config\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: \n",
      "    name: FrankaCabinet\n",
      "    env: \n",
      "        numEnvs: 256\n",
      "        envSpacing: 1.5\n",
      "        episodeLength: 500\n",
      "        cabinet_pos_init: [0.05, 0.0, 0.45]\n",
      "        cabinet_pos_delta: [0.075, 0.075, 0.05]\n",
      "        numProps: 4\n",
      "        obs_type: oracle\n",
      "        dofVelocityScale: 0.1\n",
      "        actionScale: 7.5\n",
      "        handleDistRewardScale: 0.06\n",
      "        aroundHandleRewardScale: 1.0\n",
      "        openBonusRewardScale: 2.0\n",
      "        goalDistRewardScale: 6.0\n",
      "        openPoseRewardScale: 3.0\n",
      "        goalBonusRewardScale: 2.0\n",
      "        actionPenaltyScale: 0.01\n",
      "        asset: \n",
      "            assetRoot: assets\n",
      "            assetFileNameFranka: urdf/franka_description/robots/franka_panda.urdf\n",
      "            assetFileNameCabinet: urdf/sektion_cabinet_model/urdf/sektion_cabinet_2.urdf\n",
      "    sim: \n",
      "        substeps: 1\n",
      "        physx: \n",
      "            num_threads: 4\n",
      "            solver_type: 1\n",
      "            num_position_iterations: 12\n",
      "            num_velocity_iterations: 1\n",
      "            contact_offset: 0.005\n",
      "            rest_offset: 0.0\n",
      "            bounce_threshold_velocity: 0.2\n",
      "            max_depenetration_velocity: 1000.0\n",
      "            default_buffer_size_multiplier: 5.0\n",
      "            always_use_articulations: False\n",
      "    task: \n",
      "        randomize: False\n",
      "train: \n",
      "    seed: 0\n",
      "    torch_deterministic: False\n",
      "    policy: \n",
      "        pi_hid_sizes: [256, 128, 64]\n",
      "        vf_hid_sizes: [256, 128, 64]\n",
      "    learn: \n",
      "        agent_name: franka_ppo\n",
      "        test: False\n",
      "        resume: 0\n",
      "        save_interval: 50\n",
      "        print_log: True\n",
      "        max_iterations: 1000\n",
      "        cliprange: 0.1\n",
      "        ent_coef: 0\n",
      "        nsteps: 32\n",
      "        noptepochs: 10\n",
      "        nminibatches: 4\n",
      "        max_grad_norm: 1\n",
      "        optim_stepsize: 0.001\n",
      "        schedule: cos\n",
      "        gamma: 0.99\n",
      "        lam: 0.95\n",
      "        init_noise_std: 1.0\n",
      "        log_interval: 1\n",
      "physics_engine: physx\n",
      "pipeline: gpu\n",
      "sim_device: cuda:0\n",
      "rl_device: cuda:0\n",
      "graphics_device_id: 0\n",
      "num_gpus: 1\n",
      "test: False\n",
      "resume: 0\n",
      "logdir: ./ckpts/FrankaCabinet\n",
      "cptdir: \n",
      "headless: True\n"
     ]
    }
   ],
   "source": [
    "cfg_dict = omegaconf_to_dict(cfg)\n",
    "print_dict(cfg_dict)\n",
    "#print(cfg_dict['task']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote config to: ./ckpts/FrankaCabinet/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create logdir and dump cfg\n",
    "if not cfg.test:\n",
    "    os.makedirs(cfg.logdir, exist_ok=True)\n",
    "    dump_cfg(cfg, cfg.logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed: 0\n",
      "Setting sim options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to PVD\n",
      "+++ Using GPU PhysX\n",
      "Physics Engine: PhysX\n",
      "Physics Device: cuda:0\n",
      "GPU Pipeline: enabled\n",
      "num franka bodies:  11\n",
      "num franka dofs:  9\n",
      "num cabinet bodies:  5\n",
      "num cabinet dofs:  4\n",
      "RL device:  cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yujun/anaconda3/envs/mvp/lib/python3.7/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "# Set up python env\n",
    "set_np_formatting()\n",
    "set_seed(cfg.train.seed, cfg.train.torch_deterministic)\n",
    "\n",
    "# Construct task\n",
    "sim_params = parse_sim_params(cfg, cfg_dict)\n",
    "env = parse_task(cfg, cfg_dict, sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=34, out_features=256, bias=True)\n",
      "  (1): SELU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): SELU()\n",
      "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (5): SELU()\n",
      "  (6): Linear(in_features=64, out_features=9, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=34, out_features=256, bias=True)\n",
      "  (1): SELU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): SELU()\n",
      "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (5): SELU()\n",
      "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 0/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 3834 steps/s (collection: 1.667s, learning 0.470s)\n",
      "               Value function loss: 5.6593\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 0.00\n",
      "               Mean episode length: 0.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.36\n",
      "       Mean episode length/episode: 32.00\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8192\n",
      "                    Iteration time: 2.14s\n",
      "                        Total time: 2.14s\n",
      "                               ETA: 2136.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 1/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 5890 steps/s (collection: 0.822s, learning 0.569s)\n",
      "               Value function loss: 3.8551\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 12.71\n",
      "               Mean episode length: 24.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.25\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 16384\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 3.53s\n",
      "                               ETA: 1761.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 2/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 6443 steps/s (collection: 0.712s, learning 0.559s)\n",
      "               Value function loss: 4.2121\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 12.46\n",
      "               Mean episode length: 31.24\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.21\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 24576\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 4.80s\n",
      "                               ETA: 1596.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 3/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 5694 steps/s (collection: 0.726s, learning 0.712s)\n",
      "               Value function loss: 6.4184\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 13.93\n",
      "               Mean episode length: 39.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.23\n",
      "       Mean episode length/episode: 27.96\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 32768\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 6.24s\n",
      "                               ETA: 1554.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 4/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 5620 steps/s (collection: 0.837s, learning 0.620s)\n",
      "               Value function loss: 7.9984\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 15.92\n",
      "               Mean episode length: 51.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.24\n",
      "       Mean episode length/episode: 27.31\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 40960\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 7.69s\n",
      "                               ETA: 1532.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 5/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 6411 steps/s (collection: 0.746s, learning 0.531s)\n",
      "               Value function loss: 11.4284\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 18.13\n",
      "               Mean episode length: 64.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.29\n",
      "       Mean episode length/episode: 26.77\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 49152\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 8.97s\n",
      "                               ETA: 1487.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 6/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 6352 steps/s (collection: 0.737s, learning 0.552s)\n",
      "               Value function loss: 15.8211\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 22.07\n",
      "               Mean episode length: 82.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.34\n",
      "       Mean episode length/episode: 25.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 57344\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 10.26s\n",
      "                               ETA: 1457.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 7/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 6856 steps/s (collection: 0.706s, learning 0.489s)\n",
      "               Value function loss: 18.9027\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 23.62\n",
      "               Mean episode length: 90.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.38\n",
      "       Mean episode length/episode: 24.31\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 65536\n",
      "                    Iteration time: 1.19s\n",
      "                        Total time: 11.46s\n",
      "                               ETA: 1422.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 8/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 6722 steps/s (collection: 0.707s, learning 0.512s)\n",
      "               Value function loss: 19.1959\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 24.40\n",
      "               Mean episode length: 90.03\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.39\n",
      "       Mean episode length/episode: 25.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 73728\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 12.68s\n",
      "                               ETA: 1397.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 9/1000 \u001b[0m                       \n",
      "\n",
      "                       Computation: 6614 steps/s (collection: 0.725s, learning 0.514s)\n",
      "               Value function loss: 25.9587\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 26.99\n",
      "               Mean episode length: 95.28\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.44\n",
      "       Mean episode length/episode: 25.76\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 81920\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 13.91s\n",
      "                               ETA: 1378.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 10/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6692 steps/s (collection: 0.774s, learning 0.451s)\n",
      "               Value function loss: 26.0865\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 27.57\n",
      "               Mean episode length: 92.67\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.46\n",
      "       Mean episode length/episode: 26.77\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 90112\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 15.14s\n",
      "                               ETA: 1362.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 11/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6175 steps/s (collection: 0.734s, learning 0.593s)\n",
      "               Value function loss: 37.9304\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 32.40\n",
      "               Mean episode length: 97.15\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.50\n",
      "       Mean episode length/episode: 26.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 98304\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 16.46s\n",
      "                               ETA: 1357.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 12/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6893 steps/s (collection: 0.689s, learning 0.499s)\n",
      "               Value function loss: 43.8502\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 33.79\n",
      "               Mean episode length: 97.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.56\n",
      "       Mean episode length/episode: 28.25\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 106496\n",
      "                    Iteration time: 1.19s\n",
      "                        Total time: 17.65s\n",
      "                               ETA: 1341.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 13/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6612 steps/s (collection: 0.747s, learning 0.492s)\n",
      "               Value function loss: 45.9329\n",
      "                    Surrogate loss: -0.0065\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 34.13\n",
      "               Mean episode length: 93.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.55\n",
      "       Mean episode length/episode: 28.25\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 114688\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 18.89s\n",
      "                               ETA: 1331.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 14/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6414 steps/s (collection: 0.696s, learning 0.581s)\n",
      "               Value function loss: 50.7321\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 38.14\n",
      "               Mean episode length: 96.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.59\n",
      "       Mean episode length/episode: 27.40\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 122880\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 20.17s\n",
      "                               ETA: 1325.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 15/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6804 steps/s (collection: 0.691s, learning 0.513s)\n",
      "               Value function loss: 130.9291\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 91.99\n",
      "               Mean episode length: 227.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.63\n",
      "       Mean episode length/episode: 23.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 131072\n",
      "                    Iteration time: 1.20s\n",
      "                        Total time: 21.37s\n",
      "                               ETA: 1315.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 16/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6668 steps/s (collection: 0.733s, learning 0.495s)\n",
      "               Value function loss: 56.9882\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 90.51\n",
      "               Mean episode length: 218.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.66\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 139264\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 22.60s\n",
      "                               ETA: 1308.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 17/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6362 steps/s (collection: 0.728s, learning 0.559s)\n",
      "               Value function loss: 68.5335\n",
      "                    Surrogate loss: -0.0078\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 93.04\n",
      "               Mean episode length: 222.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.71\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 147456\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 23.89s\n",
      "                               ETA: 1304.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 18/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6932 steps/s (collection: 0.706s, learning 0.476s)\n",
      "               Value function loss: 55.7676\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 100.73\n",
      "               Mean episode length: 234.79\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.69\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 155648\n",
      "                    Iteration time: 1.18s\n",
      "                        Total time: 25.07s\n",
      "                               ETA: 1295.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 19/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6444 steps/s (collection: 0.744s, learning 0.527s)\n",
      "               Value function loss: 78.4563\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 103.66\n",
      "               Mean episode length: 231.65\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.74\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 163840\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 26.34s\n",
      "                               ETA: 1292.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 20/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6186 steps/s (collection: 0.827s, learning 0.497s)\n",
      "               Value function loss: 85.5333\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 106.20\n",
      "               Mean episode length: 232.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.81\n",
      "       Mean episode length/episode: 28.85\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 172032\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 27.67s\n",
      "                               ETA: 1291.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 21/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6631 steps/s (collection: 0.722s, learning 0.514s)\n",
      "               Value function loss: 87.6881\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 104.49\n",
      "               Mean episode length: 205.20\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.87\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 180224\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 28.90s\n",
      "                               ETA: 1286.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 22/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6001 steps/s (collection: 0.824s, learning 0.541s)\n",
      "               Value function loss: 104.9051\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 95.36\n",
      "               Mean episode length: 162.34\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.88\n",
      "       Mean episode length/episode: 28.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 188416\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 30.27s\n",
      "                               ETA: 1287.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 23/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6552 steps/s (collection: 0.693s, learning 0.557s)\n",
      "               Value function loss: 119.5531\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 117.41\n",
      "               Mean episode length: 202.09\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.87\n",
      "       Mean episode length/episode: 28.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 196608\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 31.52s\n",
      "                               ETA: 1283.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 24/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 7117 steps/s (collection: 0.716s, learning 0.436s)\n",
      "               Value function loss: 112.5447\n",
      "                    Surrogate loss: 0.0053\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 140.06\n",
      "               Mean episode length: 228.97\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.87\n",
      "       Mean episode length/episode: 28.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 204800\n",
      "                    Iteration time: 1.15s\n",
      "                        Total time: 32.67s\n",
      "                               ETA: 1275.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 25/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6192 steps/s (collection: 0.733s, learning 0.590s)\n",
      "               Value function loss: 120.2996\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 158.18\n",
      "               Mean episode length: 253.60\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.88\n",
      "       Mean episode length/episode: 27.96\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 212992\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 33.99s\n",
      "                               ETA: 1274.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 26/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6479 steps/s (collection: 0.697s, learning 0.568s)\n",
      "               Value function loss: 120.2914\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 180.74\n",
      "               Mean episode length: 273.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.94\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 221184\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 35.25s\n",
      "                               ETA: 1271.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 27/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6347 steps/s (collection: 0.759s, learning 0.532s)\n",
      "               Value function loss: 122.7513\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 185.61\n",
      "               Mean episode length: 276.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.96\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 229376\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 36.55s\n",
      "                               ETA: 1269.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 28/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6447 steps/s (collection: 0.753s, learning 0.517s)\n",
      "               Value function loss: 123.3381\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 190.46\n",
      "               Mean episode length: 275.23\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 0.97\n",
      "       Mean episode length/episode: 28.54\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 237568\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 37.82s\n",
      "                               ETA: 1267.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 29/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 7052 steps/s (collection: 0.698s, learning 0.463s)\n",
      "               Value function loss: 117.5121\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 206.62\n",
      "               Mean episode length: 282.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.05\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 245760\n",
      "                    Iteration time: 1.16s\n",
      "                        Total time: 38.98s\n",
      "                               ETA: 1261.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 30/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6829 steps/s (collection: 0.710s, learning 0.489s)\n",
      "               Value function loss: 140.3071\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 207.25\n",
      "               Mean episode length: 269.72\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.03\n",
      "       Mean episode length/episode: 27.96\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 253952\n",
      "                    Iteration time: 1.20s\n",
      "                        Total time: 40.18s\n",
      "                               ETA: 1257.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 31/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6528 steps/s (collection: 0.717s, learning 0.538s)\n",
      "               Value function loss: 208.0633\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 248.46\n",
      "               Mean episode length: 295.33\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.07\n",
      "       Mean episode length/episode: 26.77\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 262144\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 41.43s\n",
      "                               ETA: 1254.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 32/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6990 steps/s (collection: 0.718s, learning 0.454s)\n",
      "               Value function loss: 175.4927\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 238.46\n",
      "               Mean episode length: 277.10\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.22\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 270336\n",
      "                    Iteration time: 1.17s\n",
      "                        Total time: 42.60s\n",
      "                               ETA: 1249.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 33/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6659 steps/s (collection: 0.688s, learning 0.542s)\n",
      "               Value function loss: 143.4535\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 235.66\n",
      "               Mean episode length: 267.87\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.19\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 278528\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 43.83s\n",
      "                               ETA: 1246.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 34/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6400 steps/s (collection: 0.732s, learning 0.548s)\n",
      "               Value function loss: 215.6334\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 251.22\n",
      "               Mean episode length: 275.08\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.21\n",
      "       Mean episode length/episode: 28.64\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 286720\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 45.11s\n",
      "                               ETA: 1245.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 35/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6672 steps/s (collection: 0.722s, learning 0.506s)\n",
      "               Value function loss: 169.5990\n",
      "                    Surrogate loss: 0.0049\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 261.02\n",
      "               Mean episode length: 278.88\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.20\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 294912\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 46.34s\n",
      "                               ETA: 1242.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 36/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 7162 steps/s (collection: 0.717s, learning 0.427s)\n",
      "               Value function loss: 202.5947\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 266.16\n",
      "               Mean episode length: 280.98\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.23\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 303104\n",
      "                    Iteration time: 1.14s\n",
      "                        Total time: 47.48s\n",
      "                               ETA: 1237.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 37/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6998 steps/s (collection: 0.752s, learning 0.418s)\n",
      "               Value function loss: 219.8914\n",
      "                    Surrogate loss: 0.0031\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 283.48\n",
      "               Mean episode length: 297.36\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.32\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 311296\n",
      "                    Iteration time: 1.17s\n",
      "                        Total time: 48.66s\n",
      "                               ETA: 1233.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 38/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6700 steps/s (collection: 0.727s, learning 0.496s)\n",
      "               Value function loss: 182.7904\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 282.04\n",
      "               Mean episode length: 288.03\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.31\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 319488\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 49.88s\n",
      "                               ETA: 1230.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 39/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6658 steps/s (collection: 0.750s, learning 0.481s)\n",
      "               Value function loss: 231.8479\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 289.81\n",
      "               Mean episode length: 279.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.22\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 327680\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 51.11s\n",
      "                               ETA: 1227.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 40/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6314 steps/s (collection: 0.757s, learning 0.540s)\n",
      "               Value function loss: 249.3100\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 319.16\n",
      "               Mean episode length: 302.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.23\n",
      "       Mean episode length/episode: 28.85\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 335872\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 52.41s\n",
      "                               ETA: 1227.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 41/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6216 steps/s (collection: 0.694s, learning 0.624s)\n",
      "               Value function loss: 217.6704\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 341.84\n",
      "               Mean episode length: 318.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.28\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 344064\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 53.72s\n",
      "                               ETA: 1226.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 42/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6646 steps/s (collection: 0.739s, learning 0.494s)\n",
      "               Value function loss: 202.8286\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 357.28\n",
      "               Mean episode length: 324.05\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.28\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 352256\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 54.96s\n",
      "                               ETA: 1224.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 43/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6623 steps/s (collection: 0.743s, learning 0.494s)\n",
      "               Value function loss: 203.0177\n",
      "                    Surrogate loss: -0.0068\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 378.92\n",
      "               Mean episode length: 332.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.24\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 360448\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 56.19s\n",
      "                               ETA: 1222.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 44/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6805 steps/s (collection: 0.702s, learning 0.502s)\n",
      "               Value function loss: 175.1945\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 390.69\n",
      "               Mean episode length: 339.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.23\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 368640\n",
      "                    Iteration time: 1.20s\n",
      "                        Total time: 57.40s\n",
      "                               ETA: 1219.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 45/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6889 steps/s (collection: 0.695s, learning 0.494s)\n",
      "               Value function loss: 200.9443\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 403.53\n",
      "               Mean episode length: 344.79\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.28\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 376832\n",
      "                    Iteration time: 1.19s\n",
      "                        Total time: 58.59s\n",
      "                               ETA: 1216.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 46/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6767 steps/s (collection: 0.707s, learning 0.503s)\n",
      "               Value function loss: 345.5453\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 442.19\n",
      "               Mean episode length: 365.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.29\n",
      "       Mean episode length/episode: 27.86\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 385024\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 59.80s\n",
      "                               ETA: 1213.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 47/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6748 steps/s (collection: 0.701s, learning 0.513s)\n",
      "               Value function loss: 180.7818\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 441.98\n",
      "               Mean episode length: 365.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.35\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 393216\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 61.01s\n",
      "                               ETA: 1211.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 48/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6684 steps/s (collection: 0.721s, learning 0.504s)\n",
      "               Value function loss: 146.4482\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 445.23\n",
      "               Mean episode length: 368.67\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.39\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 401408\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 62.24s\n",
      "                               ETA: 1209.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 49/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6805 steps/s (collection: 0.733s, learning 0.471s)\n",
      "               Value function loss: 212.5548\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 457.58\n",
      "               Mean episode length: 379.24\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.38\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 409600\n",
      "                    Iteration time: 1.20s\n",
      "                        Total time: 63.44s\n",
      "                               ETA: 1206.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 50/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6117 steps/s (collection: 0.848s, learning 0.491s)\n",
      "               Value function loss: 193.6330\n",
      "                    Surrogate loss: 0.0091\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 455.79\n",
      "               Mean episode length: 374.35\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.40\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 417792\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 64.78s\n",
      "                               ETA: 1206.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 51/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6388 steps/s (collection: 0.745s, learning 0.537s)\n",
      "               Value function loss: 204.9694\n",
      "                    Surrogate loss: 0.0130\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 483.56\n",
      "               Mean episode length: 389.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.43\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 425984\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 66.06s\n",
      "                               ETA: 1205.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 52/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6402 steps/s (collection: 0.732s, learning 0.548s)\n",
      "               Value function loss: 213.7871\n",
      "                    Surrogate loss: 0.0046\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 495.51\n",
      "               Mean episode length: 399.51\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.48\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 434176\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 67.34s\n",
      "                               ETA: 1204.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 53/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6379 steps/s (collection: 0.728s, learning 0.556s)\n",
      "               Value function loss: 216.6909\n",
      "                    Surrogate loss: -0.0025\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 507.60\n",
      "               Mean episode length: 406.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.47\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 442368\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 68.62s\n",
      "                               ETA: 1203.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 54/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6870 steps/s (collection: 0.722s, learning 0.470s)\n",
      "               Value function loss: 229.8591\n",
      "                    Surrogate loss: 0.0007\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 503.18\n",
      "               Mean episode length: 400.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.42\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 450560\n",
      "                    Iteration time: 1.19s\n",
      "                        Total time: 69.82s\n",
      "                               ETA: 1200.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 55/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6712 steps/s (collection: 0.716s, learning 0.504s)\n",
      "               Value function loss: 281.6512\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 516.56\n",
      "               Mean episode length: 400.49\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.49\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 458752\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 71.04s\n",
      "                               ETA: 1198.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 56/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6344 steps/s (collection: 0.734s, learning 0.558s)\n",
      "               Value function loss: 257.3090\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 504.96\n",
      "               Mean episode length: 393.11\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.56\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 466944\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 72.33s\n",
      "                               ETA: 1197.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 57/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6862 steps/s (collection: 0.707s, learning 0.486s)\n",
      "               Value function loss: 247.5724\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 501.08\n",
      "               Mean episode length: 382.51\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.56\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 475136\n",
      "                    Iteration time: 1.19s\n",
      "                        Total time: 73.52s\n",
      "                               ETA: 1195.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 58/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6792 steps/s (collection: 0.706s, learning 0.500s)\n",
      "               Value function loss: 277.2098\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 517.14\n",
      "               Mean episode length: 383.20\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.60\n",
      "       Mean episode length/episode: 28.85\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 483328\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 74.73s\n",
      "                               ETA: 1193.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 59/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6671 steps/s (collection: 0.721s, learning 0.507s)\n",
      "               Value function loss: 292.4709\n",
      "                    Surrogate loss: -0.0019\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 524.75\n",
      "               Mean episode length: 382.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.65\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 491520\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 75.96s\n",
      "                               ETA: 1191.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 60/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6017 steps/s (collection: 0.745s, learning 0.616s)\n",
      "               Value function loss: 250.1437\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 519.63\n",
      "               Mean episode length: 378.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.64\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 499712\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 77.32s\n",
      "                               ETA: 1191.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 61/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6929 steps/s (collection: 0.735s, learning 0.448s)\n",
      "               Value function loss: 318.2087\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 531.94\n",
      "               Mean episode length: 376.18\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.60\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 507904\n",
      "                    Iteration time: 1.18s\n",
      "                        Total time: 78.50s\n",
      "                               ETA: 1188.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 62/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6034 steps/s (collection: 0.771s, learning 0.586s)\n",
      "               Value function loss: 474.2446\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 559.93\n",
      "               Mean episode length: 383.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.56\n",
      "       Mean episode length/episode: 27.86\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 516096\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 79.86s\n",
      "                               ETA: 1189.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 63/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6737 steps/s (collection: 0.706s, learning 0.510s)\n",
      "               Value function loss: 189.9903\n",
      "                    Surrogate loss: 0.0037\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 577.16\n",
      "               Mean episode length: 391.78\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.58\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 524288\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 81.07s\n",
      "                               ETA: 1187.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 64/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6716 steps/s (collection: 0.685s, learning 0.534s)\n",
      "               Value function loss: 217.6976\n",
      "                    Surrogate loss: 0.0002\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 586.47\n",
      "               Mean episode length: 397.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.63\n",
      "       Mean episode length/episode: 31.27\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 532480\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 82.29s\n",
      "                               ETA: 1185.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 65/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6713 steps/s (collection: 0.724s, learning 0.496s)\n",
      "               Value function loss: 319.8434\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 594.69\n",
      "               Mean episode length: 401.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.67\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 540672\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 83.51s\n",
      "                               ETA: 1183.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 66/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6616 steps/s (collection: 0.694s, learning 0.544s)\n",
      "               Value function loss: 291.2605\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 604.51\n",
      "               Mean episode length: 404.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.65\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 548864\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 84.75s\n",
      "                               ETA: 1181.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 67/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6796 steps/s (collection: 0.706s, learning 0.499s)\n",
      "               Value function loss: 261.1785\n",
      "                    Surrogate loss: -0.0019\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 620.96\n",
      "               Mean episode length: 410.68\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.62\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 557056\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 85.96s\n",
      "                               ETA: 1179.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 68/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6584 steps/s (collection: 0.742s, learning 0.502s)\n",
      "               Value function loss: 331.5221\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 654.35\n",
      "               Mean episode length: 425.56\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.70\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 565248\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 87.20s\n",
      "                               ETA: 1177.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 69/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6718 steps/s (collection: 0.744s, learning 0.476s)\n",
      "               Value function loss: 368.6162\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 664.91\n",
      "               Mean episode length: 428.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.73\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 573440\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 88.42s\n",
      "                               ETA: 1176.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 70/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6187 steps/s (collection: 0.751s, learning 0.573s)\n",
      "               Value function loss: 367.8218\n",
      "                    Surrogate loss: -0.0011\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 673.78\n",
      "               Mean episode length: 430.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.76\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 581632\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 89.74s\n",
      "                               ETA: 1175.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 71/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6389 steps/s (collection: 0.755s, learning 0.527s)\n",
      "               Value function loss: 352.9478\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 684.64\n",
      "               Mean episode length: 438.79\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.75\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 589824\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 91.03s\n",
      "                               ETA: 1174.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 72/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6181 steps/s (collection: 0.787s, learning 0.539s)\n",
      "               Value function loss: 364.8101\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 687.31\n",
      "               Mean episode length: 440.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.76\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 598016\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 92.35s\n",
      "                               ETA: 1174.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 73/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6972 steps/s (collection: 0.719s, learning 0.456s)\n",
      "               Value function loss: 449.8030\n",
      "                    Surrogate loss: 0.0035\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 667.64\n",
      "               Mean episode length: 424.05\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.76\n",
      "       Mean episode length/episode: 28.74\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 606208\n",
      "                    Iteration time: 1.17s\n",
      "                        Total time: 93.53s\n",
      "                               ETA: 1171.6s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 74/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6764 steps/s (collection: 0.696s, learning 0.515s)\n",
      "               Value function loss: 461.4131\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 688.09\n",
      "               Mean episode length: 430.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.69\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 614400\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 94.74s\n",
      "                               ETA: 1169.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 75/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6692 steps/s (collection: 0.759s, learning 0.465s)\n",
      "               Value function loss: 286.2611\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 689.49\n",
      "               Mean episode length: 430.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.74\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 622592\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 95.96s\n",
      "                               ETA: 1167.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 76/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6627 steps/s (collection: 0.737s, learning 0.499s)\n",
      "               Value function loss: 322.6119\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 687.15\n",
      "               Mean episode length: 425.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.78\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 630784\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 97.20s\n",
      "                               ETA: 1166.4s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 77/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6450 steps/s (collection: 0.780s, learning 0.490s)\n",
      "               Value function loss: 338.1169\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 700.27\n",
      "               Mean episode length: 428.71\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.78\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 638976\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 98.47s\n",
      "                               ETA: 1165.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 78/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6580 steps/s (collection: 0.743s, learning 0.502s)\n",
      "               Value function loss: 443.4078\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 710.75\n",
      "               Mean episode length: 432.60\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.83\n",
      "       Mean episode length/episode: 28.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 647168\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 99.71s\n",
      "                               ETA: 1163.7s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 79/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6663 steps/s (collection: 0.729s, learning 0.500s)\n",
      "               Value function loss: 245.7879\n",
      "                    Surrogate loss: 0.0018\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 710.29\n",
      "               Mean episode length: 432.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.96\n",
      "       Mean episode length/episode: 31.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 655360\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 100.94s\n",
      "                               ETA: 1162.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 80/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6853 steps/s (collection: 0.733s, learning 0.462s)\n",
      "               Value function loss: 353.1768\n",
      "                    Surrogate loss: 0.0021\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 723.45\n",
      "               Mean episode length: 437.96\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.96\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 663552\n",
      "                    Iteration time: 1.20s\n",
      "                        Total time: 102.14s\n",
      "                               ETA: 1160.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 81/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5987 steps/s (collection: 0.784s, learning 0.585s)\n",
      "               Value function loss: 422.6999\n",
      "                    Surrogate loss: 0.0005\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 715.36\n",
      "               Mean episode length: 428.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.95\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 671744\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 103.51s\n",
      "                               ETA: 1160.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 82/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6397 steps/s (collection: 0.759s, learning 0.521s)\n",
      "               Value function loss: 433.9866\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 712.76\n",
      "               Mean episode length: 420.52\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.93\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 679936\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 104.79s\n",
      "                               ETA: 1159.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 83/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5891 steps/s (collection: 0.778s, learning 0.613s)\n",
      "               Value function loss: 417.0364\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 724.36\n",
      "               Mean episode length: 422.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.98\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 688128\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 106.18s\n",
      "                               ETA: 1159.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 84/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6727 steps/s (collection: 0.716s, learning 0.502s)\n",
      "               Value function loss: 428.8165\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 740.17\n",
      "               Mean episode length: 426.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.87\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 696320\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 107.39s\n",
      "                               ETA: 1157.3s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 85/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5615 steps/s (collection: 0.790s, learning 0.669s)\n",
      "               Value function loss: 452.4319\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 745.61\n",
      "               Mean episode length: 422.16\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.87\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 704512\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 108.85s\n",
      "                               ETA: 1158.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 86/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5507 steps/s (collection: 0.794s, learning 0.694s)\n",
      "               Value function loss: 452.4793\n",
      "                    Surrogate loss: 0.0105\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 763.18\n",
      "               Mean episode length: 420.33\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.84\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 712704\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 110.34s\n",
      "                               ETA: 1159.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 87/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6395 steps/s (collection: 0.762s, learning 0.519s)\n",
      "               Value function loss: 426.4023\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 766.55\n",
      "               Mean episode length: 417.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.84\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 720896\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 111.62s\n",
      "                               ETA: 1158.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 88/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6147 steps/s (collection: 0.767s, learning 0.565s)\n",
      "               Value function loss: 346.9053\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 770.12\n",
      "               Mean episode length: 417.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.91\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 729088\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 112.95s\n",
      "                               ETA: 1157.5s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 89/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6539 steps/s (collection: 0.721s, learning 0.531s)\n",
      "               Value function loss: 500.6841\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 767.65\n",
      "               Mean episode length: 413.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.88\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 737280\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 114.21s\n",
      "                               ETA: 1156.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 90/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6425 steps/s (collection: 0.737s, learning 0.538s)\n",
      "               Value function loss: 484.9949\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 763.88\n",
      "               Mean episode length: 407.60\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.90\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 745472\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 115.48s\n",
      "                               ETA: 1154.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 91/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6218 steps/s (collection: 0.699s, learning 0.618s)\n",
      "               Value function loss: 347.7896\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 785.82\n",
      "               Mean episode length: 415.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.91\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 753664\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 116.80s\n",
      "                               ETA: 1154.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 92/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5959 steps/s (collection: 0.769s, learning 0.605s)\n",
      "               Value function loss: 478.5359\n",
      "                    Surrogate loss: 0.0105\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 784.72\n",
      "               Mean episode length: 415.11\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.97\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 761856\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 118.17s\n",
      "                               ETA: 1153.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 93/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6645 steps/s (collection: 0.744s, learning 0.488s)\n",
      "               Value function loss: 702.3571\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 814.17\n",
      "               Mean episode length: 423.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.99\n",
      "       Mean episode length/episode: 28.74\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 770048\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 119.41s\n",
      "                               ETA: 1152.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 94/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6376 steps/s (collection: 0.726s, learning 0.559s)\n",
      "               Value function loss: 294.0468\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 820.42\n",
      "               Mean episode length: 425.83\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.99\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 778240\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 120.69s\n",
      "                               ETA: 1151.0s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 95/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6944 steps/s (collection: 0.726s, learning 0.454s)\n",
      "               Value function loss: 333.5989\n",
      "                    Surrogate loss: 0.0007\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 830.59\n",
      "               Mean episode length: 432.22\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.98\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 786432\n",
      "                    Iteration time: 1.18s\n",
      "                        Total time: 121.87s\n",
      "                               ETA: 1148.9s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 96/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6731 steps/s (collection: 0.718s, learning 0.499s)\n",
      "               Value function loss: 515.2182\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 818.41\n",
      "               Mean episode length: 427.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.97\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 794624\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 123.09s\n",
      "                               ETA: 1147.1s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 97/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6279 steps/s (collection: 0.755s, learning 0.550s)\n",
      "               Value function loss: 467.2745\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 827.33\n",
      "               Mean episode length: 433.69\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.94\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 802816\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 124.39s\n",
      "                               ETA: 1146.2s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 98/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6508 steps/s (collection: 0.734s, learning 0.524s)\n",
      "               Value function loss: 400.0298\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 806.71\n",
      "               Mean episode length: 425.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.91\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 811008\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 125.65s\n",
      "                               ETA: 1144.8s\n",
      "\n",
      "################################################################################\n",
      "                      \u001b[1m Learning iteration 99/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6101 steps/s (collection: 0.758s, learning 0.585s)\n",
      "               Value function loss: 491.3097\n",
      "                    Surrogate loss: 0.0027\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 805.52\n",
      "               Mean episode length: 424.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.96\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 819200\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 126.99s\n",
      "                               ETA: 1144.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 100/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6328 steps/s (collection: 0.758s, learning 0.537s)\n",
      "               Value function loss: 465.3695\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 811.89\n",
      "               Mean episode length: 424.52\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.92\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 827392\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 128.29s\n",
      "                               ETA: 1143.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 101/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5921 steps/s (collection: 0.772s, learning 0.611s)\n",
      "               Value function loss: 418.7289\n",
      "                    Surrogate loss: 0.0112\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 809.22\n",
      "               Mean episode length: 420.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.03\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 835584\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 129.67s\n",
      "                               ETA: 1142.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 102/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6355 steps/s (collection: 0.756s, learning 0.533s)\n",
      "               Value function loss: 433.9137\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 825.17\n",
      "               Mean episode length: 424.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.03\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 843776\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 130.96s\n",
      "                               ETA: 1141.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 103/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6580 steps/s (collection: 0.704s, learning 0.541s)\n",
      "               Value function loss: 419.7829\n",
      "                    Surrogate loss: 0.0006\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 841.78\n",
      "               Mean episode length: 431.40\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.00\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 851968\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 132.20s\n",
      "                               ETA: 1140.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 104/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6473 steps/s (collection: 0.709s, learning 0.556s)\n",
      "               Value function loss: 470.1044\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 841.27\n",
      "               Mean episode length: 432.95\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.97\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 860160\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 133.47s\n",
      "                               ETA: 1138.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 105/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6084 steps/s (collection: 0.751s, learning 0.596s)\n",
      "               Value function loss: 457.2998\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 814.86\n",
      "               Mean episode length: 423.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.98\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 868352\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 134.82s\n",
      "                               ETA: 1138.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 106/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6286 steps/s (collection: 0.751s, learning 0.552s)\n",
      "               Value function loss: 490.8824\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 805.88\n",
      "               Mean episode length: 418.77\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.95\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 876544\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 136.12s\n",
      "                               ETA: 1137.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 107/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6520 steps/s (collection: 0.720s, learning 0.536s)\n",
      "               Value function loss: 438.5341\n",
      "                    Surrogate loss: 0.0288\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 782.98\n",
      "               Mean episode length: 407.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.96\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 884736\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 137.38s\n",
      "                               ETA: 1135.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 108/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6442 steps/s (collection: 0.744s, learning 0.527s)\n",
      "               Value function loss: 497.2030\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 784.32\n",
      "               Mean episode length: 400.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.91\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 892928\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 138.65s\n",
      "                               ETA: 1134.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 109/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6031 steps/s (collection: 0.749s, learning 0.609s)\n",
      "               Value function loss: 848.7093\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 787.58\n",
      "               Mean episode length: 401.57\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.87\n",
      "       Mean episode length/episode: 27.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 901120\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 140.01s\n",
      "                               ETA: 1134.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 110/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6064 steps/s (collection: 0.755s, learning 0.595s)\n",
      "               Value function loss: 315.8592\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 793.00\n",
      "               Mean episode length: 404.96\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.94\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 909312\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 141.36s\n",
      "                               ETA: 1133.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 111/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6555 steps/s (collection: 0.730s, learning 0.520s)\n",
      "               Value function loss: 354.5444\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 791.11\n",
      "               Mean episode length: 404.96\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.00\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 917504\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 142.61s\n",
      "                               ETA: 1131.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 112/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5816 steps/s (collection: 0.843s, learning 0.565s)\n",
      "               Value function loss: 536.6494\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 803.57\n",
      "               Mean episode length: 410.78\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.99\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 925696\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 144.01s\n",
      "                               ETA: 1131.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 113/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6497 steps/s (collection: 0.760s, learning 0.501s)\n",
      "               Value function loss: 460.7909\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 802.55\n",
      "               Mean episode length: 410.78\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.99\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 933888\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 145.28s\n",
      "                               ETA: 1130.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 114/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5723 steps/s (collection: 0.868s, learning 0.564s)\n",
      "               Value function loss: 436.0667\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 797.92\n",
      "               Mean episode length: 405.24\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.03\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 942080\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 146.71s\n",
      "                               ETA: 1130.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 115/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6239 steps/s (collection: 0.787s, learning 0.525s)\n",
      "               Value function loss: 465.6484\n",
      "                    Surrogate loss: 0.0174\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 795.07\n",
      "               Mean episode length: 403.23\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.06\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 950272\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 148.02s\n",
      "                               ETA: 1129.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 116/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6268 steps/s (collection: 0.740s, learning 0.567s)\n",
      "               Value function loss: 496.9679\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 825.88\n",
      "               Mean episode length: 420.20\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.05\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 958464\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 149.33s\n",
      "                               ETA: 1128.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 117/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5843 steps/s (collection: 0.734s, learning 0.668s)\n",
      "               Value function loss: 407.8446\n",
      "                    Surrogate loss: 0.0105\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 827.54\n",
      "               Mean episode length: 421.40\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.06\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 966656\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 150.73s\n",
      "                               ETA: 1127.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 118/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6038 steps/s (collection: 0.795s, learning 0.562s)\n",
      "               Value function loss: 480.5750\n",
      "                    Surrogate loss: 0.0122\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 852.01\n",
      "               Mean episode length: 430.58\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.01\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 974848\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 152.08s\n",
      "                               ETA: 1127.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 119/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6531 steps/s (collection: 0.713s, learning 0.541s)\n",
      "               Value function loss: 444.8136\n",
      "                    Surrogate loss: 0.0000\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 866.78\n",
      "               Mean episode length: 438.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.03\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 983040\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 153.34s\n",
      "                               ETA: 1125.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 120/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6340 steps/s (collection: 0.731s, learning 0.562s)\n",
      "               Value function loss: 721.9295\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 828.44\n",
      "               Mean episode length: 425.02\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.98\n",
      "       Mean episode length/episode: 28.44\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 991232\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 154.63s\n",
      "                               ETA: 1124.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 121/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6146 steps/s (collection: 0.762s, learning 0.571s)\n",
      "               Value function loss: 567.0298\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 833.15\n",
      "               Mean episode length: 424.61\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.98\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 999424\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 155.96s\n",
      "                               ETA: 1123.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 122/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6042 steps/s (collection: 0.743s, learning 0.613s)\n",
      "               Value function loss: 359.1930\n",
      "                    Surrogate loss: 0.0002\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 816.53\n",
      "               Mean episode length: 414.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.00\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1007616\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 157.32s\n",
      "                               ETA: 1123.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 123/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6127 steps/s (collection: 0.756s, learning 0.582s)\n",
      "               Value function loss: 581.3795\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 788.25\n",
      "               Mean episode length: 401.39\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.02\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1015808\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 158.66s\n",
      "                               ETA: 1122.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 124/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6083 steps/s (collection: 0.786s, learning 0.560s)\n",
      "               Value function loss: 734.4318\n",
      "                    Surrogate loss: 0.0014\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 814.32\n",
      "               Mean episode length: 411.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.03\n",
      "       Mean episode length/episode: 28.54\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1024000\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 160.00s\n",
      "                               ETA: 1121.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 125/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6831 steps/s (collection: 0.749s, learning 0.450s)\n",
      "               Value function loss: 595.8673\n",
      "                    Surrogate loss: 0.0083\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 809.01\n",
      "               Mean episode length: 405.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.10\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1032192\n",
      "                    Iteration time: 1.20s\n",
      "                        Total time: 161.20s\n",
      "                               ETA: 1119.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 126/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6510 steps/s (collection: 0.754s, learning 0.504s)\n",
      "               Value function loss: 350.2045\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 793.22\n",
      "               Mean episode length: 397.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.08\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1040384\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 162.46s\n",
      "                               ETA: 1118.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 127/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6477 steps/s (collection: 0.710s, learning 0.555s)\n",
      "               Value function loss: 370.4131\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 778.23\n",
      "               Mean episode length: 396.18\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.00\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1048576\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 163.73s\n",
      "                               ETA: 1116.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 128/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5050 steps/s (collection: 0.739s, learning 0.883s)\n",
      "               Value function loss: 430.6052\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 790.44\n",
      "               Mean episode length: 400.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.96\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1056768\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 165.35s\n",
      "                               ETA: 1117.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 129/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5959 steps/s (collection: 0.743s, learning 0.631s)\n",
      "               Value function loss: 402.9179\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 787.05\n",
      "               Mean episode length: 397.90\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.93\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1064960\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 166.72s\n",
      "                               ETA: 1117.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 130/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6612 steps/s (collection: 0.731s, learning 0.507s)\n",
      "               Value function loss: 384.7884\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 797.75\n",
      "               Mean episode length: 401.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 1.98\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1073152\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 167.96s\n",
      "                               ETA: 1115.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 131/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6667 steps/s (collection: 0.717s, learning 0.512s)\n",
      "               Value function loss: 529.9592\n",
      "                    Surrogate loss: 0.0078\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 815.96\n",
      "               Mean episode length: 405.49\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.07\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1081344\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 169.19s\n",
      "                               ETA: 1113.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 132/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6060 steps/s (collection: 0.778s, learning 0.574s)\n",
      "               Value function loss: 410.6361\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 821.10\n",
      "               Mean episode length: 408.14\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.07\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1089536\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 170.54s\n",
      "                               ETA: 1113.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 133/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5477 steps/s (collection: 0.811s, learning 0.684s)\n",
      "               Value function loss: 533.8960\n",
      "                    Surrogate loss: 0.0022\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 833.59\n",
      "               Mean episode length: 415.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.04\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1097728\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 172.04s\n",
      "                               ETA: 1113.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 134/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6575 steps/s (collection: 0.741s, learning 0.505s)\n",
      "               Value function loss: 533.4094\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 832.16\n",
      "               Mean episode length: 414.71\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.08\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1105920\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 173.28s\n",
      "                               ETA: 1111.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 135/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5759 steps/s (collection: 0.828s, learning 0.594s)\n",
      "               Value function loss: 520.0958\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 848.63\n",
      "               Mean episode length: 419.71\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.05\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1114112\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 174.70s\n",
      "                               ETA: 1111.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 136/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6788 steps/s (collection: 0.738s, learning 0.469s)\n",
      "               Value function loss: 741.5548\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 853.61\n",
      "               Mean episode length: 419.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.01\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1122304\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 175.91s\n",
      "                               ETA: 1109.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 137/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6263 steps/s (collection: 0.738s, learning 0.570s)\n",
      "               Value function loss: 632.7583\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 869.81\n",
      "               Mean episode length: 426.21\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.07\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1130496\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 177.22s\n",
      "                               ETA: 1108.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 138/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5474 steps/s (collection: 0.890s, learning 0.607s)\n",
      "               Value function loss: 496.5934\n",
      "                    Surrogate loss: -0.0010\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 882.29\n",
      "               Mean episode length: 428.49\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.16\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1138688\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 178.72s\n",
      "                               ETA: 1108.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 139/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5946 steps/s (collection: 0.776s, learning 0.602s)\n",
      "               Value function loss: 746.2791\n",
      "                    Surrogate loss: -0.0072\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 888.50\n",
      "               Mean episode length: 431.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.14\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1146880\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 180.09s\n",
      "                               ETA: 1107.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 140/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6557 steps/s (collection: 0.749s, learning 0.500s)\n",
      "               Value function loss: 1040.8126\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 922.49\n",
      "               Mean episode length: 449.01\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.16\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1155072\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 181.34s\n",
      "                               ETA: 1106.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 141/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6761 steps/s (collection: 0.733s, learning 0.479s)\n",
      "               Value function loss: 405.9837\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 910.70\n",
      "               Mean episode length: 448.08\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.11\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1163264\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 182.55s\n",
      "                               ETA: 1104.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 142/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6400 steps/s (collection: 0.737s, learning 0.543s)\n",
      "               Value function loss: 404.7086\n",
      "                    Surrogate loss: -0.0010\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 909.24\n",
      "               Mean episode length: 447.29\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.16\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1171456\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 183.83s\n",
      "                               ETA: 1103.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 143/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6149 steps/s (collection: 0.800s, learning 0.532s)\n",
      "               Value function loss: 553.0901\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 907.91\n",
      "               Mean episode length: 446.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.21\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1179648\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 185.17s\n",
      "                               ETA: 1102.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 144/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6268 steps/s (collection: 0.721s, learning 0.586s)\n",
      "               Value function loss: 502.8483\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 927.90\n",
      "               Mean episode length: 454.63\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.27\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1187840\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 186.47s\n",
      "                               ETA: 1100.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 145/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6209 steps/s (collection: 0.777s, learning 0.542s)\n",
      "               Value function loss: 579.4483\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 954.16\n",
      "               Mean episode length: 464.01\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.28\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1196032\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 187.79s\n",
      "                               ETA: 1099.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 146/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6581 steps/s (collection: 0.741s, learning 0.504s)\n",
      "               Value function loss: 536.0723\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 945.11\n",
      "               Mean episode length: 461.04\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.35\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1204224\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 189.04s\n",
      "                               ETA: 1098.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 147/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6537 steps/s (collection: 0.717s, learning 0.536s)\n",
      "               Value function loss: 488.0891\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 941.80\n",
      "               Mean episode length: 461.04\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.41\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1212416\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 190.29s\n",
      "                               ETA: 1096.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 148/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5903 steps/s (collection: 0.769s, learning 0.619s)\n",
      "               Value function loss: 590.9236\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 950.50\n",
      "               Mean episode length: 461.04\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.44\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1220608\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 191.68s\n",
      "                               ETA: 1096.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 149/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6439 steps/s (collection: 0.769s, learning 0.504s)\n",
      "               Value function loss: 877.9240\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 977.58\n",
      "               Mean episode length: 468.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.52\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1228800\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 192.95s\n",
      "                               ETA: 1094.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 150/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6252 steps/s (collection: 0.723s, learning 0.587s)\n",
      "               Value function loss: 697.1234\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 989.21\n",
      "               Mean episode length: 469.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.59\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1236992\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 194.26s\n",
      "                               ETA: 1093.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 151/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6042 steps/s (collection: 0.779s, learning 0.577s)\n",
      "               Value function loss: 618.7099\n",
      "                    Surrogate loss: 0.0014\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1003.37\n",
      "               Mean episode length: 473.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.59\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1245184\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 195.62s\n",
      "                               ETA: 1092.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 152/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6749 steps/s (collection: 0.722s, learning 0.491s)\n",
      "               Value function loss: 748.2729\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1021.72\n",
      "               Mean episode length: 473.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.59\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1253376\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 196.83s\n",
      "                               ETA: 1090.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 153/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6360 steps/s (collection: 0.778s, learning 0.510s)\n",
      "               Value function loss: 621.1977\n",
      "                    Surrogate loss: 0.0168\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1035.21\n",
      "               Mean episode length: 473.69\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.52\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1261568\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 198.12s\n",
      "                               ETA: 1089.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 154/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6151 steps/s (collection: 0.733s, learning 0.599s)\n",
      "               Value function loss: 637.9507\n",
      "                    Surrogate loss: 0.0058\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1067.63\n",
      "               Mean episode length: 479.56\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.45\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1269760\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 199.45s\n",
      "                               ETA: 1088.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 155/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5909 steps/s (collection: 0.738s, learning 0.648s)\n",
      "               Value function loss: 726.0701\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1086.63\n",
      "               Mean episode length: 481.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.42\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1277952\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 200.84s\n",
      "                               ETA: 1087.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 156/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6485 steps/s (collection: 0.747s, learning 0.516s)\n",
      "               Value function loss: 1178.4085\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1092.88\n",
      "               Mean episode length: 473.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.44\n",
      "       Mean episode length/episode: 27.77\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1286144\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 202.10s\n",
      "                               ETA: 1086.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 157/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6716 steps/s (collection: 0.734s, learning 0.486s)\n",
      "               Value function loss: 661.8384\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1082.59\n",
      "               Mean episode length: 462.69\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.34\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1294336\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 203.32s\n",
      "                               ETA: 1084.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 158/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5775 steps/s (collection: 0.868s, learning 0.551s)\n",
      "               Value function loss: 428.8884\n",
      "                    Surrogate loss: -0.0012\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1084.68\n",
      "               Mean episode length: 461.95\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.37\n",
      "       Mean episode length/episode: 31.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1302528\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 204.74s\n",
      "                               ETA: 1084.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 159/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6130 steps/s (collection: 0.782s, learning 0.555s)\n",
      "               Value function loss: 638.2723\n",
      "                    Surrogate loss: 0.0172\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1104.99\n",
      "               Mean episode length: 465.64\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.38\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1310720\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 206.07s\n",
      "                               ETA: 1083.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 160/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6526 steps/s (collection: 0.758s, learning 0.497s)\n",
      "               Value function loss: 512.2441\n",
      "                    Surrogate loss: 0.0176\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1100.79\n",
      "               Mean episode length: 461.63\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.56\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1318912\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 207.33s\n",
      "                               ETA: 1081.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 161/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6063 steps/s (collection: 0.756s, learning 0.595s)\n",
      "               Value function loss: 791.6566\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1107.21\n",
      "               Mean episode length: 460.14\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.56\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1327104\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 208.68s\n",
      "                               ETA: 1080.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 162/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6117 steps/s (collection: 0.715s, learning 0.624s)\n",
      "               Value function loss: 720.5430\n",
      "                    Surrogate loss: -0.0028\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1119.56\n",
      "               Mean episode length: 460.14\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.46\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1335296\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 210.02s\n",
      "                               ETA: 1079.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 163/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6213 steps/s (collection: 0.738s, learning 0.581s)\n",
      "               Value function loss: 672.3278\n",
      "                    Surrogate loss: 0.0011\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1117.21\n",
      "               Mean episode length: 455.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.44\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1343488\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 211.34s\n",
      "                               ETA: 1078.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 164/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5788 steps/s (collection: 0.769s, learning 0.646s)\n",
      "               Value function loss: 802.7010\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1109.70\n",
      "               Mean episode length: 451.71\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.43\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1351680\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 212.75s\n",
      "                               ETA: 1077.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 165/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6050 steps/s (collection: 0.758s, learning 0.596s)\n",
      "               Value function loss: 697.4046\n",
      "                    Surrogate loss: 0.0168\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1113.78\n",
      "               Mean episode length: 450.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.39\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1359872\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 214.11s\n",
      "                               ETA: 1077.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 166/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6481 steps/s (collection: 0.736s, learning 0.528s)\n",
      "               Value function loss: 519.2886\n",
      "                    Surrogate loss: 0.0030\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1114.93\n",
      "               Mean episode length: 450.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.41\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1368064\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 215.37s\n",
      "                               ETA: 1075.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 167/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6022 steps/s (collection: 0.784s, learning 0.576s)\n",
      "               Value function loss: 677.6583\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1128.35\n",
      "               Mean episode length: 453.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.39\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1376256\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 216.73s\n",
      "                               ETA: 1074.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 168/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6116 steps/s (collection: 0.759s, learning 0.581s)\n",
      "               Value function loss: 759.8964\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1130.58\n",
      "               Mean episode length: 456.79\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.45\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1384448\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 218.07s\n",
      "                               ETA: 1073.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 169/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6760 steps/s (collection: 0.711s, learning 0.501s)\n",
      "               Value function loss: 732.0723\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1150.66\n",
      "               Mean episode length: 463.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.46\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1392640\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 219.28s\n",
      "                               ETA: 1071.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 170/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6134 steps/s (collection: 0.781s, learning 0.555s)\n",
      "               Value function loss: 923.1455\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1156.40\n",
      "               Mean episode length: 468.07\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.49\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1400832\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 220.62s\n",
      "                               ETA: 1070.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 171/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5249 steps/s (collection: 0.822s, learning 0.739s)\n",
      "               Value function loss: 1055.3014\n",
      "                    Surrogate loss: -0.0024\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1163.18\n",
      "               Mean episode length: 470.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.48\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1409024\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 222.18s\n",
      "                               ETA: 1070.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 172/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6010 steps/s (collection: 0.755s, learning 0.608s)\n",
      "               Value function loss: 880.7708\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1157.07\n",
      "               Mean episode length: 468.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.40\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1417216\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 223.54s\n",
      "                               ETA: 1069.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 173/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6341 steps/s (collection: 0.739s, learning 0.553s)\n",
      "               Value function loss: 816.2628\n",
      "                    Surrogate loss: -0.0074\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1155.86\n",
      "               Mean episode length: 470.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.50\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1425408\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 224.83s\n",
      "                               ETA: 1068.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 174/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6460 steps/s (collection: 0.755s, learning 0.513s)\n",
      "               Value function loss: 661.4554\n",
      "                    Surrogate loss: 0.0159\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1161.73\n",
      "               Mean episode length: 473.02\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.52\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1433600\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 226.10s\n",
      "                               ETA: 1067.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 175/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6099 steps/s (collection: 0.719s, learning 0.624s)\n",
      "               Value function loss: 651.2754\n",
      "                    Surrogate loss: 0.0123\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1160.97\n",
      "               Mean episode length: 473.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.47\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1441792\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 227.44s\n",
      "                               ETA: 1066.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 176/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6524 steps/s (collection: 0.766s, learning 0.490s)\n",
      "               Value function loss: 701.3145\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1161.35\n",
      "               Mean episode length: 474.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.46\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1449984\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 228.70s\n",
      "                               ETA: 1064.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 177/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5528 steps/s (collection: 0.803s, learning 0.679s)\n",
      "               Value function loss: 687.2003\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1146.39\n",
      "               Mean episode length: 469.56\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.38\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1458176\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 230.18s\n",
      "                               ETA: 1064.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 178/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6390 steps/s (collection: 0.745s, learning 0.536s)\n",
      "               Value function loss: 739.5317\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1147.01\n",
      "               Mean episode length: 469.56\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.39\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1466368\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 231.46s\n",
      "                               ETA: 1062.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 179/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6259 steps/s (collection: 0.733s, learning 0.575s)\n",
      "               Value function loss: 748.2075\n",
      "                    Surrogate loss: 0.0094\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1140.26\n",
      "               Mean episode length: 466.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.36\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1474560\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 232.77s\n",
      "                               ETA: 1061.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 180/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5891 steps/s (collection: 0.718s, learning 0.672s)\n",
      "               Value function loss: 787.3970\n",
      "                    Surrogate loss: 0.0012\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1150.36\n",
      "               Mean episode length: 466.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.41\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1482752\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 234.16s\n",
      "                               ETA: 1060.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 181/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6106 steps/s (collection: 0.754s, learning 0.588s)\n",
      "               Value function loss: 679.7525\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1160.58\n",
      "               Mean episode length: 473.68\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.44\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1490944\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 235.50s\n",
      "                               ETA: 1059.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 182/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6514 steps/s (collection: 0.744s, learning 0.514s)\n",
      "               Value function loss: 657.4886\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1145.24\n",
      "               Mean episode length: 467.33\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.43\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1499136\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 236.76s\n",
      "                               ETA: 1058.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 183/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6048 steps/s (collection: 0.817s, learning 0.537s)\n",
      "               Value function loss: 622.5355\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1135.06\n",
      "               Mean episode length: 466.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.47\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1507328\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 238.11s\n",
      "                               ETA: 1057.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 184/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6029 steps/s (collection: 0.713s, learning 0.645s)\n",
      "               Value function loss: 852.3608\n",
      "                    Surrogate loss: 0.0008\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1124.77\n",
      "               Mean episode length: 464.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.52\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1515520\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 239.47s\n",
      "                               ETA: 1056.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 185/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6009 steps/s (collection: 0.753s, learning 0.610s)\n",
      "               Value function loss: 694.0909\n",
      "                    Surrogate loss: 0.0018\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1098.73\n",
      "               Mean episode length: 452.36\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.56\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1523712\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 240.84s\n",
      "                               ETA: 1055.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 186/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6351 steps/s (collection: 0.725s, learning 0.564s)\n",
      "               Value function loss: 1139.4644\n",
      "                    Surrogate loss: 0.0137\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1109.75\n",
      "               Mean episode length: 453.55\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.46\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1531904\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 242.13s\n",
      "                               ETA: 1054.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 187/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6613 steps/s (collection: 0.760s, learning 0.479s)\n",
      "               Value function loss: 1575.1620\n",
      "                    Surrogate loss: 0.0004\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1136.48\n",
      "               Mean episode length: 462.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.44\n",
      "       Mean episode length/episode: 28.54\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1540096\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 243.37s\n",
      "                               ETA: 1052.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 188/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5631 steps/s (collection: 0.788s, learning 0.667s)\n",
      "               Value function loss: 616.9350\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1133.06\n",
      "               Mean episode length: 462.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.46\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1548288\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 244.82s\n",
      "                               ETA: 1051.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 189/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6134 steps/s (collection: 0.711s, learning 0.624s)\n",
      "               Value function loss: 485.2678\n",
      "                    Surrogate loss: 0.0297\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1129.98\n",
      "               Mean episode length: 462.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.54\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1556480\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 246.15s\n",
      "                               ETA: 1050.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 190/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5796 steps/s (collection: 0.788s, learning 0.625s)\n",
      "               Value function loss: 804.4204\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1131.21\n",
      "               Mean episode length: 463.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.53\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1564672\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 247.57s\n",
      "                               ETA: 1049.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 191/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6000 steps/s (collection: 0.782s, learning 0.583s)\n",
      "               Value function loss: 543.0683\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1113.57\n",
      "               Mean episode length: 459.05\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.51\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1572864\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 248.93s\n",
      "                               ETA: 1048.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 192/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6432 steps/s (collection: 0.720s, learning 0.553s)\n",
      "               Value function loss: 983.7320\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1116.40\n",
      "               Mean episode length: 456.96\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.51\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1581056\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 250.21s\n",
      "                               ETA: 1047.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 193/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6688 steps/s (collection: 0.730s, learning 0.495s)\n",
      "               Value function loss: 842.2075\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1128.75\n",
      "               Mean episode length: 463.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.54\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1589248\n",
      "                    Iteration time: 1.22s\n",
      "                        Total time: 251.43s\n",
      "                               ETA: 1045.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 194/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5817 steps/s (collection: 0.806s, learning 0.603s)\n",
      "               Value function loss: 816.3713\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1126.87\n",
      "               Mean episode length: 462.01\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.59\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1597440\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 252.84s\n",
      "                               ETA: 1045.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 195/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5913 steps/s (collection: 0.781s, learning 0.604s)\n",
      "               Value function loss: 648.1551\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1144.46\n",
      "               Mean episode length: 469.18\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.64\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1605632\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 254.23s\n",
      "                               ETA: 1044.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 196/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6518 steps/s (collection: 0.788s, learning 0.469s)\n",
      "               Value function loss: 750.2287\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1172.50\n",
      "               Mean episode length: 479.21\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.61\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1613824\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 255.48s\n",
      "                               ETA: 1042.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 197/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6126 steps/s (collection: 0.726s, learning 0.611s)\n",
      "               Value function loss: 716.4597\n",
      "                    Surrogate loss: 0.0100\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1180.45\n",
      "               Mean episode length: 482.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.59\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1622016\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 256.82s\n",
      "                               ETA: 1041.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 198/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6178 steps/s (collection: 0.755s, learning 0.571s)\n",
      "               Value function loss: 960.2179\n",
      "                    Surrogate loss: 0.0001\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1165.86\n",
      "               Mean episode length: 475.49\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.58\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1630208\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 258.15s\n",
      "                               ETA: 1040.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 199/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6140 steps/s (collection: 0.746s, learning 0.588s)\n",
      "               Value function loss: 950.9344\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1154.95\n",
      "               Mean episode length: 471.45\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.63\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1638400\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 259.48s\n",
      "                               ETA: 1039.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 200/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5518 steps/s (collection: 0.769s, learning 0.716s)\n",
      "               Value function loss: 788.7896\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1136.54\n",
      "               Mean episode length: 463.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1646592\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 260.96s\n",
      "                               ETA: 1038.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 201/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5931 steps/s (collection: 0.720s, learning 0.661s)\n",
      "               Value function loss: 1007.5887\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1140.89\n",
      "               Mean episode length: 460.04\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.75\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1654784\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 262.34s\n",
      "                               ETA: 1037.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 202/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6187 steps/s (collection: 0.755s, learning 0.569s)\n",
      "               Value function loss: 755.8317\n",
      "                    Surrogate loss: 0.0007\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1167.20\n",
      "               Mean episode length: 466.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.76\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1662976\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 263.67s\n",
      "                               ETA: 1036.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 203/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6568 steps/s (collection: 0.777s, learning 0.470s)\n",
      "               Value function loss: 1168.8381\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1191.85\n",
      "               Mean episode length: 469.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.69\n",
      "       Mean episode length/episode: 28.74\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1671168\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 264.92s\n",
      "                               ETA: 1035.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 204/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6343 steps/s (collection: 0.747s, learning 0.545s)\n",
      "               Value function loss: 889.4375\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1198.38\n",
      "               Mean episode length: 469.11\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.86\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1679360\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 266.21s\n",
      "                               ETA: 1033.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 205/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5831 steps/s (collection: 0.700s, learning 0.704s)\n",
      "               Value function loss: 380.8825\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1207.87\n",
      "               Mean episode length: 470.55\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.90\n",
      "       Mean episode length/episode: 31.27\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1687552\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 267.61s\n",
      "                               ETA: 1032.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 206/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5802 steps/s (collection: 0.841s, learning 0.571s)\n",
      "               Value function loss: 642.9819\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1223.98\n",
      "               Mean episode length: 471.64\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.83\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1695744\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 269.02s\n",
      "                               ETA: 1031.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 207/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5943 steps/s (collection: 0.798s, learning 0.580s)\n",
      "               Value function loss: 702.5016\n",
      "                    Surrogate loss: 0.0004\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1221.30\n",
      "               Mean episode length: 468.74\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.81\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1703936\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 270.40s\n",
      "                               ETA: 1030.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 208/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6211 steps/s (collection: 0.747s, learning 0.572s)\n",
      "               Value function loss: 773.4211\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1228.69\n",
      "               Mean episode length: 468.74\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.80\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1712128\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 271.72s\n",
      "                               ETA: 1029.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 209/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6164 steps/s (collection: 0.737s, learning 0.591s)\n",
      "               Value function loss: 835.1509\n",
      "                    Surrogate loss: 0.0167\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1235.58\n",
      "               Mean episode length: 468.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.93\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1720320\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 273.05s\n",
      "                               ETA: 1028.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 210/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6577 steps/s (collection: 0.781s, learning 0.464s)\n",
      "               Value function loss: 1008.8461\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1264.26\n",
      "               Mean episode length: 475.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.90\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1728512\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 274.30s\n",
      "                               ETA: 1027.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 211/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6438 steps/s (collection: 0.696s, learning 0.576s)\n",
      "               Value function loss: 1051.4517\n",
      "                    Surrogate loss: 0.0111\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1276.80\n",
      "               Mean episode length: 475.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.84\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1736704\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 275.57s\n",
      "                               ETA: 1025.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 212/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6132 steps/s (collection: 0.757s, learning 0.579s)\n",
      "               Value function loss: 689.3811\n",
      "                    Surrogate loss: -0.0025\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1305.65\n",
      "               Mean episode length: 483.26\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.87\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1744896\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 276.90s\n",
      "                               ETA: 1024.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 213/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5913 steps/s (collection: 0.752s, learning 0.633s)\n",
      "               Value function loss: 690.0859\n",
      "                    Surrogate loss: 0.0080\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1310.58\n",
      "               Mean episode length: 483.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.93\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1753088\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 278.29s\n",
      "                               ETA: 1023.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 214/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6414 steps/s (collection: 0.709s, learning 0.568s)\n",
      "               Value function loss: 1187.9932\n",
      "                    Surrogate loss: 0.0054\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1328.43\n",
      "               Mean episode length: 486.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.86\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1761280\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 279.57s\n",
      "                               ETA: 1022.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 215/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6747 steps/s (collection: 0.728s, learning 0.486s)\n",
      "               Value function loss: 1142.4440\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1307.92\n",
      "               Mean episode length: 476.14\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.77\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1769472\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 280.78s\n",
      "                               ETA: 1020.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 216/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5362 steps/s (collection: 0.756s, learning 0.772s)\n",
      "               Value function loss: 878.3996\n",
      "                    Surrogate loss: -0.0071\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1296.38\n",
      "               Mean episode length: 468.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.84\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1777664\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 282.31s\n",
      "                               ETA: 1020.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 217/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6039 steps/s (collection: 0.779s, learning 0.577s)\n",
      "               Value function loss: 1116.9188\n",
      "                    Surrogate loss: 0.0045\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1301.89\n",
      "               Mean episode length: 467.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.83\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1785856\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 283.66s\n",
      "                               ETA: 1018.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 218/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6202 steps/s (collection: 0.768s, learning 0.553s)\n",
      "               Value function loss: 1369.2174\n",
      "                    Surrogate loss: 0.0038\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1323.40\n",
      "               Mean episode length: 472.40\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.81\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1794048\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 284.98s\n",
      "                               ETA: 1017.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 219/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6015 steps/s (collection: 0.782s, learning 0.580s)\n",
      "               Value function loss: 762.1850\n",
      "                    Surrogate loss: -0.0025\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1320.54\n",
      "               Mean episode length: 469.29\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.71\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1802240\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 286.35s\n",
      "                               ETA: 1016.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 220/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5933 steps/s (collection: 0.749s, learning 0.632s)\n",
      "               Value function loss: 762.1768\n",
      "                    Surrogate loss: 0.0071\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1324.05\n",
      "               Mean episode length: 469.27\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1810432\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 287.73s\n",
      "                               ETA: 1015.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 221/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6440 steps/s (collection: 0.743s, learning 0.530s)\n",
      "               Value function loss: 778.8970\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1297.69\n",
      "               Mean episode length: 461.15\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.81\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1818624\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 289.00s\n",
      "                               ETA: 1014.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 222/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5729 steps/s (collection: 0.739s, learning 0.690s)\n",
      "               Value function loss: 859.4093\n",
      "                    Surrogate loss: 0.0096\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1285.33\n",
      "               Mean episode length: 454.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.81\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1826816\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 290.43s\n",
      "                               ETA: 1013.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 223/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6210 steps/s (collection: 0.757s, learning 0.562s)\n",
      "               Value function loss: 1094.4017\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1299.04\n",
      "               Mean episode length: 454.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.85\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1835008\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 291.75s\n",
      "                               ETA: 1012.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 224/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5581 steps/s (collection: 0.784s, learning 0.684s)\n",
      "               Value function loss: 890.1741\n",
      "                    Surrogate loss: -0.0025\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1223.92\n",
      "               Mean episode length: 431.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.83\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1843200\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 293.22s\n",
      "                               ETA: 1011.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 225/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5734 steps/s (collection: 0.920s, learning 0.508s)\n",
      "               Value function loss: 648.5992\n",
      "                    Surrogate loss: 0.0038\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1212.29\n",
      "               Mean episode length: 428.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.83\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1851392\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 294.64s\n",
      "                               ETA: 1010.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 226/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5871 steps/s (collection: 0.788s, learning 0.607s)\n",
      "               Value function loss: 927.2004\n",
      "                    Surrogate loss: 0.0002\n",
      "             Mean action noise std: 0.94\n",
      "                       Mean reward: 1211.77\n",
      "               Mean episode length: 428.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.78\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1859584\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 296.04s\n",
      "                               ETA: 1009.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 227/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5626 steps/s (collection: 0.778s, learning 0.679s)\n",
      "               Value function loss: 997.5601\n",
      "                    Surrogate loss: -0.0065\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1210.52\n",
      "               Mean episode length: 427.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1867776\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 297.50s\n",
      "                               ETA: 1008.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 228/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6166 steps/s (collection: 0.805s, learning 0.524s)\n",
      "               Value function loss: 723.3019\n",
      "                    Surrogate loss: -0.0065\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1204.32\n",
      "               Mean episode length: 426.15\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.74\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1875968\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 298.82s\n",
      "                               ETA: 1007.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 229/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5692 steps/s (collection: 0.774s, learning 0.666s)\n",
      "               Value function loss: 905.8917\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1182.61\n",
      "               Mean episode length: 420.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.75\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1884160\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 300.26s\n",
      "                               ETA: 1006.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 230/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6169 steps/s (collection: 0.807s, learning 0.521s)\n",
      "               Value function loss: 1109.4049\n",
      "                    Surrogate loss: 0.0235\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1173.33\n",
      "               Mean episode length: 417.24\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.76\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1892352\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 301.59s\n",
      "                               ETA: 1005.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 231/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5951 steps/s (collection: 0.751s, learning 0.626s)\n",
      "               Value function loss: 1064.9000\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1176.12\n",
      "               Mean episode length: 415.56\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.73\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1900544\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 302.97s\n",
      "                               ETA: 1004.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 232/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5942 steps/s (collection: 0.755s, learning 0.623s)\n",
      "               Value function loss: 736.8315\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 0.94\n",
      "                       Mean reward: 1200.36\n",
      "               Mean episode length: 422.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.74\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1908736\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 304.35s\n",
      "                               ETA: 1003.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 233/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5678 steps/s (collection: 0.801s, learning 0.642s)\n",
      "               Value function loss: 973.0521\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1183.75\n",
      "               Mean episode length: 420.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1916928\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 305.79s\n",
      "                               ETA: 1002.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 234/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5809 steps/s (collection: 0.911s, learning 0.499s)\n",
      "               Value function loss: 1446.3955\n",
      "                    Surrogate loss: -0.0010\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1248.53\n",
      "               Mean episode length: 441.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.69\n",
      "       Mean episode length/episode: 28.74\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1925120\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 307.20s\n",
      "                               ETA: 1001.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 235/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5809 steps/s (collection: 0.801s, learning 0.609s)\n",
      "               Value function loss: 673.9133\n",
      "                    Surrogate loss: 0.0117\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1275.59\n",
      "               Mean episode length: 451.10\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.69\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1933312\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 308.61s\n",
      "                               ETA: 1000.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 236/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6477 steps/s (collection: 0.762s, learning 0.502s)\n",
      "               Value function loss: 489.3896\n",
      "                    Surrogate loss: 0.0011\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1273.94\n",
      "               Mean episode length: 451.72\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.64\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1941504\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 309.87s\n",
      "                               ETA: 998.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 237/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6213 steps/s (collection: 0.751s, learning 0.568s)\n",
      "               Value function loss: 848.6896\n",
      "                    Surrogate loss: -0.0012\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1283.81\n",
      "               Mean episode length: 457.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.67\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1949696\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 311.19s\n",
      "                               ETA: 997.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 238/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6384 steps/s (collection: 0.767s, learning 0.516s)\n",
      "               Value function loss: 912.6617\n",
      "                    Surrogate loss: 0.0022\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1257.84\n",
      "               Mean episode length: 452.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.65\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1957888\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 312.47s\n",
      "                               ETA: 996.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 239/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6860 steps/s (collection: 0.724s, learning 0.470s)\n",
      "               Value function loss: 1150.6447\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1277.94\n",
      "               Mean episode length: 457.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.61\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1966080\n",
      "                    Iteration time: 1.19s\n",
      "                        Total time: 313.67s\n",
      "                               ETA: 994.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 240/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6052 steps/s (collection: 0.731s, learning 0.622s)\n",
      "               Value function loss: 695.1176\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1283.88\n",
      "               Mean episode length: 460.74\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.66\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1974272\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 315.02s\n",
      "                               ETA: 993.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 241/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5167 steps/s (collection: 0.779s, learning 0.806s)\n",
      "               Value function loss: 746.5777\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1294.29\n",
      "               Mean episode length: 464.58\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1982464\n",
      "                    Iteration time: 1.59s\n",
      "                        Total time: 316.61s\n",
      "                               ETA: 993.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 242/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5879 steps/s (collection: 0.794s, learning 0.599s)\n",
      "               Value function loss: 781.5263\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1261.66\n",
      "               Mean episode length: 457.59\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.67\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1990656\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 318.00s\n",
      "                               ETA: 992.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 243/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6360 steps/s (collection: 0.734s, learning 0.554s)\n",
      "               Value function loss: 714.6230\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1276.62\n",
      "               Mean episode length: 464.49\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.67\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 1998848\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 319.29s\n",
      "                               ETA: 990.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 244/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5978 steps/s (collection: 0.785s, learning 0.586s)\n",
      "               Value function loss: 760.3347\n",
      "                    Surrogate loss: 0.0020\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1270.67\n",
      "               Mean episode length: 464.49\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.70\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2007040\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 320.66s\n",
      "                               ETA: 989.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 245/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6062 steps/s (collection: 0.772s, learning 0.579s)\n",
      "               Value function loss: 992.2241\n",
      "                    Surrogate loss: -0.0023\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1276.60\n",
      "               Mean episode length: 469.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.73\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2015232\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 322.01s\n",
      "                               ETA: 988.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 246/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5379 steps/s (collection: 0.937s, learning 0.586s)\n",
      "               Value function loss: 1280.0668\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1247.26\n",
      "               Mean episode length: 462.58\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.65\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2023424\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 323.53s\n",
      "                               ETA: 987.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 247/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5748 steps/s (collection: 0.807s, learning 0.619s)\n",
      "               Value function loss: 838.8000\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1237.29\n",
      "               Mean episode length: 460.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.66\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2031616\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 324.96s\n",
      "                               ETA: 986.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 248/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5598 steps/s (collection: 0.850s, learning 0.613s)\n",
      "               Value function loss: 728.5784\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1207.94\n",
      "               Mean episode length: 450.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.70\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2039808\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 326.42s\n",
      "                               ETA: 985.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 249/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5492 steps/s (collection: 0.794s, learning 0.697s)\n",
      "               Value function loss: 1053.5680\n",
      "                    Surrogate loss: 0.0026\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1226.16\n",
      "               Mean episode length: 463.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2048000\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 327.91s\n",
      "                               ETA: 985.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 250/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5534 steps/s (collection: 0.895s, learning 0.586s)\n",
      "               Value function loss: 761.4419\n",
      "                    Surrogate loss: 0.0067\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1203.01\n",
      "               Mean episode length: 455.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.77\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2056192\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 329.39s\n",
      "                               ETA: 984.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 251/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6330 steps/s (collection: 0.785s, learning 0.509s)\n",
      "               Value function loss: 805.6107\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1206.24\n",
      "               Mean episode length: 455.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.85\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2064384\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 330.69s\n",
      "                               ETA: 982.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 252/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6059 steps/s (collection: 0.771s, learning 0.581s)\n",
      "               Value function loss: 419.7362\n",
      "                    Surrogate loss: 0.0119\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1206.75\n",
      "               Mean episode length: 455.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.88\n",
      "       Mean episode length/episode: 31.27\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2072576\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 332.04s\n",
      "                               ETA: 981.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 253/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5900 steps/s (collection: 0.760s, learning 0.629s)\n",
      "               Value function loss: 941.6952\n",
      "                    Surrogate loss: -0.0019\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1224.01\n",
      "               Mean episode length: 457.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.85\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2080768\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 333.43s\n",
      "                               ETA: 980.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 254/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5651 steps/s (collection: 0.753s, learning 0.697s)\n",
      "               Value function loss: 733.9719\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1229.43\n",
      "               Mean episode length: 457.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.85\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2088960\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 334.88s\n",
      "                               ETA: 979.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 255/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6100 steps/s (collection: 0.787s, learning 0.555s)\n",
      "               Value function loss: 1001.3283\n",
      "                    Surrogate loss: -0.0012\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1210.75\n",
      "               Mean episode length: 449.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.71\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2097152\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 336.22s\n",
      "                               ETA: 978.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 256/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5497 steps/s (collection: 0.831s, learning 0.659s)\n",
      "               Value function loss: 638.6751\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1199.09\n",
      "               Mean episode length: 444.04\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.64\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2105344\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 337.71s\n",
      "                               ETA: 977.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 257/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6061 steps/s (collection: 0.755s, learning 0.597s)\n",
      "               Value function loss: 947.4106\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1196.84\n",
      "               Mean episode length: 440.49\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.64\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2113536\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 339.06s\n",
      "                               ETA: 976.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 258/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5616 steps/s (collection: 0.737s, learning 0.721s)\n",
      "               Value function loss: 724.7088\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1199.23\n",
      "               Mean episode length: 441.49\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.60\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2121728\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 340.52s\n",
      "                               ETA: 975.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 259/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5812 steps/s (collection: 0.775s, learning 0.635s)\n",
      "               Value function loss: 587.5872\n",
      "                    Surrogate loss: 0.0078\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1201.82\n",
      "               Mean episode length: 443.04\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.59\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2129920\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 341.93s\n",
      "                               ETA: 974.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 260/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6136 steps/s (collection: 0.754s, learning 0.581s)\n",
      "               Value function loss: 571.6250\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1209.71\n",
      "               Mean episode length: 440.83\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.60\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2138112\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 343.26s\n",
      "                               ETA: 973.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 261/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5354 steps/s (collection: 0.784s, learning 0.746s)\n",
      "               Value function loss: 1132.6367\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1233.94\n",
      "               Mean episode length: 445.51\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.56\n",
      "       Mean episode length/episode: 28.85\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2146304\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 344.79s\n",
      "                               ETA: 972.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 262/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5672 steps/s (collection: 0.822s, learning 0.623s)\n",
      "               Value function loss: 651.8615\n",
      "                    Surrogate loss: -0.0023\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1208.22\n",
      "               Mean episode length: 441.21\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.53\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2154496\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 346.24s\n",
      "                               ETA: 971.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 263/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6486 steps/s (collection: 0.773s, learning 0.490s)\n",
      "               Value function loss: 779.9848\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1211.90\n",
      "               Mean episode length: 443.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.56\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2162688\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 347.50s\n",
      "                               ETA: 970.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 264/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5691 steps/s (collection: 0.804s, learning 0.636s)\n",
      "               Value function loss: 807.6763\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1200.43\n",
      "               Mean episode length: 441.52\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.57\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2170880\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 348.94s\n",
      "                               ETA: 969.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 265/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5814 steps/s (collection: 0.795s, learning 0.613s)\n",
      "               Value function loss: 1119.9203\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1213.12\n",
      "               Mean episode length: 446.13\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.58\n",
      "       Mean episode length/episode: 28.85\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2179072\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 350.35s\n",
      "                               ETA: 968.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 266/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5883 steps/s (collection: 0.765s, learning 0.627s)\n",
      "               Value function loss: 442.9187\n",
      "                    Surrogate loss: 0.0020\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1186.09\n",
      "               Mean episode length: 441.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.61\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2187264\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 351.74s\n",
      "                               ETA: 967.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 267/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6149 steps/s (collection: 0.749s, learning 0.583s)\n",
      "               Value function loss: 451.2718\n",
      "                    Surrogate loss: 0.0013\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1167.96\n",
      "               Mean episode length: 438.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.67\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2195456\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 353.07s\n",
      "                               ETA: 965.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 268/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6203 steps/s (collection: 0.753s, learning 0.567s)\n",
      "               Value function loss: 613.7375\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1158.56\n",
      "               Mean episode length: 436.68\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.77\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2203648\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 354.39s\n",
      "                               ETA: 964.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 269/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5284 steps/s (collection: 0.787s, learning 0.763s)\n",
      "               Value function loss: 821.7158\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1114.95\n",
      "               Mean episode length: 421.40\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.73\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2211840\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 355.94s\n",
      "                               ETA: 963.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 270/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5941 steps/s (collection: 0.781s, learning 0.598s)\n",
      "               Value function loss: 892.0736\n",
      "                    Surrogate loss: 0.0048\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1111.70\n",
      "               Mean episode length: 425.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.69\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2220032\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 357.32s\n",
      "                               ETA: 962.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 271/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6428 steps/s (collection: 0.802s, learning 0.472s)\n",
      "               Value function loss: 663.3412\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1105.67\n",
      "               Mean episode length: 423.11\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.68\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2228224\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 358.60s\n",
      "                               ETA: 961.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 272/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5840 steps/s (collection: 0.755s, learning 0.648s)\n",
      "               Value function loss: 420.9665\n",
      "                    Surrogate loss: 0.0040\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1110.87\n",
      "               Mean episode length: 424.91\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.76\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2236416\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 360.00s\n",
      "                               ETA: 960.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 273/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5569 steps/s (collection: 0.793s, learning 0.678s)\n",
      "               Value function loss: 820.7502\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1123.10\n",
      "               Mean episode length: 430.26\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2244608\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 361.47s\n",
      "                               ETA: 959.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 274/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5755 steps/s (collection: 0.802s, learning 0.621s)\n",
      "               Value function loss: 610.8253\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1109.71\n",
      "               Mean episode length: 425.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.74\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2252800\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 362.89s\n",
      "                               ETA: 958.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 275/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5814 steps/s (collection: 0.803s, learning 0.606s)\n",
      "               Value function loss: 688.4493\n",
      "                    Surrogate loss: 0.0208\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1119.24\n",
      "               Mean episode length: 429.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2260992\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 364.30s\n",
      "                               ETA: 957.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 276/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5970 steps/s (collection: 0.739s, learning 0.633s)\n",
      "               Value function loss: 558.7517\n",
      "                    Surrogate loss: 0.0126\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1132.28\n",
      "               Mean episode length: 433.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.74\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2269184\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 365.68s\n",
      "                               ETA: 955.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 277/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4929 steps/s (collection: 0.863s, learning 0.799s)\n",
      "               Value function loss: 1149.7765\n",
      "                    Surrogate loss: 0.0022\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1150.85\n",
      "               Mean episode length: 440.46\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.65\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2277376\n",
      "                    Iteration time: 1.66s\n",
      "                        Total time: 367.34s\n",
      "                               ETA: 955.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 278/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5998 steps/s (collection: 0.772s, learning 0.594s)\n",
      "               Value function loss: 570.0448\n",
      "                    Surrogate loss: 0.0106\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1181.74\n",
      "               Mean episode length: 449.96\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.74\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2285568\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 368.70s\n",
      "                               ETA: 954.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 279/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5654 steps/s (collection: 0.820s, learning 0.628s)\n",
      "               Value function loss: 699.7239\n",
      "                    Surrogate loss: -0.0024\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1207.11\n",
      "               Mean episode length: 454.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.73\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2293760\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 370.15s\n",
      "                               ETA: 953.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 280/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5957 steps/s (collection: 0.787s, learning 0.588s)\n",
      "               Value function loss: 804.2160\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1216.08\n",
      "               Mean episode length: 456.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.76\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2301952\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 371.53s\n",
      "                               ETA: 952.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 281/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6015 steps/s (collection: 0.757s, learning 0.604s)\n",
      "               Value function loss: 1207.8265\n",
      "                    Surrogate loss: 0.0018\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1257.69\n",
      "               Mean episode length: 466.88\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.71\n",
      "       Mean episode length/episode: 28.85\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2310144\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 372.89s\n",
      "                               ETA: 950.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 282/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6227 steps/s (collection: 0.757s, learning 0.559s)\n",
      "               Value function loss: 611.9168\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1273.57\n",
      "               Mean episode length: 471.36\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2318336\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 374.20s\n",
      "                               ETA: 949.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 283/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6088 steps/s (collection: 0.748s, learning 0.598s)\n",
      "               Value function loss: 343.2805\n",
      "                    Surrogate loss: -0.0009\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1248.80\n",
      "               Mean episode length: 462.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.81\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2326528\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 375.55s\n",
      "                               ETA: 948.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 284/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5809 steps/s (collection: 0.786s, learning 0.624s)\n",
      "               Value function loss: 798.1234\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 0.95\n",
      "                       Mean reward: 1250.54\n",
      "               Mean episode length: 460.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2334720\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 376.96s\n",
      "                               ETA: 947.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 285/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6263 steps/s (collection: 0.791s, learning 0.517s)\n",
      "               Value function loss: 787.5670\n",
      "                    Surrogate loss: -0.0013\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1261.75\n",
      "               Mean episode length: 465.23\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.75\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2342912\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 378.27s\n",
      "                               ETA: 945.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 286/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5563 steps/s (collection: 0.886s, learning 0.586s)\n",
      "               Value function loss: 865.0503\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1264.77\n",
      "               Mean episode length: 462.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.73\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2351104\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 379.74s\n",
      "                               ETA: 944.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 287/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6220 steps/s (collection: 0.796s, learning 0.521s)\n",
      "               Value function loss: 699.4347\n",
      "                    Surrogate loss: 0.0014\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1265.78\n",
      "               Mean episode length: 460.95\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.76\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2359296\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 381.06s\n",
      "                               ETA: 943.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 288/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6416 steps/s (collection: 0.755s, learning 0.522s)\n",
      "               Value function loss: 586.2862\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1268.75\n",
      "               Mean episode length: 460.95\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.77\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2367488\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 382.33s\n",
      "                               ETA: 941.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 289/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6204 steps/s (collection: 0.809s, learning 0.512s)\n",
      "               Value function loss: 833.6984\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1265.15\n",
      "               Mean episode length: 457.69\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.75\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2375680\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 383.65s\n",
      "                               ETA: 940.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 290/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5680 steps/s (collection: 0.778s, learning 0.664s)\n",
      "               Value function loss: 550.9566\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1265.23\n",
      "               Mean episode length: 458.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.78\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2383872\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 385.10s\n",
      "                               ETA: 939.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 291/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5608 steps/s (collection: 0.775s, learning 0.685s)\n",
      "               Value function loss: 897.6410\n",
      "                    Surrogate loss: 0.0021\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1291.29\n",
      "               Mean episode length: 463.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.78\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2392064\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 386.56s\n",
      "                               ETA: 938.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 292/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6206 steps/s (collection: 0.764s, learning 0.555s)\n",
      "               Value function loss: 944.1723\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1303.55\n",
      "               Mean episode length: 470.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.80\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2400256\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 387.88s\n",
      "                               ETA: 937.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 293/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5602 steps/s (collection: 0.743s, learning 0.719s)\n",
      "               Value function loss: 717.0486\n",
      "                    Surrogate loss: 0.0136\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1312.52\n",
      "               Mean episode length: 472.64\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.78\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2408448\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 389.34s\n",
      "                               ETA: 936.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 294/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6326 steps/s (collection: 0.802s, learning 0.493s)\n",
      "               Value function loss: 784.4695\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1312.55\n",
      "               Mean episode length: 472.14\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.81\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2416640\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 390.63s\n",
      "                               ETA: 934.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 295/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5996 steps/s (collection: 0.757s, learning 0.609s)\n",
      "               Value function loss: 835.2892\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1328.77\n",
      "               Mean episode length: 477.18\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2424832\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 392.00s\n",
      "                               ETA: 933.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 296/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5612 steps/s (collection: 0.783s, learning 0.677s)\n",
      "               Value function loss: 1058.3014\n",
      "                    Surrogate loss: 0.0066\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1314.63\n",
      "               Mean episode length: 470.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.77\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2433024\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 393.46s\n",
      "                               ETA: 932.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 297/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5701 steps/s (collection: 0.775s, learning 0.662s)\n",
      "               Value function loss: 537.5707\n",
      "                    Surrogate loss: 0.0066\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1298.11\n",
      "               Mean episode length: 467.35\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.80\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2441216\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 394.90s\n",
      "                               ETA: 931.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 298/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5403 steps/s (collection: 0.780s, learning 0.736s)\n",
      "               Value function loss: 531.3149\n",
      "                    Surrogate loss: -0.0025\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1296.46\n",
      "               Mean episode length: 466.33\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.84\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2449408\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 396.41s\n",
      "                               ETA: 930.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 299/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5922 steps/s (collection: 0.811s, learning 0.573s)\n",
      "               Value function loss: 690.7376\n",
      "                    Surrogate loss: -0.0019\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1283.85\n",
      "               Mean episode length: 462.34\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.84\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2457600\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 397.80s\n",
      "                               ETA: 929.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 300/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5780 steps/s (collection: 0.813s, learning 0.604s)\n",
      "               Value function loss: 849.8880\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1255.83\n",
      "               Mean episode length: 453.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.75\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2465792\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 399.21s\n",
      "                               ETA: 928.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 301/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5737 steps/s (collection: 0.793s, learning 0.635s)\n",
      "               Value function loss: 616.8531\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1259.07\n",
      "               Mean episode length: 453.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.75\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2473984\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 400.64s\n",
      "                               ETA: 927.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 302/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5802 steps/s (collection: 0.819s, learning 0.593s)\n",
      "               Value function loss: 840.3322\n",
      "                    Surrogate loss: -0.0009\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1268.68\n",
      "               Mean episode length: 455.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.74\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2482176\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 402.05s\n",
      "                               ETA: 926.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 303/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6424 steps/s (collection: 0.767s, learning 0.508s)\n",
      "               Value function loss: 448.8433\n",
      "                    Surrogate loss: -0.0006\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1261.79\n",
      "               Mean episode length: 451.56\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2490368\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 403.33s\n",
      "                               ETA: 924.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 304/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6461 steps/s (collection: 0.800s, learning 0.468s)\n",
      "               Value function loss: 722.9432\n",
      "                    Surrogate loss: 0.0059\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1252.43\n",
      "               Mean episode length: 450.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.69\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2498560\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 404.59s\n",
      "                               ETA: 923.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 305/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5674 steps/s (collection: 0.835s, learning 0.609s)\n",
      "               Value function loss: 793.3268\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 0.96\n",
      "                       Mean reward: 1234.91\n",
      "               Mean episode length: 442.08\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.67\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2506752\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 406.04s\n",
      "                               ETA: 922.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 306/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5561 steps/s (collection: 0.775s, learning 0.698s)\n",
      "               Value function loss: 490.4487\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1221.33\n",
      "               Mean episode length: 440.59\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.70\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2514944\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 407.51s\n",
      "                               ETA: 921.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 307/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6398 steps/s (collection: 0.777s, learning 0.503s)\n",
      "               Value function loss: 518.2804\n",
      "                    Surrogate loss: -0.0028\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1188.93\n",
      "               Mean episode length: 432.39\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.67\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2523136\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 408.79s\n",
      "                               ETA: 919.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 308/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6113 steps/s (collection: 0.763s, learning 0.577s)\n",
      "               Value function loss: 1142.9323\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1163.97\n",
      "               Mean episode length: 423.71\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.65\n",
      "       Mean episode length/episode: 28.25\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2531328\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 410.13s\n",
      "                               ETA: 918.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 309/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6048 steps/s (collection: 0.771s, learning 0.583s)\n",
      "               Value function loss: 590.4375\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1114.41\n",
      "               Mean episode length: 409.07\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.68\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2539520\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 411.49s\n",
      "                               ETA: 917.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 310/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5766 steps/s (collection: 0.783s, learning 0.637s)\n",
      "               Value function loss: 734.3130\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1109.65\n",
      "               Mean episode length: 405.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.78\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2547712\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 412.91s\n",
      "                               ETA: 916.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 311/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5815 steps/s (collection: 0.847s, learning 0.561s)\n",
      "               Value function loss: 999.0619\n",
      "                    Surrogate loss: 0.0039\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1091.40\n",
      "               Mean episode length: 398.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.74\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2555904\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 414.32s\n",
      "                               ETA: 914.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 312/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6220 steps/s (collection: 0.801s, learning 0.516s)\n",
      "               Value function loss: 1375.6805\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1080.05\n",
      "               Mean episode length: 393.60\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 28.64\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2564096\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 415.63s\n",
      "                               ETA: 913.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 313/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5966 steps/s (collection: 0.777s, learning 0.596s)\n",
      "               Value function loss: 442.3601\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1042.52\n",
      "               Mean episode length: 379.05\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.78\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2572288\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 417.01s\n",
      "                               ETA: 912.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 314/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5734 steps/s (collection: 0.748s, learning 0.680s)\n",
      "               Value function loss: 704.0959\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1026.82\n",
      "               Mean episode length: 376.88\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.86\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2580480\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 418.43s\n",
      "                               ETA: 911.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 315/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5616 steps/s (collection: 0.812s, learning 0.646s)\n",
      "               Value function loss: 991.4440\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1035.02\n",
      "               Mean episode length: 377.88\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.90\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2588672\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 419.89s\n",
      "                               ETA: 910.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 316/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6086 steps/s (collection: 0.861s, learning 0.485s)\n",
      "               Value function loss: 946.6111\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1053.10\n",
      "               Mean episode length: 383.69\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.85\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2596864\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 421.24s\n",
      "                               ETA: 908.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 317/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5935 steps/s (collection: 0.767s, learning 0.613s)\n",
      "               Value function loss: 668.6360\n",
      "                    Surrogate loss: 0.0020\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1057.60\n",
      "               Mean episode length: 385.66\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.91\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2605056\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 422.62s\n",
      "                               ETA: 907.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 318/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5596 steps/s (collection: 0.774s, learning 0.689s)\n",
      "               Value function loss: 765.3194\n",
      "                    Surrogate loss: 0.0017\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1101.43\n",
      "               Mean episode length: 396.77\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.89\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2613248\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 424.08s\n",
      "                               ETA: 906.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 319/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5776 steps/s (collection: 0.782s, learning 0.636s)\n",
      "               Value function loss: 702.7501\n",
      "                    Surrogate loss: 0.0055\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1126.23\n",
      "               Mean episode length: 405.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.92\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2621440\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 425.50s\n",
      "                               ETA: 905.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 320/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5451 steps/s (collection: 0.796s, learning 0.707s)\n",
      "               Value function loss: 628.5809\n",
      "                    Surrogate loss: 0.0003\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1156.80\n",
      "               Mean episode length: 416.26\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.82\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2629632\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 427.00s\n",
      "                               ETA: 904.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 321/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6148 steps/s (collection: 0.771s, learning 0.561s)\n",
      "               Value function loss: 599.9193\n",
      "                    Surrogate loss: 0.0059\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1149.18\n",
      "               Mean episode length: 414.04\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.80\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2637824\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 428.34s\n",
      "                               ETA: 903.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 322/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5653 steps/s (collection: 0.787s, learning 0.662s)\n",
      "               Value function loss: 746.1931\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1136.09\n",
      "               Mean episode length: 410.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2646016\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 429.78s\n",
      "                               ETA: 902.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 323/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5675 steps/s (collection: 0.845s, learning 0.598s)\n",
      "               Value function loss: 841.1081\n",
      "                    Surrogate loss: 0.0066\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1140.99\n",
      "               Mean episode length: 411.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.78\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2654208\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 431.23s\n",
      "                               ETA: 901.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 324/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5671 steps/s (collection: 0.766s, learning 0.678s)\n",
      "               Value function loss: 1298.7075\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1179.54\n",
      "               Mean episode length: 425.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 28.54\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2662400\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 432.67s\n",
      "                               ETA: 900.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 325/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5690 steps/s (collection: 0.798s, learning 0.641s)\n",
      "               Value function loss: 590.4715\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1201.06\n",
      "               Mean episode length: 434.02\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.80\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2670592\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 434.11s\n",
      "                               ETA: 898.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 326/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5957 steps/s (collection: 0.799s, learning 0.576s)\n",
      "               Value function loss: 716.0921\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1216.54\n",
      "               Mean episode length: 437.88\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.87\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2678784\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 435.49s\n",
      "                               ETA: 897.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 327/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5526 steps/s (collection: 0.833s, learning 0.650s)\n",
      "               Value function loss: 1180.9311\n",
      "                    Surrogate loss: 0.0050\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1232.54\n",
      "               Mean episode length: 437.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.80\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2686976\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 436.97s\n",
      "                               ETA: 896.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 328/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5564 steps/s (collection: 0.789s, learning 0.683s)\n",
      "               Value function loss: 997.8553\n",
      "                    Surrogate loss: 0.0020\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1241.50\n",
      "               Mean episode length: 438.72\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.76\n",
      "       Mean episode length/episode: 28.64\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2695168\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 438.44s\n",
      "                               ETA: 895.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 329/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6119 steps/s (collection: 0.734s, learning 0.605s)\n",
      "               Value function loss: 745.6172\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1231.34\n",
      "               Mean episode length: 434.76\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.86\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2703360\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 439.78s\n",
      "                               ETA: 894.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 330/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5486 steps/s (collection: 0.851s, learning 0.642s)\n",
      "               Value function loss: 542.6415\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1237.18\n",
      "               Mean episode length: 434.76\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.86\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2711552\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 441.27s\n",
      "                               ETA: 893.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 331/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5376 steps/s (collection: 0.803s, learning 0.721s)\n",
      "               Value function loss: 795.8762\n",
      "                    Surrogate loss: 0.0043\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1264.58\n",
      "               Mean episode length: 443.20\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2719744\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 442.80s\n",
      "                               ETA: 892.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 332/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5893 steps/s (collection: 0.809s, learning 0.582s)\n",
      "               Value function loss: 607.8481\n",
      "                    Surrogate loss: 0.0040\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1266.58\n",
      "               Mean episode length: 447.34\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.77\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2727936\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 444.19s\n",
      "                               ETA: 891.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 333/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6060 steps/s (collection: 0.727s, learning 0.624s)\n",
      "               Value function loss: 671.8861\n",
      "                    Surrogate loss: 0.0025\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1268.81\n",
      "               Mean episode length: 446.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.78\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2736128\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 445.54s\n",
      "                               ETA: 889.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 334/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5629 steps/s (collection: 0.837s, learning 0.618s)\n",
      "               Value function loss: 566.1150\n",
      "                    Surrogate loss: 0.0098\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1278.49\n",
      "               Mean episode length: 450.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.77\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2744320\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 446.99s\n",
      "                               ETA: 888.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 335/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5955 steps/s (collection: 0.762s, learning 0.614s)\n",
      "               Value function loss: 1048.3570\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1244.11\n",
      "               Mean episode length: 441.58\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.81\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2752512\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 448.37s\n",
      "                               ETA: 887.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 336/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6015 steps/s (collection: 0.855s, learning 0.506s)\n",
      "               Value function loss: 1116.3978\n",
      "                    Surrogate loss: 0.0012\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1133.92\n",
      "               Mean episode length: 402.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.50\n",
      "       Mean episode length/episode: 27.58\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2760704\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 449.73s\n",
      "                               ETA: 886.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 337/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5893 steps/s (collection: 0.841s, learning 0.549s)\n",
      "               Value function loss: 509.3810\n",
      "                    Surrogate loss: -0.0011\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1080.85\n",
      "               Mean episode length: 389.63\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.46\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2768896\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 451.12s\n",
      "                               ETA: 884.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 338/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5927 steps/s (collection: 0.803s, learning 0.579s)\n",
      "               Value function loss: 767.9834\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1027.53\n",
      "               Mean episode length: 372.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.40\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2777088\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 452.50s\n",
      "                               ETA: 883.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 339/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5954 steps/s (collection: 0.777s, learning 0.599s)\n",
      "               Value function loss: 861.9886\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1028.20\n",
      "               Mean episode length: 374.13\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.34\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2785280\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 453.88s\n",
      "                               ETA: 882.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 340/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5208 steps/s (collection: 0.828s, learning 0.745s)\n",
      "               Value function loss: 631.9089\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1002.02\n",
      "               Mean episode length: 363.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.33\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2793472\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 455.45s\n",
      "                               ETA: 881.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 341/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5849 steps/s (collection: 0.763s, learning 0.638s)\n",
      "               Value function loss: 622.8277\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 1003.15\n",
      "               Mean episode length: 365.59\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.44\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2801664\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 456.85s\n",
      "                               ETA: 880.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 342/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5937 steps/s (collection: 0.744s, learning 0.635s)\n",
      "               Value function loss: 619.1996\n",
      "                    Surrogate loss: -0.0015\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 980.76\n",
      "               Mean episode length: 360.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.43\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2809856\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 458.23s\n",
      "                               ETA: 879.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 343/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5960 steps/s (collection: 0.821s, learning 0.554s)\n",
      "               Value function loss: 649.9105\n",
      "                    Surrogate loss: 0.0010\n",
      "             Mean action noise std: 0.97\n",
      "                       Mean reward: 983.53\n",
      "               Mean episode length: 365.21\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.43\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2818048\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 459.61s\n",
      "                               ETA: 877.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 344/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5448 steps/s (collection: 0.854s, learning 0.649s)\n",
      "               Value function loss: 438.9769\n",
      "                    Surrogate loss: 0.0357\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1005.31\n",
      "               Mean episode length: 372.24\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.52\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2826240\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 461.11s\n",
      "                               ETA: 876.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 345/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6613 steps/s (collection: 0.759s, learning 0.479s)\n",
      "               Value function loss: 748.2068\n",
      "                    Surrogate loss: 0.0051\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1049.04\n",
      "               Mean episode length: 393.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.50\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2834432\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 462.35s\n",
      "                               ETA: 875.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 346/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5566 steps/s (collection: 0.818s, learning 0.653s)\n",
      "               Value function loss: 399.2282\n",
      "                    Surrogate loss: 0.0007\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1081.41\n",
      "               Mean episode length: 408.67\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.49\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2842624\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 463.82s\n",
      "                               ETA: 874.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 347/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5532 steps/s (collection: 0.806s, learning 0.675s)\n",
      "               Value function loss: 449.1584\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1090.92\n",
      "               Mean episode length: 418.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.41\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2850816\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 465.30s\n",
      "                               ETA: 873.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 348/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5958 steps/s (collection: 0.736s, learning 0.639s)\n",
      "               Value function loss: 383.4429\n",
      "                    Surrogate loss: -0.0079\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1118.55\n",
      "               Mean episode length: 429.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.40\n",
      "       Mean episode length/episode: 31.27\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2859008\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 466.68s\n",
      "                               ETA: 871.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 349/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5879 steps/s (collection: 0.804s, learning 0.589s)\n",
      "               Value function loss: 531.4208\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1139.07\n",
      "               Mean episode length: 441.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.40\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2867200\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 468.07s\n",
      "                               ETA: 870.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 350/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5472 steps/s (collection: 0.883s, learning 0.614s)\n",
      "               Value function loss: 470.4744\n",
      "                    Surrogate loss: -0.0071\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1134.14\n",
      "               Mean episode length: 444.41\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.35\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2875392\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 469.57s\n",
      "                               ETA: 869.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 351/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5666 steps/s (collection: 0.779s, learning 0.666s)\n",
      "               Value function loss: 888.8525\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1101.25\n",
      "               Mean episode length: 444.02\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.28\n",
      "       Mean episode length/episode: 28.74\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2883584\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 471.01s\n",
      "                               ETA: 868.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 352/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5866 steps/s (collection: 0.801s, learning 0.595s)\n",
      "               Value function loss: 903.6564\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1107.51\n",
      "               Mean episode length: 456.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.28\n",
      "       Mean episode length/episode: 28.74\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2891776\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 472.41s\n",
      "                               ETA: 867.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 353/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5057 steps/s (collection: 0.993s, learning 0.627s)\n",
      "               Value function loss: 807.9728\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1106.44\n",
      "               Mean episode length: 462.97\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.37\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2899968\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 474.03s\n",
      "                               ETA: 866.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 354/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5787 steps/s (collection: 0.766s, learning 0.650s)\n",
      "               Value function loss: 533.6483\n",
      "                    Surrogate loss: 0.0110\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1119.68\n",
      "               Mean episode length: 467.96\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.41\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2908160\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 475.44s\n",
      "                               ETA: 865.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 355/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5980 steps/s (collection: 0.791s, learning 0.579s)\n",
      "               Value function loss: 1013.4396\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1114.61\n",
      "               Mean episode length: 469.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.39\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2916352\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 476.81s\n",
      "                               ETA: 863.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 356/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5566 steps/s (collection: 0.824s, learning 0.648s)\n",
      "               Value function loss: 560.6389\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1122.76\n",
      "               Mean episode length: 469.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.38\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2924544\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 478.28s\n",
      "                               ETA: 862.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 357/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5561 steps/s (collection: 0.738s, learning 0.735s)\n",
      "               Value function loss: 516.4638\n",
      "                    Surrogate loss: 0.0018\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1120.21\n",
      "               Mean episode length: 469.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.39\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2932736\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 479.76s\n",
      "                               ETA: 861.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 358/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5002 steps/s (collection: 0.840s, learning 0.798s)\n",
      "               Value function loss: 690.5449\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1147.61\n",
      "               Mean episode length: 473.74\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.36\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2940928\n",
      "                    Iteration time: 1.64s\n",
      "                        Total time: 481.39s\n",
      "                               ETA: 860.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 359/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5743 steps/s (collection: 0.840s, learning 0.587s)\n",
      "               Value function loss: 745.3003\n",
      "                    Surrogate loss: 0.0033\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1158.95\n",
      "               Mean episode length: 476.53\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.37\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2949120\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 482.82s\n",
      "                               ETA: 859.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 360/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6177 steps/s (collection: 0.774s, learning 0.552s)\n",
      "               Value function loss: 488.8842\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1161.68\n",
      "               Mean episode length: 477.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.42\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2957312\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 484.15s\n",
      "                               ETA: 858.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 361/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5434 steps/s (collection: 0.778s, learning 0.729s)\n",
      "               Value function loss: 412.1699\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1156.26\n",
      "               Mean episode length: 478.41\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.37\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2965504\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 485.65s\n",
      "                               ETA: 857.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 362/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5445 steps/s (collection: 0.824s, learning 0.680s)\n",
      "               Value function loss: 386.5960\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1153.99\n",
      "               Mean episode length: 476.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.40\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2973696\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 487.16s\n",
      "                               ETA: 856.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 363/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6031 steps/s (collection: 0.776s, learning 0.582s)\n",
      "               Value function loss: 361.8506\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1151.85\n",
      "               Mean episode length: 478.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.43\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2981888\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 488.52s\n",
      "                               ETA: 854.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 364/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6544 steps/s (collection: 0.761s, learning 0.491s)\n",
      "               Value function loss: 496.2116\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1139.06\n",
      "               Mean episode length: 475.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.41\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2990080\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 489.77s\n",
      "                               ETA: 853.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 365/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5417 steps/s (collection: 0.866s, learning 0.647s)\n",
      "               Value function loss: 439.5545\n",
      "                    Surrogate loss: 0.0015\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1138.12\n",
      "               Mean episode length: 475.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.47\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 2998272\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 491.28s\n",
      "                               ETA: 852.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 366/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6243 steps/s (collection: 0.746s, learning 0.566s)\n",
      "               Value function loss: 623.1927\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1144.32\n",
      "               Mean episode length: 475.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.47\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3006464\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 492.59s\n",
      "                               ETA: 851.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 367/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5317 steps/s (collection: 0.875s, learning 0.666s)\n",
      "               Value function loss: 946.3791\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1137.95\n",
      "               Mean episode length: 473.26\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.40\n",
      "       Mean episode length/episode: 28.64\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3014656\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 494.13s\n",
      "                               ETA: 850.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 368/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5669 steps/s (collection: 0.842s, learning 0.603s)\n",
      "               Value function loss: 543.8250\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1135.29\n",
      "               Mean episode length: 473.26\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.46\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3022848\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 495.58s\n",
      "                               ETA: 848.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 369/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6020 steps/s (collection: 0.755s, learning 0.606s)\n",
      "               Value function loss: 668.8837\n",
      "                    Surrogate loss: -0.0010\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1128.38\n",
      "               Mean episode length: 473.26\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.48\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3031040\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 496.94s\n",
      "                               ETA: 847.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 370/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5713 steps/s (collection: 0.818s, learning 0.616s)\n",
      "               Value function loss: 629.7577\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1124.03\n",
      "               Mean episode length: 473.26\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.50\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3039232\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 498.37s\n",
      "                               ETA: 846.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 371/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5688 steps/s (collection: 0.863s, learning 0.577s)\n",
      "               Value function loss: 855.7055\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1133.50\n",
      "               Mean episode length: 477.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.49\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3047424\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 499.81s\n",
      "                               ETA: 845.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 372/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5977 steps/s (collection: 0.748s, learning 0.623s)\n",
      "               Value function loss: 630.8307\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1129.67\n",
      "               Mean episode length: 473.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.56\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3055616\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 501.18s\n",
      "                               ETA: 843.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 373/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5420 steps/s (collection: 0.773s, learning 0.738s)\n",
      "               Value function loss: 562.7009\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1142.15\n",
      "               Mean episode length: 476.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.62\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3063808\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 502.70s\n",
      "                               ETA: 842.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 374/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5597 steps/s (collection: 0.917s, learning 0.546s)\n",
      "               Value function loss: 769.9117\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1184.18\n",
      "               Mean episode length: 486.08\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.59\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3072000\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 504.16s\n",
      "                               ETA: 841.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 375/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6539 steps/s (collection: 0.775s, learning 0.478s)\n",
      "               Value function loss: 426.7217\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1177.03\n",
      "               Mean episode length: 482.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.63\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3080192\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 505.41s\n",
      "                               ETA: 840.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 376/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5663 steps/s (collection: 0.744s, learning 0.702s)\n",
      "               Value function loss: 516.9576\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1182.42\n",
      "               Mean episode length: 482.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.69\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3088384\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 506.86s\n",
      "                               ETA: 838.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 377/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5450 steps/s (collection: 0.749s, learning 0.754s)\n",
      "               Value function loss: 477.0773\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1186.07\n",
      "               Mean episode length: 481.88\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.68\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3096576\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 508.36s\n",
      "                               ETA: 837.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 378/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6135 steps/s (collection: 0.832s, learning 0.503s)\n",
      "               Value function loss: 682.3822\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1201.58\n",
      "               Mean episode length: 485.49\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3104768\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 509.70s\n",
      "                               ETA: 836.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 379/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5395 steps/s (collection: 0.788s, learning 0.730s)\n",
      "               Value function loss: 369.8572\n",
      "                    Surrogate loss: -0.0019\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1202.21\n",
      "               Mean episode length: 481.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.73\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3112960\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 511.21s\n",
      "                               ETA: 835.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 380/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5998 steps/s (collection: 0.793s, learning 0.573s)\n",
      "               Value function loss: 563.3115\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1182.45\n",
      "               Mean episode length: 477.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.77\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3121152\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 512.58s\n",
      "                               ETA: 834.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 381/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5923 steps/s (collection: 0.773s, learning 0.610s)\n",
      "               Value function loss: 487.9439\n",
      "                    Surrogate loss: 0.0068\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1185.21\n",
      "               Mean episode length: 477.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3129344\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 513.96s\n",
      "                               ETA: 832.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 382/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5746 steps/s (collection: 0.782s, learning 0.643s)\n",
      "               Value function loss: 1275.8296\n",
      "                    Surrogate loss: 0.0037\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1208.66\n",
      "               Mean episode length: 477.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.71\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3137536\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 515.39s\n",
      "                               ETA: 831.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 383/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5931 steps/s (collection: 0.792s, learning 0.589s)\n",
      "               Value function loss: 1048.9579\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1220.55\n",
      "               Mean episode length: 477.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.63\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3145728\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 516.77s\n",
      "                               ETA: 830.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 384/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5455 steps/s (collection: 0.902s, learning 0.600s)\n",
      "               Value function loss: 751.4909\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 0.98\n",
      "                       Mean reward: 1250.21\n",
      "               Mean episode length: 486.02\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.71\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3153920\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 518.27s\n",
      "                               ETA: 829.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 385/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5994 steps/s (collection: 0.781s, learning 0.585s)\n",
      "               Value function loss: 688.3048\n",
      "                    Surrogate loss: 0.0031\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1267.47\n",
      "               Mean episode length: 487.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.67\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3162112\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 519.64s\n",
      "                               ETA: 827.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 386/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5680 steps/s (collection: 0.751s, learning 0.692s)\n",
      "               Value function loss: 838.9281\n",
      "                    Surrogate loss: -0.0019\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1269.68\n",
      "               Mean episode length: 487.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.65\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3170304\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 521.08s\n",
      "                               ETA: 826.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 387/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5101 steps/s (collection: 0.907s, learning 0.699s)\n",
      "               Value function loss: 744.2459\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1288.76\n",
      "               Mean episode length: 490.79\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.68\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3178496\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 522.69s\n",
      "                               ETA: 825.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 388/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5573 steps/s (collection: 0.816s, learning 0.654s)\n",
      "               Value function loss: 526.4952\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1297.01\n",
      "               Mean episode length: 490.79\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.73\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3186688\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 524.16s\n",
      "                               ETA: 824.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 389/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5528 steps/s (collection: 0.814s, learning 0.668s)\n",
      "               Value function loss: 699.6227\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1296.10\n",
      "               Mean episode length: 491.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.74\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3194880\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 525.64s\n",
      "                               ETA: 823.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 390/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5184 steps/s (collection: 0.889s, learning 0.692s)\n",
      "               Value function loss: 873.9233\n",
      "                    Surrogate loss: -0.0078\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1312.95\n",
      "               Mean episode length: 495.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3203072\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 527.22s\n",
      "                               ETA: 822.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 391/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5912 steps/s (collection: 0.798s, learning 0.588s)\n",
      "               Value function loss: 470.5047\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1315.70\n",
      "               Mean episode length: 491.63\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3211264\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 528.60s\n",
      "                               ETA: 821.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 392/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5530 steps/s (collection: 0.812s, learning 0.670s)\n",
      "               Value function loss: 748.3696\n",
      "                    Surrogate loss: 0.0076\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1298.46\n",
      "               Mean episode length: 482.67\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.65\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3219456\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 530.08s\n",
      "                               ETA: 820.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 393/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6034 steps/s (collection: 0.830s, learning 0.528s)\n",
      "               Value function loss: 520.3381\n",
      "                    Surrogate loss: 0.0151\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1287.58\n",
      "               Mean episode length: 482.67\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.71\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3227648\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 531.44s\n",
      "                               ETA: 818.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 394/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6079 steps/s (collection: 0.739s, learning 0.609s)\n",
      "               Value function loss: 517.3570\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1243.90\n",
      "               Mean episode length: 466.88\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.73\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3235840\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 532.79s\n",
      "                               ETA: 817.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 395/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6193 steps/s (collection: 0.758s, learning 0.565s)\n",
      "               Value function loss: 553.9634\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1234.31\n",
      "               Mean episode length: 460.76\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.76\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3244032\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 534.11s\n",
      "                               ETA: 816.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 396/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5470 steps/s (collection: 0.880s, learning 0.617s)\n",
      "               Value function loss: 613.1635\n",
      "                    Surrogate loss: -0.0076\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1227.27\n",
      "               Mean episode length: 456.98\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.72\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3252224\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 535.61s\n",
      "                               ETA: 814.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 397/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6204 steps/s (collection: 0.797s, learning 0.524s)\n",
      "               Value function loss: 419.2929\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1229.17\n",
      "               Mean episode length: 456.98\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.75\n",
      "       Mean episode length/episode: 31.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3260416\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 536.93s\n",
      "                               ETA: 813.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 398/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5561 steps/s (collection: 0.848s, learning 0.625s)\n",
      "               Value function loss: 1068.1032\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1210.98\n",
      "               Mean episode length: 450.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.68\n",
      "       Mean episode length/episode: 28.44\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3268608\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 538.40s\n",
      "                               ETA: 812.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 399/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5416 steps/s (collection: 0.885s, learning 0.627s)\n",
      "               Value function loss: 922.6972\n",
      "                    Surrogate loss: -0.0076\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1203.14\n",
      "               Mean episode length: 445.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.67\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3276800\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 539.92s\n",
      "                               ETA: 811.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 400/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6323 steps/s (collection: 0.765s, learning 0.531s)\n",
      "               Value function loss: 745.7306\n",
      "                    Surrogate loss: -0.0067\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1209.42\n",
      "               Mean episode length: 445.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.74\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3284992\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 541.21s\n",
      "                               ETA: 809.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 401/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5096 steps/s (collection: 0.835s, learning 0.772s)\n",
      "               Value function loss: 584.0993\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1197.65\n",
      "               Mean episode length: 442.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.79\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3293184\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 542.82s\n",
      "                               ETA: 808.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 402/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5584 steps/s (collection: 0.798s, learning 0.669s)\n",
      "               Value function loss: 1054.4266\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1240.10\n",
      "               Mean episode length: 457.46\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.84\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3301376\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 544.28s\n",
      "                               ETA: 807.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 403/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6070 steps/s (collection: 0.831s, learning 0.518s)\n",
      "               Value function loss: 433.8847\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1243.98\n",
      "               Mean episode length: 459.96\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.87\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3309568\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 545.63s\n",
      "                               ETA: 806.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 404/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5036 steps/s (collection: 0.830s, learning 0.797s)\n",
      "               Value function loss: 608.1446\n",
      "                    Surrogate loss: -0.0009\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1247.58\n",
      "               Mean episode length: 459.96\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.87\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3317760\n",
      "                    Iteration time: 1.63s\n",
      "                        Total time: 547.26s\n",
      "                               ETA: 805.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 405/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5755 steps/s (collection: 0.786s, learning 0.638s)\n",
      "               Value function loss: 583.8909\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1273.64\n",
      "               Mean episode length: 470.21\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.84\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3325952\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 548.68s\n",
      "                               ETA: 804.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 406/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5798 steps/s (collection: 0.802s, learning 0.611s)\n",
      "               Value function loss: 763.0523\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1299.10\n",
      "               Mean episode length: 478.10\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.84\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3334144\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 550.10s\n",
      "                               ETA: 802.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 407/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6375 steps/s (collection: 0.720s, learning 0.565s)\n",
      "               Value function loss: 856.6572\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1307.67\n",
      "               Mean episode length: 477.98\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.87\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3342336\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 551.38s\n",
      "                               ETA: 801.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 408/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4930 steps/s (collection: 0.944s, learning 0.718s)\n",
      "               Value function loss: 500.6413\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1303.48\n",
      "               Mean episode length: 477.98\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.84\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3350528\n",
      "                    Iteration time: 1.66s\n",
      "                        Total time: 553.04s\n",
      "                               ETA: 800.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 409/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5559 steps/s (collection: 0.814s, learning 0.660s)\n",
      "               Value function loss: 626.3128\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1322.03\n",
      "               Mean episode length: 481.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.93\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3358720\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 554.52s\n",
      "                               ETA: 799.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 410/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5850 steps/s (collection: 0.757s, learning 0.643s)\n",
      "               Value function loss: 772.6651\n",
      "                    Surrogate loss: -0.0065\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1338.40\n",
      "               Mean episode length: 484.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.89\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3366912\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 555.92s\n",
      "                               ETA: 798.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 411/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5222 steps/s (collection: 0.786s, learning 0.782s)\n",
      "               Value function loss: 776.2250\n",
      "                    Surrogate loss: -0.0084\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1341.59\n",
      "               Mean episode length: 484.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.94\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3375104\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 557.49s\n",
      "                               ETA: 797.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 412/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6292 steps/s (collection: 0.838s, learning 0.464s)\n",
      "               Value function loss: 670.1512\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1357.45\n",
      "               Mean episode length: 489.04\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.98\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3383296\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 558.79s\n",
      "                               ETA: 795.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 413/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5745 steps/s (collection: 0.785s, learning 0.641s)\n",
      "               Value function loss: 974.2556\n",
      "                    Surrogate loss: 0.0080\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1353.64\n",
      "               Mean episode length: 486.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.96\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3391488\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 560.21s\n",
      "                               ETA: 794.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 414/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5447 steps/s (collection: 0.863s, learning 0.641s)\n",
      "               Value function loss: 1321.9784\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1380.02\n",
      "               Mean episode length: 489.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.88\n",
      "       Mean episode length/episode: 28.85\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3399680\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 561.72s\n",
      "                               ETA: 793.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 415/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6598 steps/s (collection: 0.733s, learning 0.509s)\n",
      "               Value function loss: 763.0995\n",
      "                    Surrogate loss: 0.0012\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1389.92\n",
      "               Mean episode length: 489.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.88\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3407872\n",
      "                    Iteration time: 1.24s\n",
      "                        Total time: 562.96s\n",
      "                               ETA: 791.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 416/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5406 steps/s (collection: 0.908s, learning 0.607s)\n",
      "               Value function loss: 850.9474\n",
      "                    Surrogate loss: 0.0105\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1392.59\n",
      "               Mean episode length: 489.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.90\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3416064\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 564.47s\n",
      "                               ETA: 790.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 417/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5189 steps/s (collection: 0.790s, learning 0.789s)\n",
      "               Value function loss: 789.3326\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1398.50\n",
      "               Mean episode length: 489.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.96\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3424256\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 566.05s\n",
      "                               ETA: 789.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 418/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6414 steps/s (collection: 0.737s, learning 0.540s)\n",
      "               Value function loss: 999.9577\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1405.51\n",
      "               Mean episode length: 488.65\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.88\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3432448\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 567.33s\n",
      "                               ETA: 788.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 419/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5999 steps/s (collection: 0.788s, learning 0.578s)\n",
      "               Value function loss: 727.8003\n",
      "                    Surrogate loss: -0.0073\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1401.84\n",
      "               Mean episode length: 488.65\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.92\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3440640\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 568.69s\n",
      "                               ETA: 786.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 420/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5089 steps/s (collection: 0.809s, learning 0.801s)\n",
      "               Value function loss: 920.2411\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1401.39\n",
      "               Mean episode length: 484.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.90\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3448832\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 570.30s\n",
      "                               ETA: 785.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 421/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5326 steps/s (collection: 0.769s, learning 0.769s)\n",
      "               Value function loss: 891.4395\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1408.15\n",
      "               Mean episode length: 484.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.87\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3457024\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 571.84s\n",
      "                               ETA: 784.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 422/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6134 steps/s (collection: 0.773s, learning 0.562s)\n",
      "               Value function loss: 706.2929\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1395.83\n",
      "               Mean episode length: 480.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.87\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3465216\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 573.18s\n",
      "                               ETA: 783.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 423/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5845 steps/s (collection: 0.813s, learning 0.588s)\n",
      "               Value function loss: 992.2537\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1392.53\n",
      "               Mean episode length: 478.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.91\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3473408\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 574.58s\n",
      "                               ETA: 781.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 424/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5793 steps/s (collection: 0.754s, learning 0.660s)\n",
      "               Value function loss: 680.9360\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1379.36\n",
      "               Mean episode length: 474.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.91\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3481600\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 575.99s\n",
      "                               ETA: 780.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 425/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5868 steps/s (collection: 0.841s, learning 0.555s)\n",
      "               Value function loss: 731.9477\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1364.00\n",
      "               Mean episode length: 470.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.88\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3489792\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 577.39s\n",
      "                               ETA: 779.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 426/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5624 steps/s (collection: 0.766s, learning 0.690s)\n",
      "               Value function loss: 550.4987\n",
      "                    Surrogate loss: 0.0020\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1357.93\n",
      "               Mean episode length: 470.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.91\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3497984\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 578.85s\n",
      "                               ETA: 778.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 427/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5915 steps/s (collection: 0.799s, learning 0.585s)\n",
      "               Value function loss: 817.7741\n",
      "                    Surrogate loss: -0.0009\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1359.98\n",
      "               Mean episode length: 470.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.90\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3506176\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 580.23s\n",
      "                               ETA: 776.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 428/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6314 steps/s (collection: 0.723s, learning 0.575s)\n",
      "               Value function loss: 477.2078\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1360.97\n",
      "               Mean episode length: 470.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.91\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3514368\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 581.53s\n",
      "                               ETA: 775.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 429/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5552 steps/s (collection: 0.853s, learning 0.622s)\n",
      "               Value function loss: 1551.7707\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1368.54\n",
      "               Mean episode length: 471.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.87\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3522560\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 583.00s\n",
      "                               ETA: 774.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 430/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5756 steps/s (collection: 0.792s, learning 0.631s)\n",
      "               Value function loss: 1313.9499\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1379.78\n",
      "               Mean episode length: 475.83\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.85\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3530752\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 584.43s\n",
      "                               ETA: 772.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 431/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5698 steps/s (collection: 0.808s, learning 0.630s)\n",
      "               Value function loss: 804.2178\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1376.60\n",
      "               Mean episode length: 473.35\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.91\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3538944\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 585.86s\n",
      "                               ETA: 771.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 432/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5886 steps/s (collection: 0.827s, learning 0.564s)\n",
      "               Value function loss: 507.0313\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 0.99\n",
      "                       Mean reward: 1364.07\n",
      "               Mean episode length: 472.21\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.96\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3547136\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 587.26s\n",
      "                               ETA: 770.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 433/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4954 steps/s (collection: 0.945s, learning 0.708s)\n",
      "               Value function loss: 1340.2275\n",
      "                    Surrogate loss: -0.0010\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1362.05\n",
      "               Mean episode length: 472.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.98\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3555328\n",
      "                    Iteration time: 1.65s\n",
      "                        Total time: 588.91s\n",
      "                               ETA: 769.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 434/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5474 steps/s (collection: 0.784s, learning 0.713s)\n",
      "               Value function loss: 661.4084\n",
      "                    Surrogate loss: -0.0083\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1374.92\n",
      "               Mean episode length: 474.45\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.91\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3563520\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 590.41s\n",
      "                               ETA: 768.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 435/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6211 steps/s (collection: 0.744s, learning 0.575s)\n",
      "               Value function loss: 683.8980\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1372.84\n",
      "               Mean episode length: 476.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.97\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3571712\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 591.72s\n",
      "                               ETA: 766.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 436/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5772 steps/s (collection: 0.759s, learning 0.661s)\n",
      "               Value function loss: 890.6607\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1396.21\n",
      "               Mean episode length: 482.63\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.97\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3579904\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 593.14s\n",
      "                               ETA: 765.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 437/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6288 steps/s (collection: 0.832s, learning 0.470s)\n",
      "               Value function loss: 819.7052\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1390.92\n",
      "               Mean episode length: 480.74\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.02\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3588096\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 594.45s\n",
      "                               ETA: 764.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 438/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5832 steps/s (collection: 0.829s, learning 0.576s)\n",
      "               Value function loss: 721.2917\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1374.23\n",
      "               Mean episode length: 475.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.03\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3596288\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 595.85s\n",
      "                               ETA: 762.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 439/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6234 steps/s (collection: 0.776s, learning 0.538s)\n",
      "               Value function loss: 745.9168\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1366.22\n",
      "               Mean episode length: 471.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.98\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3604480\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 597.16s\n",
      "                               ETA: 761.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 440/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5283 steps/s (collection: 0.822s, learning 0.729s)\n",
      "               Value function loss: 668.1801\n",
      "                    Surrogate loss: -0.0071\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1357.74\n",
      "               Mean episode length: 467.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.01\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3612672\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 598.72s\n",
      "                               ETA: 760.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 441/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5633 steps/s (collection: 0.840s, learning 0.615s)\n",
      "               Value function loss: 890.7011\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1366.44\n",
      "               Mean episode length: 467.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.01\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3620864\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 600.17s\n",
      "                               ETA: 759.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 442/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5580 steps/s (collection: 0.770s, learning 0.698s)\n",
      "               Value function loss: 752.4999\n",
      "                    Surrogate loss: 0.0069\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1368.40\n",
      "               Mean episode length: 468.13\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.02\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3629056\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 601.64s\n",
      "                               ETA: 757.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 443/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5770 steps/s (collection: 0.835s, learning 0.584s)\n",
      "               Value function loss: 890.2238\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1357.33\n",
      "               Mean episode length: 462.51\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.98\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3637248\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 603.06s\n",
      "                               ETA: 756.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 444/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6263 steps/s (collection: 0.746s, learning 0.561s)\n",
      "               Value function loss: 866.0523\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1377.61\n",
      "               Mean episode length: 466.57\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.03\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3645440\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 604.37s\n",
      "                               ETA: 755.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 445/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5650 steps/s (collection: 0.863s, learning 0.587s)\n",
      "               Value function loss: 1109.0347\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1388.51\n",
      "               Mean episode length: 470.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.01\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3653632\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 605.81s\n",
      "                               ETA: 753.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 446/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5685 steps/s (collection: 0.780s, learning 0.660s)\n",
      "               Value function loss: 906.6837\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1403.87\n",
      "               Mean episode length: 470.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.00\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3661824\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 607.26s\n",
      "                               ETA: 752.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 447/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5445 steps/s (collection: 0.842s, learning 0.662s)\n",
      "               Value function loss: 882.0149\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1412.81\n",
      "               Mean episode length: 470.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.03\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3670016\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 608.76s\n",
      "                               ETA: 751.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 448/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5929 steps/s (collection: 0.774s, learning 0.607s)\n",
      "               Value function loss: 739.9366\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1424.89\n",
      "               Mean episode length: 473.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.02\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3678208\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 610.14s\n",
      "                               ETA: 750.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 449/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6077 steps/s (collection: 0.764s, learning 0.584s)\n",
      "               Value function loss: 1290.9452\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1441.83\n",
      "               Mean episode length: 476.55\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.96\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3686400\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 611.49s\n",
      "                               ETA: 748.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 450/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5678 steps/s (collection: 0.746s, learning 0.697s)\n",
      "               Value function loss: 572.8889\n",
      "                    Surrogate loss: -0.0065\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1449.33\n",
      "               Mean episode length: 480.05\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.03\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3694592\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 612.93s\n",
      "                               ETA: 747.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 451/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6343 steps/s (collection: 0.755s, learning 0.537s)\n",
      "               Value function loss: 739.3496\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1451.30\n",
      "               Mean episode length: 480.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.03\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3702784\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 614.22s\n",
      "                               ETA: 746.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 452/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5492 steps/s (collection: 0.766s, learning 0.725s)\n",
      "               Value function loss: 796.1892\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1466.62\n",
      "               Mean episode length: 484.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.98\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3710976\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 615.72s\n",
      "                               ETA: 744.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 453/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5187 steps/s (collection: 0.769s, learning 0.810s)\n",
      "               Value function loss: 692.0025\n",
      "                    Surrogate loss: -0.0068\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1455.74\n",
      "               Mean episode length: 484.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.02\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3719168\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 617.29s\n",
      "                               ETA: 743.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 454/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6422 steps/s (collection: 0.761s, learning 0.515s)\n",
      "               Value function loss: 1004.3137\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 1.00\n",
      "                       Mean reward: 1476.92\n",
      "               Mean episode length: 491.24\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.00\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3727360\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 618.57s\n",
      "                               ETA: 742.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 455/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6037 steps/s (collection: 0.869s, learning 0.488s)\n",
      "               Value function loss: 542.5472\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1491.94\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.01\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3735552\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 619.93s\n",
      "                               ETA: 740.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 456/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6046 steps/s (collection: 0.747s, learning 0.608s)\n",
      "               Value function loss: 847.8441\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1494.10\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.02\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3743744\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 621.28s\n",
      "                               ETA: 739.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 457/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5484 steps/s (collection: 0.743s, learning 0.750s)\n",
      "               Value function loss: 670.2077\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1479.29\n",
      "               Mean episode length: 491.51\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.96\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3751936\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 622.78s\n",
      "                               ETA: 738.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 458/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4902 steps/s (collection: 0.812s, learning 0.859s)\n",
      "               Value function loss: 750.1951\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1470.95\n",
      "               Mean episode length: 489.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.00\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3760128\n",
      "                    Iteration time: 1.67s\n",
      "                        Total time: 624.45s\n",
      "                               ETA: 737.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 459/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5776 steps/s (collection: 0.766s, learning 0.652s)\n",
      "               Value function loss: 833.0367\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1466.74\n",
      "               Mean episode length: 489.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.96\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3768320\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 625.86s\n",
      "                               ETA: 736.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 460/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6114 steps/s (collection: 0.738s, learning 0.602s)\n",
      "               Value function loss: 968.6375\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1460.64\n",
      "               Mean episode length: 489.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.01\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3776512\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 627.20s\n",
      "                               ETA: 734.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 461/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4851 steps/s (collection: 0.876s, learning 0.813s)\n",
      "               Value function loss: 1314.6473\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1459.20\n",
      "               Mean episode length: 489.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.96\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3784704\n",
      "                    Iteration time: 1.69s\n",
      "                        Total time: 628.89s\n",
      "                               ETA: 733.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 462/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5872 steps/s (collection: 0.757s, learning 0.638s)\n",
      "               Value function loss: 770.7519\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1468.37\n",
      "               Mean episode length: 489.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.99\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3792896\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 630.29s\n",
      "                               ETA: 732.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 463/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6499 steps/s (collection: 0.788s, learning 0.472s)\n",
      "               Value function loss: 820.1512\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1462.40\n",
      "               Mean episode length: 488.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.02\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3801088\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 631.55s\n",
      "                               ETA: 730.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 464/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5662 steps/s (collection: 0.785s, learning 0.662s)\n",
      "               Value function loss: 1054.7000\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1458.08\n",
      "               Mean episode length: 488.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.06\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3809280\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 632.99s\n",
      "                               ETA: 729.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 465/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5929 steps/s (collection: 0.843s, learning 0.538s)\n",
      "               Value function loss: 878.9334\n",
      "                    Surrogate loss: -0.0009\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1454.17\n",
      "               Mean episode length: 486.29\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.02\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3817472\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 634.38s\n",
      "                               ETA: 728.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 466/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5722 steps/s (collection: 0.793s, learning 0.639s)\n",
      "               Value function loss: 706.4168\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1428.73\n",
      "               Mean episode length: 479.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.08\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3825664\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 635.81s\n",
      "                               ETA: 727.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 467/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5371 steps/s (collection: 0.853s, learning 0.672s)\n",
      "               Value function loss: 1025.1694\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1415.83\n",
      "               Mean episode length: 474.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.08\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3833856\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 637.33s\n",
      "                               ETA: 725.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 468/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5631 steps/s (collection: 0.766s, learning 0.689s)\n",
      "               Value function loss: 760.0393\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1431.53\n",
      "               Mean episode length: 479.23\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.07\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3842048\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 638.79s\n",
      "                               ETA: 724.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 469/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5270 steps/s (collection: 0.862s, learning 0.693s)\n",
      "               Value function loss: 646.1243\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1435.48\n",
      "               Mean episode length: 479.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.07\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3850240\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 640.34s\n",
      "                               ETA: 723.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 470/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5325 steps/s (collection: 0.839s, learning 0.699s)\n",
      "               Value function loss: 1050.0006\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1443.07\n",
      "               Mean episode length: 481.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.01\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3858432\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 641.88s\n",
      "                               ETA: 722.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 471/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6189 steps/s (collection: 0.798s, learning 0.526s)\n",
      "               Value function loss: 775.2488\n",
      "                    Surrogate loss: -0.0003\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1437.32\n",
      "               Mean episode length: 478.36\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.03\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3866624\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 643.20s\n",
      "                               ETA: 720.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 472/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5815 steps/s (collection: 0.758s, learning 0.650s)\n",
      "               Value function loss: 900.0264\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1442.16\n",
      "               Mean episode length: 478.36\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.99\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3874816\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 644.61s\n",
      "                               ETA: 719.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 473/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5111 steps/s (collection: 0.844s, learning 0.759s)\n",
      "               Value function loss: 603.8569\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1428.14\n",
      "               Mean episode length: 474.08\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.00\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3883008\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 646.21s\n",
      "                               ETA: 718.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 474/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6027 steps/s (collection: 0.758s, learning 0.601s)\n",
      "               Value function loss: 829.3560\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1430.41\n",
      "               Mean episode length: 474.08\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.98\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3891200\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 647.57s\n",
      "                               ETA: 717.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 475/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5951 steps/s (collection: 0.757s, learning 0.620s)\n",
      "               Value function loss: 510.1200\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1444.14\n",
      "               Mean episode length: 475.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.02\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3899392\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 648.95s\n",
      "                               ETA: 715.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 476/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5614 steps/s (collection: 0.762s, learning 0.697s)\n",
      "               Value function loss: 1313.3221\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1438.74\n",
      "               Mean episode length: 474.09\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.96\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3907584\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 650.41s\n",
      "                               ETA: 714.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 477/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5376 steps/s (collection: 0.878s, learning 0.646s)\n",
      "               Value function loss: 1008.3320\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1433.78\n",
      "               Mean episode length: 472.90\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.94\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3915776\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 651.93s\n",
      "                               ETA: 713.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 478/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6075 steps/s (collection: 0.769s, learning 0.579s)\n",
      "               Value function loss: 809.7976\n",
      "                    Surrogate loss: -0.0078\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1455.39\n",
      "               Mean episode length: 479.71\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 2.98\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3923968\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 653.28s\n",
      "                               ETA: 711.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 479/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5985 steps/s (collection: 0.841s, learning 0.528s)\n",
      "               Value function loss: 718.5559\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1465.44\n",
      "               Mean episode length: 484.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.03\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3932160\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 654.65s\n",
      "                               ETA: 710.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 480/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6409 steps/s (collection: 0.780s, learning 0.498s)\n",
      "               Value function loss: 1255.4122\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1447.85\n",
      "               Mean episode length: 480.16\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.04\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3940352\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 655.93s\n",
      "                               ETA: 709.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 481/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6411 steps/s (collection: 0.756s, learning 0.521s)\n",
      "               Value function loss: 558.3067\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1446.19\n",
      "               Mean episode length: 480.16\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.03\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3948544\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 657.21s\n",
      "                               ETA: 707.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 482/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5142 steps/s (collection: 0.860s, learning 0.734s)\n",
      "               Value function loss: 835.5986\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1451.89\n",
      "               Mean episode length: 480.16\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.12\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3956736\n",
      "                    Iteration time: 1.59s\n",
      "                        Total time: 658.80s\n",
      "                               ETA: 706.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 483/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5607 steps/s (collection: 0.790s, learning 0.671s)\n",
      "               Value function loss: 1001.9130\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1460.49\n",
      "               Mean episode length: 483.55\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.09\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3964928\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 660.26s\n",
      "                               ETA: 705.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 484/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5441 steps/s (collection: 0.777s, learning 0.728s)\n",
      "               Value function loss: 737.3326\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1474.61\n",
      "               Mean episode length: 487.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.12\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3973120\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 661.77s\n",
      "                               ETA: 704.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 485/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5922 steps/s (collection: 0.804s, learning 0.580s)\n",
      "               Value function loss: 711.9958\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1477.79\n",
      "               Mean episode length: 487.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.14\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3981312\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 663.15s\n",
      "                               ETA: 702.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 486/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4934 steps/s (collection: 0.837s, learning 0.823s)\n",
      "               Value function loss: 686.8682\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1475.62\n",
      "               Mean episode length: 487.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.16\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3989504\n",
      "                    Iteration time: 1.66s\n",
      "                        Total time: 664.81s\n",
      "                               ETA: 701.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 487/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4344 steps/s (collection: 0.839s, learning 1.046s)\n",
      "               Value function loss: 720.2496\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1470.76\n",
      "               Mean episode length: 486.56\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.17\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 3997696\n",
      "                    Iteration time: 1.89s\n",
      "                        Total time: 666.69s\n",
      "                               ETA: 700.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 488/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4977 steps/s (collection: 0.910s, learning 0.736s)\n",
      "               Value function loss: 905.3590\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1488.78\n",
      "               Mean episode length: 490.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.13\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4005888\n",
      "                    Iteration time: 1.65s\n",
      "                        Total time: 668.34s\n",
      "                               ETA: 699.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 489/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6118 steps/s (collection: 0.811s, learning 0.528s)\n",
      "               Value function loss: 713.0236\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.01\n",
      "                       Mean reward: 1486.54\n",
      "               Mean episode length: 490.57\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.16\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4014080\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 669.68s\n",
      "                               ETA: 698.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 490/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5439 steps/s (collection: 0.919s, learning 0.587s)\n",
      "               Value function loss: 679.9410\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1493.63\n",
      "               Mean episode length: 490.57\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.15\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4022272\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 671.18s\n",
      "                               ETA: 697.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 491/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5964 steps/s (collection: 0.812s, learning 0.562s)\n",
      "               Value function loss: 846.1030\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1489.75\n",
      "               Mean episode length: 490.57\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.20\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4030464\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 672.56s\n",
      "                               ETA: 695.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 492/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5921 steps/s (collection: 0.804s, learning 0.579s)\n",
      "               Value function loss: 1539.0029\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1508.17\n",
      "               Mean episode length: 492.23\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.10\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4038656\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 673.94s\n",
      "                               ETA: 694.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 493/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5014 steps/s (collection: 0.838s, learning 0.795s)\n",
      "               Value function loss: 818.9050\n",
      "                    Surrogate loss: -0.0006\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1516.47\n",
      "               Mean episode length: 494.72\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.04\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4046848\n",
      "                    Iteration time: 1.63s\n",
      "                        Total time: 675.58s\n",
      "                               ETA: 693.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 494/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4763 steps/s (collection: 0.883s, learning 0.836s)\n",
      "               Value function loss: 845.6867\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1517.34\n",
      "               Mean episode length: 494.72\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.14\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4055040\n",
      "                    Iteration time: 1.72s\n",
      "                        Total time: 677.30s\n",
      "                               ETA: 692.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 495/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5710 steps/s (collection: 0.853s, learning 0.581s)\n",
      "               Value function loss: 731.8574\n",
      "                    Surrogate loss: 0.0054\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1519.39\n",
      "               Mean episode length: 494.72\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.17\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4063232\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 678.73s\n",
      "                               ETA: 691.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 496/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5095 steps/s (collection: 0.846s, learning 0.761s)\n",
      "               Value function loss: 1316.5482\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1521.69\n",
      "               Mean episode length: 494.72\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.12\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4071424\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 680.34s\n",
      "                               ETA: 689.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 497/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5057 steps/s (collection: 0.920s, learning 0.700s)\n",
      "               Value function loss: 826.8441\n",
      "                    Surrogate loss: 0.0055\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1518.60\n",
      "               Mean episode length: 494.72\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.21\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4079616\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 681.96s\n",
      "                               ETA: 688.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 498/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4828 steps/s (collection: 0.839s, learning 0.857s)\n",
      "               Value function loss: 1012.1765\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1507.46\n",
      "               Mean episode length: 490.64\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.19\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4087808\n",
      "                    Iteration time: 1.70s\n",
      "                        Total time: 683.65s\n",
      "                               ETA: 687.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 499/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5459 steps/s (collection: 0.950s, learning 0.550s)\n",
      "               Value function loss: 934.4605\n",
      "                    Surrogate loss: -0.0076\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1523.49\n",
      "               Mean episode length: 491.90\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.17\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4096000\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 685.15s\n",
      "                               ETA: 686.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 500/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5719 steps/s (collection: 0.795s, learning 0.638s)\n",
      "               Value function loss: 542.9289\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1523.40\n",
      "               Mean episode length: 491.90\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.23\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4104192\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 686.59s\n",
      "                               ETA: 685.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 501/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5669 steps/s (collection: 0.894s, learning 0.551s)\n",
      "               Value function loss: 1229.2186\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1526.81\n",
      "               Mean episode length: 491.27\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.18\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4112384\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 688.03s\n",
      "                               ETA: 683.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 502/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6015 steps/s (collection: 0.796s, learning 0.566s)\n",
      "               Value function loss: 597.1539\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1528.36\n",
      "               Mean episode length: 491.27\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.15\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4120576\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 689.39s\n",
      "                               ETA: 682.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 503/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4912 steps/s (collection: 0.834s, learning 0.834s)\n",
      "               Value function loss: 800.8280\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1540.31\n",
      "               Mean episode length: 491.27\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.15\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4128768\n",
      "                    Iteration time: 1.67s\n",
      "                        Total time: 691.06s\n",
      "                               ETA: 681.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 504/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5526 steps/s (collection: 0.818s, learning 0.665s)\n",
      "               Value function loss: 602.1949\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1539.58\n",
      "               Mean episode length: 491.27\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.18\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4136960\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 692.54s\n",
      "                               ETA: 680.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 505/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6225 steps/s (collection: 0.809s, learning 0.507s)\n",
      "               Value function loss: 845.8906\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1539.34\n",
      "               Mean episode length: 491.27\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.21\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4145152\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 693.86s\n",
      "                               ETA: 678.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 506/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5467 steps/s (collection: 0.749s, learning 0.750s)\n",
      "               Value function loss: 548.8869\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1540.74\n",
      "               Mean episode length: 491.27\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.21\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4153344\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 695.36s\n",
      "                               ETA: 677.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 507/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5713 steps/s (collection: 0.828s, learning 0.606s)\n",
      "               Value function loss: 1451.6014\n",
      "                    Surrogate loss: -0.0028\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1545.63\n",
      "               Mean episode length: 491.27\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.22\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4161536\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 696.79s\n",
      "                               ETA: 676.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 508/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5846 steps/s (collection: 0.819s, learning 0.582s)\n",
      "               Value function loss: 1388.1846\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1537.29\n",
      "               Mean episode length: 487.76\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.15\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4169728\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 698.19s\n",
      "                               ETA: 674.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 509/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5131 steps/s (collection: 0.819s, learning 0.777s)\n",
      "               Value function loss: 1021.0976\n",
      "                    Surrogate loss: -0.0082\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1553.46\n",
      "               Mean episode length: 487.76\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.17\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4177920\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 699.79s\n",
      "                               ETA: 673.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 510/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6214 steps/s (collection: 0.739s, learning 0.579s)\n",
      "               Value function loss: 950.2406\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1565.97\n",
      "               Mean episode length: 491.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.14\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4186112\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 701.11s\n",
      "                               ETA: 672.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 511/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5558 steps/s (collection: 0.791s, learning 0.683s)\n",
      "               Value function loss: 1241.2356\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1554.84\n",
      "               Mean episode length: 491.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.14\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4194304\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 702.58s\n",
      "                               ETA: 671.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 512/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5373 steps/s (collection: 0.803s, learning 0.722s)\n",
      "               Value function loss: 931.4042\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1543.02\n",
      "               Mean episode length: 488.67\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.11\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4202496\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 704.11s\n",
      "                               ETA: 669.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 513/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6316 steps/s (collection: 0.806s, learning 0.491s)\n",
      "               Value function loss: 684.4911\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1553.01\n",
      "               Mean episode length: 493.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.23\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4210688\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 705.40s\n",
      "                               ETA: 668.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 514/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6062 steps/s (collection: 0.789s, learning 0.563s)\n",
      "               Value function loss: 1391.4509\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1541.30\n",
      "               Mean episode length: 489.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.20\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4218880\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 706.75s\n",
      "                               ETA: 667.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 515/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6184 steps/s (collection: 0.798s, learning 0.527s)\n",
      "               Value function loss: 497.9190\n",
      "                    Surrogate loss: 0.0001\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1529.23\n",
      "               Mean episode length: 489.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.20\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4227072\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 708.08s\n",
      "                               ETA: 665.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 516/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5959 steps/s (collection: 0.857s, learning 0.518s)\n",
      "               Value function loss: 856.8894\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1533.21\n",
      "               Mean episode length: 489.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.28\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4235264\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 709.45s\n",
      "                               ETA: 664.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 517/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6162 steps/s (collection: 0.747s, learning 0.583s)\n",
      "               Value function loss: 1070.7458\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1539.02\n",
      "               Mean episode length: 489.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.19\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4243456\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 710.78s\n",
      "                               ETA: 662.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 518/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5334 steps/s (collection: 0.918s, learning 0.617s)\n",
      "               Value function loss: 938.9826\n",
      "                    Surrogate loss: -0.0072\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1541.40\n",
      "               Mean episode length: 489.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.23\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4251648\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 712.32s\n",
      "                               ETA: 661.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 519/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5697 steps/s (collection: 0.829s, learning 0.609s)\n",
      "               Value function loss: 1160.5392\n",
      "                    Surrogate loss: -0.0076\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1544.89\n",
      "               Mean episode length: 488.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.26\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4259840\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 713.76s\n",
      "                               ETA: 660.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 520/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5301 steps/s (collection: 0.765s, learning 0.780s)\n",
      "               Value function loss: 790.7480\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1540.87\n",
      "               Mean episode length: 488.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.32\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4268032\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 715.30s\n",
      "                               ETA: 659.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 521/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6024 steps/s (collection: 0.858s, learning 0.502s)\n",
      "               Value function loss: 1026.4466\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1541.11\n",
      "               Mean episode length: 488.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.30\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4276224\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 716.66s\n",
      "                               ETA: 657.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 522/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5949 steps/s (collection: 0.805s, learning 0.572s)\n",
      "               Value function loss: 737.2520\n",
      "                    Surrogate loss: 0.0207\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1542.02\n",
      "               Mean episode length: 488.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.35\n",
      "       Mean episode length/episode: 31.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4284416\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 718.04s\n",
      "                               ETA: 656.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 523/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6485 steps/s (collection: 0.741s, learning 0.522s)\n",
      "               Value function loss: 1788.2969\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1555.97\n",
      "               Mean episode length: 488.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.28\n",
      "       Mean episode length/episode: 28.85\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4292608\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 719.30s\n",
      "                               ETA: 654.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 524/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5278 steps/s (collection: 0.909s, learning 0.643s)\n",
      "               Value function loss: 1030.7641\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 1.02\n",
      "                       Mean reward: 1563.60\n",
      "               Mean episode length: 489.61\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.27\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4300800\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 720.85s\n",
      "                               ETA: 653.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 525/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5964 steps/s (collection: 0.807s, learning 0.567s)\n",
      "               Value function loss: 1006.5972\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1575.25\n",
      "               Mean episode length: 492.11\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.32\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4308992\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 722.23s\n",
      "                               ETA: 652.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 526/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5027 steps/s (collection: 0.978s, learning 0.651s)\n",
      "               Value function loss: 987.1149\n",
      "                    Surrogate loss: -0.0024\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1566.75\n",
      "               Mean episode length: 487.61\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.32\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4317184\n",
      "                    Iteration time: 1.63s\n",
      "                        Total time: 723.86s\n",
      "                               ETA: 651.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 527/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5425 steps/s (collection: 0.784s, learning 0.727s)\n",
      "               Value function loss: 1692.4436\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1580.49\n",
      "               Mean episode length: 486.63\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.21\n",
      "       Mean episode length/episode: 28.54\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4325376\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 725.37s\n",
      "                               ETA: 649.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 528/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5160 steps/s (collection: 0.768s, learning 0.819s)\n",
      "               Value function loss: 647.9038\n",
      "                    Surrogate loss: -0.0076\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1535.30\n",
      "               Mean episode length: 473.39\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.21\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4333568\n",
      "                    Iteration time: 1.59s\n",
      "                        Total time: 726.95s\n",
      "                               ETA: 648.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 529/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6455 steps/s (collection: 0.786s, learning 0.483s)\n",
      "               Value function loss: 844.8094\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1530.05\n",
      "               Mean episode length: 469.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.21\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4341760\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 728.22s\n",
      "                               ETA: 647.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 530/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5595 steps/s (collection: 0.726s, learning 0.738s)\n",
      "               Value function loss: 909.8252\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1537.74\n",
      "               Mean episode length: 470.64\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.25\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4349952\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 729.69s\n",
      "                               ETA: 645.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 531/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5717 steps/s (collection: 0.767s, learning 0.666s)\n",
      "               Value function loss: 833.3725\n",
      "                    Surrogate loss: 0.0015\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1539.86\n",
      "               Mean episode length: 469.68\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.28\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4358144\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 731.12s\n",
      "                               ETA: 644.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 532/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5774 steps/s (collection: 0.833s, learning 0.585s)\n",
      "               Value function loss: 1127.7140\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1535.80\n",
      "               Mean episode length: 469.68\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.22\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4366336\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 732.54s\n",
      "                               ETA: 643.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 533/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5569 steps/s (collection: 0.833s, learning 0.638s)\n",
      "               Value function loss: 813.8471\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1525.42\n",
      "               Mean episode length: 465.35\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.18\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4374528\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 734.01s\n",
      "                               ETA: 641.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 534/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5688 steps/s (collection: 0.780s, learning 0.660s)\n",
      "               Value function loss: 812.4036\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1520.96\n",
      "               Mean episode length: 465.35\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.26\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4382720\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 735.45s\n",
      "                               ETA: 640.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 535/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5787 steps/s (collection: 0.804s, learning 0.611s)\n",
      "               Value function loss: 1095.2340\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1511.03\n",
      "               Mean episode length: 461.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.25\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4390912\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 736.86s\n",
      "                               ETA: 639.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 536/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5117 steps/s (collection: 0.880s, learning 0.721s)\n",
      "               Value function loss: 1027.0363\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1488.24\n",
      "               Mean episode length: 453.71\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.20\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4399104\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 738.46s\n",
      "                               ETA: 638.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 537/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6200 steps/s (collection: 0.832s, learning 0.489s)\n",
      "               Value function loss: 923.8583\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1482.55\n",
      "               Mean episode length: 452.01\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.28\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4407296\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 739.79s\n",
      "                               ETA: 636.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 538/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5794 steps/s (collection: 0.739s, learning 0.674s)\n",
      "               Value function loss: 1142.8334\n",
      "                    Surrogate loss: -0.0010\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1464.37\n",
      "               Mean episode length: 448.02\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.36\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4415488\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 741.20s\n",
      "                               ETA: 635.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 539/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5926 steps/s (collection: 0.831s, learning 0.552s)\n",
      "               Value function loss: 1637.6807\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1483.87\n",
      "               Mean episode length: 455.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.21\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4423680\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 742.58s\n",
      "                               ETA: 633.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 540/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5336 steps/s (collection: 0.859s, learning 0.676s)\n",
      "               Value function loss: 989.0505\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1508.16\n",
      "               Mean episode length: 462.57\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.25\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4431872\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 744.12s\n",
      "                               ETA: 632.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 541/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5493 steps/s (collection: 0.799s, learning 0.692s)\n",
      "               Value function loss: 1326.5772\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1513.52\n",
      "               Mean episode length: 466.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.30\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4440064\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 745.61s\n",
      "                               ETA: 631.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 542/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5003 steps/s (collection: 0.874s, learning 0.763s)\n",
      "               Value function loss: 1143.7441\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1492.10\n",
      "               Mean episode length: 460.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.32\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4448256\n",
      "                    Iteration time: 1.64s\n",
      "                        Total time: 747.25s\n",
      "                               ETA: 630.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 543/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4606 steps/s (collection: 0.843s, learning 0.935s)\n",
      "               Value function loss: 1654.0428\n",
      "                    Surrogate loss: -0.0065\n",
      "             Mean action noise std: 1.03\n",
      "                       Mean reward: 1476.30\n",
      "               Mean episode length: 456.15\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.21\n",
      "       Mean episode length/episode: 28.64\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4456448\n",
      "                    Iteration time: 1.78s\n",
      "                        Total time: 749.02s\n",
      "                               ETA: 629.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 544/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5141 steps/s (collection: 0.835s, learning 0.758s)\n",
      "               Value function loss: 1231.8983\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1479.76\n",
      "               Mean episode length: 456.58\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.22\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4464640\n",
      "                    Iteration time: 1.59s\n",
      "                        Total time: 750.62s\n",
      "                               ETA: 628.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 545/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5086 steps/s (collection: 0.867s, learning 0.744s)\n",
      "               Value function loss: 1190.0855\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1469.86\n",
      "               Mean episode length: 452.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.28\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4472832\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 752.23s\n",
      "                               ETA: 626.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 546/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5184 steps/s (collection: 0.803s, learning 0.777s)\n",
      "               Value function loss: 909.7946\n",
      "                    Surrogate loss: -0.0006\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1478.37\n",
      "               Mean episode length: 455.90\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.29\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4481024\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 753.81s\n",
      "                               ETA: 625.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 547/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5603 steps/s (collection: 0.860s, learning 0.602s)\n",
      "               Value function loss: 705.2485\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1478.54\n",
      "               Mean episode length: 456.18\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.33\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4489216\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 755.27s\n",
      "                               ETA: 624.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 548/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5537 steps/s (collection: 0.729s, learning 0.751s)\n",
      "               Value function loss: 1317.6414\n",
      "                    Surrogate loss: -0.0013\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1503.18\n",
      "               Mean episode length: 463.87\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.21\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4497408\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 756.75s\n",
      "                               ETA: 623.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 549/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5986 steps/s (collection: 0.733s, learning 0.636s)\n",
      "               Value function loss: 880.2204\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1518.95\n",
      "               Mean episode length: 466.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.26\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4505600\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 758.12s\n",
      "                               ETA: 621.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 550/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5619 steps/s (collection: 0.853s, learning 0.604s)\n",
      "               Value function loss: 936.4906\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1523.88\n",
      "               Mean episode length: 466.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.35\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4513792\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 759.58s\n",
      "                               ETA: 620.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 551/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5625 steps/s (collection: 0.804s, learning 0.652s)\n",
      "               Value function loss: 719.3532\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1528.23\n",
      "               Mean episode length: 466.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.35\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4521984\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 761.03s\n",
      "                               ETA: 619.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 552/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5922 steps/s (collection: 0.756s, learning 0.627s)\n",
      "               Value function loss: 890.8649\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1546.26\n",
      "               Mean episode length: 470.05\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.41\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4530176\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 762.41s\n",
      "                               ETA: 617.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 553/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6507 steps/s (collection: 0.744s, learning 0.515s)\n",
      "               Value function loss: 704.9597\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1532.71\n",
      "               Mean episode length: 465.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.42\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4538368\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 763.67s\n",
      "                               ETA: 616.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 554/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5495 steps/s (collection: 0.925s, learning 0.566s)\n",
      "               Value function loss: 1377.7341\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1550.59\n",
      "               Mean episode length: 471.66\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.39\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4546560\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 765.16s\n",
      "                               ETA: 614.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 555/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5535 steps/s (collection: 0.718s, learning 0.762s)\n",
      "               Value function loss: 1189.3338\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1544.02\n",
      "               Mean episode length: 469.65\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.38\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4554752\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 766.64s\n",
      "                               ETA: 613.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 556/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5057 steps/s (collection: 0.890s, learning 0.730s)\n",
      "               Value function loss: 1286.7792\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1540.00\n",
      "               Mean episode length: 470.26\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.46\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4562944\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 768.26s\n",
      "                               ETA: 612.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 557/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6019 steps/s (collection: 0.826s, learning 0.535s)\n",
      "               Value function loss: 1102.7584\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1543.75\n",
      "               Mean episode length: 471.67\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.46\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4571136\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 769.62s\n",
      "                               ETA: 611.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 558/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5140 steps/s (collection: 0.790s, learning 0.804s)\n",
      "               Value function loss: 1773.3888\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1576.43\n",
      "               Mean episode length: 476.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.45\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4579328\n",
      "                    Iteration time: 1.59s\n",
      "                        Total time: 771.22s\n",
      "                               ETA: 609.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 559/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5700 steps/s (collection: 0.841s, learning 0.596s)\n",
      "               Value function loss: 1283.0443\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1598.37\n",
      "               Mean episode length: 479.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.38\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4587520\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 772.66s\n",
      "                               ETA: 608.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 560/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4889 steps/s (collection: 0.827s, learning 0.849s)\n",
      "               Value function loss: 892.9700\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1571.54\n",
      "               Mean episode length: 472.65\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.45\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4595712\n",
      "                    Iteration time: 1.68s\n",
      "                        Total time: 774.33s\n",
      "                               ETA: 607.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 561/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5233 steps/s (collection: 0.778s, learning 0.787s)\n",
      "               Value function loss: 1535.0205\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1575.23\n",
      "               Mean episode length: 472.65\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.50\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4603904\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 775.90s\n",
      "                               ETA: 606.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 562/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5985 steps/s (collection: 0.775s, learning 0.593s)\n",
      "               Value function loss: 1027.5913\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1566.57\n",
      "               Mean episode length: 468.69\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.51\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4612096\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 777.26s\n",
      "                               ETA: 604.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 563/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5276 steps/s (collection: 0.804s, learning 0.748s)\n",
      "               Value function loss: 730.2517\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1554.32\n",
      "               Mean episode length: 465.81\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.50\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4620288\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 778.82s\n",
      "                               ETA: 603.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 564/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5624 steps/s (collection: 0.817s, learning 0.640s)\n",
      "               Value function loss: 1147.3669\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1578.95\n",
      "               Mean episode length: 470.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.45\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4628480\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 780.27s\n",
      "                               ETA: 602.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 565/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5230 steps/s (collection: 0.799s, learning 0.767s)\n",
      "               Value function loss: 1036.6233\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1591.33\n",
      "               Mean episode length: 470.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.48\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4636672\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 781.84s\n",
      "                               ETA: 600.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 566/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5898 steps/s (collection: 0.847s, learning 0.542s)\n",
      "               Value function loss: 1380.9072\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1587.42\n",
      "               Mean episode length: 466.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.49\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4644864\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 783.23s\n",
      "                               ETA: 599.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 567/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5078 steps/s (collection: 0.799s, learning 0.814s)\n",
      "               Value function loss: 1038.7299\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1609.82\n",
      "               Mean episode length: 471.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.48\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4653056\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 784.84s\n",
      "                               ETA: 598.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 568/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6263 steps/s (collection: 0.821s, learning 0.487s)\n",
      "               Value function loss: 1029.9916\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1625.22\n",
      "               Mean episode length: 475.57\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.45\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4661248\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 786.15s\n",
      "                               ETA: 596.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 569/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5411 steps/s (collection: 0.741s, learning 0.773s)\n",
      "               Value function loss: 1070.4579\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1628.19\n",
      "               Mean episode length: 475.57\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.51\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4669440\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 787.66s\n",
      "                               ETA: 595.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 570/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5511 steps/s (collection: 0.834s, learning 0.652s)\n",
      "               Value function loss: 1919.3525\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1613.01\n",
      "               Mean episode length: 470.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.43\n",
      "       Mean episode length/episode: 28.85\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4677632\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 789.15s\n",
      "                               ETA: 594.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 571/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5913 steps/s (collection: 0.716s, learning 0.669s)\n",
      "               Value function loss: 1345.2696\n",
      "                    Surrogate loss: -0.0078\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1614.89\n",
      "               Mean episode length: 470.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.41\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4685824\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 790.53s\n",
      "                               ETA: 592.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 572/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5570 steps/s (collection: 0.774s, learning 0.697s)\n",
      "               Value function loss: 1518.6649\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1617.43\n",
      "               Mean episode length: 469.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.47\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4694016\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 792.01s\n",
      "                               ETA: 591.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 573/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5314 steps/s (collection: 0.877s, learning 0.664s)\n",
      "               Value function loss: 1320.0630\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1574.82\n",
      "               Mean episode length: 457.97\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.47\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4702208\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 793.55s\n",
      "                               ETA: 590.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 574/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5906 steps/s (collection: 0.753s, learning 0.634s)\n",
      "               Value function loss: 2115.5445\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1573.76\n",
      "               Mean episode length: 455.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.39\n",
      "       Mean episode length/episode: 28.54\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4710400\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 794.93s\n",
      "                               ETA: 588.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 575/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6147 steps/s (collection: 0.794s, learning 0.539s)\n",
      "               Value function loss: 1117.2587\n",
      "                    Surrogate loss: -0.0072\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1574.24\n",
      "               Mean episode length: 455.93\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.50\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4718592\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 796.27s\n",
      "                               ETA: 587.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 576/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5478 steps/s (collection: 0.773s, learning 0.722s)\n",
      "               Value function loss: 1264.2796\n",
      "                    Surrogate loss: -0.0067\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1577.02\n",
      "               Mean episode length: 459.31\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.58\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4726784\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 797.76s\n",
      "                               ETA: 586.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 577/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5819 steps/s (collection: 0.798s, learning 0.610s)\n",
      "               Value function loss: 1039.0831\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1598.39\n",
      "               Mean episode length: 463.87\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.57\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4734976\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 799.17s\n",
      "                               ETA: 584.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 578/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5508 steps/s (collection: 0.795s, learning 0.693s)\n",
      "               Value function loss: 1061.3362\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1606.11\n",
      "               Mean episode length: 463.87\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.58\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4743168\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 800.66s\n",
      "                               ETA: 583.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 579/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5894 steps/s (collection: 0.852s, learning 0.538s)\n",
      "               Value function loss: 1536.0083\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1591.64\n",
      "               Mean episode length: 457.46\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.56\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4751360\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 802.05s\n",
      "                               ETA: 582.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 580/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5729 steps/s (collection: 0.784s, learning 0.646s)\n",
      "               Value function loss: 828.9616\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1599.24\n",
      "               Mean episode length: 461.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.52\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4759552\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 803.48s\n",
      "                               ETA: 580.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 581/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5047 steps/s (collection: 0.777s, learning 0.846s)\n",
      "               Value function loss: 1336.7283\n",
      "                    Surrogate loss: -0.0080\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1586.89\n",
      "               Mean episode length: 456.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.48\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4767744\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 805.10s\n",
      "                               ETA: 579.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 582/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6447 steps/s (collection: 0.722s, learning 0.548s)\n",
      "               Value function loss: 1165.3888\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1579.75\n",
      "               Mean episode length: 456.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.44\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4775936\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 806.37s\n",
      "                               ETA: 578.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 583/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6414 steps/s (collection: 0.773s, learning 0.504s)\n",
      "               Value function loss: 1350.6108\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1601.53\n",
      "               Mean episode length: 460.10\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.50\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4784128\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 807.65s\n",
      "                               ETA: 576.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 584/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6045 steps/s (collection: 0.851s, learning 0.504s)\n",
      "               Value function loss: 1085.2849\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1618.21\n",
      "               Mean episode length: 464.26\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.39\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4792320\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 809.00s\n",
      "                               ETA: 575.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 585/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5575 steps/s (collection: 0.826s, learning 0.643s)\n",
      "               Value function loss: 1230.6693\n",
      "                    Surrogate loss: 0.0020\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1638.16\n",
      "               Mean episode length: 467.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.47\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4800512\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 810.47s\n",
      "                               ETA: 574.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 586/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6468 steps/s (collection: 0.767s, learning 0.500s)\n",
      "               Value function loss: 1685.2200\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1637.37\n",
      "               Mean episode length: 467.40\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.45\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4808704\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 811.74s\n",
      "                               ETA: 572.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 587/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5325 steps/s (collection: 0.890s, learning 0.648s)\n",
      "               Value function loss: 1377.0117\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1624.98\n",
      "               Mean episode length: 466.67\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.46\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4816896\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 813.28s\n",
      "                               ETA: 571.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 588/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5194 steps/s (collection: 0.835s, learning 0.742s)\n",
      "               Value function loss: 1686.2862\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1610.41\n",
      "               Mean episode length: 459.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.40\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4825088\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 814.85s\n",
      "                               ETA: 570.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 589/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5577 steps/s (collection: 0.824s, learning 0.645s)\n",
      "               Value function loss: 1876.5313\n",
      "                    Surrogate loss: -0.0084\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1589.07\n",
      "               Mean episode length: 452.87\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.40\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4833280\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 816.32s\n",
      "                               ETA: 568.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 590/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5683 steps/s (collection: 0.775s, learning 0.667s)\n",
      "               Value function loss: 1677.6521\n",
      "                    Surrogate loss: -0.0075\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1561.40\n",
      "               Mean episode length: 446.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.34\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4841472\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 817.76s\n",
      "                               ETA: 567.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 591/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5428 steps/s (collection: 0.878s, learning 0.631s)\n",
      "               Value function loss: 1214.5403\n",
      "                    Surrogate loss: -0.0084\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1570.14\n",
      "               Mean episode length: 451.69\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.38\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4849664\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 819.27s\n",
      "                               ETA: 566.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 592/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5578 steps/s (collection: 0.793s, learning 0.675s)\n",
      "               Value function loss: 1678.9546\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1567.80\n",
      "               Mean episode length: 450.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.48\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4857856\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 820.74s\n",
      "                               ETA: 564.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 593/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5279 steps/s (collection: 0.864s, learning 0.687s)\n",
      "               Value function loss: 1168.9096\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1543.61\n",
      "               Mean episode length: 444.05\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.46\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4866048\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 822.29s\n",
      "                               ETA: 563.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 594/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6414 steps/s (collection: 0.770s, learning 0.507s)\n",
      "               Value function loss: 936.8026\n",
      "                    Surrogate loss: -0.0006\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1539.35\n",
      "               Mean episode length: 446.03\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.61\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4874240\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 823.57s\n",
      "                               ETA: 562.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 595/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5981 steps/s (collection: 0.882s, learning 0.487s)\n",
      "               Value function loss: 1271.6477\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1563.20\n",
      "               Mean episode length: 452.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.52\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4882432\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 824.94s\n",
      "                               ETA: 560.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 596/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5465 steps/s (collection: 0.750s, learning 0.748s)\n",
      "               Value function loss: 1425.4882\n",
      "                    Surrogate loss: -0.0067\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1552.11\n",
      "               Mean episode length: 448.76\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.57\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4890624\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 826.44s\n",
      "                               ETA: 559.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 597/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5418 steps/s (collection: 0.756s, learning 0.756s)\n",
      "               Value function loss: 1251.9940\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1535.33\n",
      "               Mean episode length: 443.36\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.55\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4898816\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 827.95s\n",
      "                               ETA: 558.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 598/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5572 steps/s (collection: 0.810s, learning 0.660s)\n",
      "               Value function loss: 1065.1070\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1540.88\n",
      "               Mean episode length: 443.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.55\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4907008\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 829.42s\n",
      "                               ETA: 556.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 599/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5615 steps/s (collection: 0.798s, learning 0.661s)\n",
      "               Value function loss: 1208.6905\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1536.94\n",
      "               Mean episode length: 443.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.58\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4915200\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 830.88s\n",
      "                               ETA: 555.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 600/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5599 steps/s (collection: 0.798s, learning 0.665s)\n",
      "               Value function loss: 801.6790\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1547.34\n",
      "               Mean episode length: 445.22\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.68\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4923392\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 832.34s\n",
      "                               ETA: 554.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 601/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6115 steps/s (collection: 0.747s, learning 0.593s)\n",
      "               Value function loss: 1589.4743\n",
      "                    Surrogate loss: -0.0076\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1547.26\n",
      "               Mean episode length: 446.69\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.62\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4931584\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 833.68s\n",
      "                               ETA: 552.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 602/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5560 steps/s (collection: 0.769s, learning 0.704s)\n",
      "               Value function loss: 1314.1643\n",
      "                    Surrogate loss: -0.0072\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1559.24\n",
      "               Mean episode length: 449.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.53\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4939776\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 835.15s\n",
      "                               ETA: 551.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 603/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5398 steps/s (collection: 0.719s, learning 0.799s)\n",
      "               Value function loss: 1520.3697\n",
      "                    Surrogate loss: -0.0073\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1571.79\n",
      "               Mean episode length: 449.46\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.47\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4947968\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 836.67s\n",
      "                               ETA: 549.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 604/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5335 steps/s (collection: 0.810s, learning 0.726s)\n",
      "               Value function loss: 1843.5911\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1568.74\n",
      "               Mean episode length: 448.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.47\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4956160\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 838.21s\n",
      "                               ETA: 548.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 605/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5571 steps/s (collection: 0.830s, learning 0.640s)\n",
      "               Value function loss: 2038.0852\n",
      "                    Surrogate loss: -0.0072\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1544.37\n",
      "               Mean episode length: 441.96\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.54\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4964352\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 839.68s\n",
      "                               ETA: 547.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 606/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5154 steps/s (collection: 0.931s, learning 0.658s)\n",
      "               Value function loss: 1122.3380\n",
      "                    Surrogate loss: -0.0080\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1520.60\n",
      "               Mean episode length: 437.56\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.56\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4972544\n",
      "                    Iteration time: 1.59s\n",
      "                        Total time: 841.27s\n",
      "                               ETA: 546.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 607/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5587 steps/s (collection: 0.751s, learning 0.715s)\n",
      "               Value function loss: 1209.9426\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1551.27\n",
      "               Mean episode length: 444.58\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.58\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4980736\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 842.73s\n",
      "                               ETA: 544.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 608/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5910 steps/s (collection: 0.782s, learning 0.604s)\n",
      "               Value function loss: 1478.9502\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1512.29\n",
      "               Mean episode length: 435.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.57\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4988928\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 844.12s\n",
      "                               ETA: 543.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 609/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5086 steps/s (collection: 0.782s, learning 0.828s)\n",
      "               Value function loss: 1285.8284\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1520.56\n",
      "               Mean episode length: 436.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.63\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 4997120\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 845.73s\n",
      "                               ETA: 542.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 610/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5852 steps/s (collection: 0.823s, learning 0.577s)\n",
      "               Value function loss: 992.5555\n",
      "                    Surrogate loss: -0.0076\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1537.04\n",
      "               Mean episode length: 438.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.61\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5005312\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 847.13s\n",
      "                               ETA: 540.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 611/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6254 steps/s (collection: 0.776s, learning 0.534s)\n",
      "               Value function loss: 1339.4218\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1504.40\n",
      "               Mean episode length: 427.24\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.65\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5013504\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 848.44s\n",
      "                               ETA: 539.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 612/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5117 steps/s (collection: 0.848s, learning 0.753s)\n",
      "               Value function loss: 1403.9936\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1519.20\n",
      "               Mean episode length: 431.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.71\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5021696\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 850.04s\n",
      "                               ETA: 538.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 613/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5320 steps/s (collection: 0.789s, learning 0.751s)\n",
      "               Value function loss: 1588.9894\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1522.84\n",
      "               Mean episode length: 429.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.75\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5029888\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 851.58s\n",
      "                               ETA: 536.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 614/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5721 steps/s (collection: 0.761s, learning 0.671s)\n",
      "               Value function loss: 1247.3613\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1511.24\n",
      "               Mean episode length: 425.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.70\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5038080\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 853.01s\n",
      "                               ETA: 535.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 615/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5771 steps/s (collection: 0.836s, learning 0.583s)\n",
      "               Value function loss: 976.7308\n",
      "                    Surrogate loss: 0.0005\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1513.44\n",
      "               Mean episode length: 426.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.67\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5046272\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 854.43s\n",
      "                               ETA: 534.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 616/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5114 steps/s (collection: 0.804s, learning 0.797s)\n",
      "               Value function loss: 1368.7745\n",
      "                    Surrogate loss: -0.0068\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1495.19\n",
      "               Mean episode length: 416.46\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.70\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5054464\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 856.03s\n",
      "                               ETA: 532.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 617/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5441 steps/s (collection: 0.875s, learning 0.630s)\n",
      "               Value function loss: 1931.8531\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1542.02\n",
      "               Mean episode length: 425.72\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.56\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5062656\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 857.54s\n",
      "                               ETA: 531.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 618/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6638 steps/s (collection: 0.723s, learning 0.511s)\n",
      "               Value function loss: 1139.5267\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1547.54\n",
      "               Mean episode length: 425.14\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.61\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5070848\n",
      "                    Iteration time: 1.23s\n",
      "                        Total time: 858.77s\n",
      "                               ETA: 530.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 619/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5050 steps/s (collection: 0.977s, learning 0.645s)\n",
      "               Value function loss: 1415.0075\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1582.75\n",
      "               Mean episode length: 432.90\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.55\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5079040\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 860.39s\n",
      "                               ETA: 528.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 620/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5825 steps/s (collection: 0.809s, learning 0.597s)\n",
      "               Value function loss: 1802.6297\n",
      "                    Surrogate loss: -0.0072\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1608.66\n",
      "               Mean episode length: 438.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.53\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5087232\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 861.80s\n",
      "                               ETA: 527.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 621/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6175 steps/s (collection: 0.757s, learning 0.570s)\n",
      "               Value function loss: 2223.1555\n",
      "                    Surrogate loss: -0.0067\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1628.51\n",
      "               Mean episode length: 445.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.52\n",
      "       Mean episode length/episode: 28.74\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5095424\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 863.13s\n",
      "                               ETA: 525.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 622/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5418 steps/s (collection: 0.880s, learning 0.632s)\n",
      "               Value function loss: 1211.2386\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1625.27\n",
      "               Mean episode length: 444.88\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.58\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5103616\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 864.64s\n",
      "                               ETA: 524.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 623/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6002 steps/s (collection: 0.783s, learning 0.582s)\n",
      "               Value function loss: 1260.7638\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1640.87\n",
      "               Mean episode length: 447.77\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.56\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5111808\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 866.00s\n",
      "                               ETA: 523.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 624/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5919 steps/s (collection: 0.784s, learning 0.600s)\n",
      "               Value function loss: 1459.9638\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1670.22\n",
      "               Mean episode length: 456.45\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.61\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5120000\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 867.39s\n",
      "                               ETA: 521.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 625/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5022 steps/s (collection: 0.902s, learning 0.729s)\n",
      "               Value function loss: 958.5809\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1635.88\n",
      "               Mean episode length: 447.64\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.63\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5128192\n",
      "                    Iteration time: 1.63s\n",
      "                        Total time: 869.02s\n",
      "                               ETA: 520.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 626/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6214 steps/s (collection: 0.765s, learning 0.553s)\n",
      "               Value function loss: 1537.1133\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1670.17\n",
      "               Mean episode length: 460.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.64\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5136384\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 870.34s\n",
      "                               ETA: 519.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 627/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5518 steps/s (collection: 0.886s, learning 0.599s)\n",
      "               Value function loss: 922.8508\n",
      "                    Surrogate loss: -0.0065\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1638.33\n",
      "               Mean episode length: 453.61\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.72\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5144576\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 871.82s\n",
      "                               ETA: 517.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 628/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5403 steps/s (collection: 0.820s, learning 0.697s)\n",
      "               Value function loss: 1286.1236\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1632.69\n",
      "               Mean episode length: 451.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.73\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5152768\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 873.34s\n",
      "                               ETA: 516.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 629/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5955 steps/s (collection: 0.741s, learning 0.635s)\n",
      "               Value function loss: 1599.6543\n",
      "                    Surrogate loss: -0.0067\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1617.16\n",
      "               Mean episode length: 446.78\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.71\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5160960\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 874.71s\n",
      "                               ETA: 515.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 630/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5686 steps/s (collection: 0.837s, learning 0.604s)\n",
      "               Value function loss: 1149.5362\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1589.81\n",
      "               Mean episode length: 443.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.70\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5169152\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 876.15s\n",
      "                               ETA: 513.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 631/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5688 steps/s (collection: 0.832s, learning 0.608s)\n",
      "               Value function loss: 1194.9233\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1583.29\n",
      "               Mean episode length: 441.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.72\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5177344\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 877.59s\n",
      "                               ETA: 512.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 632/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6353 steps/s (collection: 0.751s, learning 0.538s)\n",
      "               Value function loss: 1299.3653\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1604.21\n",
      "               Mean episode length: 446.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.68\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5185536\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 878.88s\n",
      "                               ETA: 510.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 633/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6195 steps/s (collection: 0.854s, learning 0.468s)\n",
      "               Value function loss: 1928.9215\n",
      "                    Surrogate loss: -0.0071\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1603.65\n",
      "               Mean episode length: 446.26\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.71\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5193728\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 880.20s\n",
      "                               ETA: 509.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 634/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5436 steps/s (collection: 0.798s, learning 0.709s)\n",
      "               Value function loss: 1330.5664\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1616.76\n",
      "               Mean episode length: 449.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.79\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5201920\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 881.71s\n",
      "                               ETA: 508.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 635/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6351 steps/s (collection: 0.781s, learning 0.509s)\n",
      "               Value function loss: 1773.9705\n",
      "                    Surrogate loss: -0.0069\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1605.09\n",
      "               Mean episode length: 445.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.73\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5210112\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 883.00s\n",
      "                               ETA: 506.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 636/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6029 steps/s (collection: 0.845s, learning 0.514s)\n",
      "               Value function loss: 2283.8358\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1605.25\n",
      "               Mean episode length: 441.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.63\n",
      "       Mean episode length/episode: 28.85\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5218304\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 884.36s\n",
      "                               ETA: 505.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 637/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5792 steps/s (collection: 0.802s, learning 0.612s)\n",
      "               Value function loss: 1708.8025\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1641.68\n",
      "               Mean episode length: 447.79\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.58\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5226496\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 885.77s\n",
      "                               ETA: 504.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 638/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5979 steps/s (collection: 0.757s, learning 0.613s)\n",
      "               Value function loss: 1446.9389\n",
      "                    Surrogate loss: -0.0074\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1656.59\n",
      "               Mean episode length: 451.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.72\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5234688\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 887.14s\n",
      "                               ETA: 502.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 639/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5980 steps/s (collection: 0.874s, learning 0.496s)\n",
      "               Value function loss: 1752.0344\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1637.43\n",
      "               Mean episode length: 446.18\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.69\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5242880\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 888.51s\n",
      "                               ETA: 501.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 640/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5732 steps/s (collection: 0.783s, learning 0.646s)\n",
      "               Value function loss: 1140.4158\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1640.13\n",
      "               Mean episode length: 445.68\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.72\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5251072\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 889.94s\n",
      "                               ETA: 499.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 641/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5004 steps/s (collection: 0.778s, learning 0.859s)\n",
      "               Value function loss: 1269.4027\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1642.09\n",
      "               Mean episode length: 444.91\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.75\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5259264\n",
      "                    Iteration time: 1.64s\n",
      "                        Total time: 891.58s\n",
      "                               ETA: 498.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 642/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6444 steps/s (collection: 0.804s, learning 0.468s)\n",
      "               Value function loss: 1595.7981\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1607.78\n",
      "               Mean episode length: 436.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.65\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5267456\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 892.85s\n",
      "                               ETA: 497.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 643/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5131 steps/s (collection: 0.867s, learning 0.729s)\n",
      "               Value function loss: 1616.8201\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1611.31\n",
      "               Mean episode length: 436.14\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.63\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5275648\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 894.45s\n",
      "                               ETA: 495.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 644/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5286 steps/s (collection: 0.762s, learning 0.788s)\n",
      "               Value function loss: 1345.6876\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1587.61\n",
      "               Mean episode length: 429.64\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.66\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5283840\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 896.00s\n",
      "                               ETA: 494.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 645/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6256 steps/s (collection: 0.795s, learning 0.514s)\n",
      "               Value function loss: 1239.2859\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1602.86\n",
      "               Mean episode length: 433.78\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.68\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5292032\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 897.31s\n",
      "                               ETA: 493.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 646/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5586 steps/s (collection: 0.829s, learning 0.638s)\n",
      "               Value function loss: 1350.1485\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1551.59\n",
      "               Mean episode length: 419.70\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.72\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5300224\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 898.77s\n",
      "                               ETA: 491.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 647/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5062 steps/s (collection: 0.790s, learning 0.828s)\n",
      "               Value function loss: 1055.1534\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1573.66\n",
      "               Mean episode length: 426.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.72\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5308416\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 900.39s\n",
      "                               ETA: 490.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 648/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5779 steps/s (collection: 0.794s, learning 0.623s)\n",
      "               Value function loss: 1723.3947\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1526.34\n",
      "               Mean episode length: 417.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.71\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5316608\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 901.81s\n",
      "                               ETA: 489.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 649/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6549 steps/s (collection: 0.792s, learning 0.459s)\n",
      "               Value function loss: 1130.1191\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1547.56\n",
      "               Mean episode length: 422.08\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.68\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5324800\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 903.06s\n",
      "                               ETA: 487.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 650/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5340 steps/s (collection: 0.875s, learning 0.659s)\n",
      "               Value function loss: 1663.1483\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1550.51\n",
      "               Mean episode length: 424.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.72\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5332992\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 904.59s\n",
      "                               ETA: 486.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 651/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5500 steps/s (collection: 0.779s, learning 0.711s)\n",
      "               Value function loss: 1653.7529\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1582.70\n",
      "               Mean episode length: 431.02\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.67\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5341184\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 906.08s\n",
      "                               ETA: 485.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 652/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5980 steps/s (collection: 0.858s, learning 0.512s)\n",
      "               Value function loss: 2467.6518\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1631.19\n",
      "               Mean episode length: 442.15\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.66\n",
      "       Mean episode length/episode: 28.95\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5349376\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 907.45s\n",
      "                               ETA: 483.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 653/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5187 steps/s (collection: 0.871s, learning 0.709s)\n",
      "               Value function loss: 992.9721\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1635.83\n",
      "               Mean episode length: 443.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.64\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5357568\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 909.03s\n",
      "                               ETA: 482.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 654/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6092 steps/s (collection: 0.750s, learning 0.595s)\n",
      "               Value function loss: 1324.8826\n",
      "                    Surrogate loss: 0.0021\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1646.62\n",
      "               Mean episode length: 446.56\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.70\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5365760\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 910.38s\n",
      "                               ETA: 480.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 655/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5394 steps/s (collection: 0.861s, learning 0.658s)\n",
      "               Value function loss: 1564.1903\n",
      "                    Surrogate loss: 0.0000\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1654.62\n",
      "               Mean episode length: 448.67\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.67\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5373952\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 911.89s\n",
      "                               ETA: 479.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 656/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6020 steps/s (collection: 0.809s, learning 0.552s)\n",
      "               Value function loss: 1611.7999\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1673.50\n",
      "               Mean episode length: 453.04\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.71\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5382144\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 913.26s\n",
      "                               ETA: 478.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 657/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5557 steps/s (collection: 0.787s, learning 0.687s)\n",
      "               Value function loss: 1014.9153\n",
      "                    Surrogate loss: -0.0075\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1684.12\n",
      "               Mean episode length: 457.45\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.68\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5390336\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 914.73s\n",
      "                               ETA: 476.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 658/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5540 steps/s (collection: 0.849s, learning 0.629s)\n",
      "               Value function loss: 1388.6388\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1720.04\n",
      "               Mean episode length: 465.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.65\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5398528\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 916.21s\n",
      "                               ETA: 475.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 659/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5929 steps/s (collection: 0.832s, learning 0.549s)\n",
      "               Value function loss: 1548.0108\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1739.02\n",
      "               Mean episode length: 468.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.71\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5406720\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 917.59s\n",
      "                               ETA: 474.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 660/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4785 steps/s (collection: 0.804s, learning 0.908s)\n",
      "               Value function loss: 1304.0885\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1724.12\n",
      "               Mean episode length: 464.33\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.75\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5414912\n",
      "                    Iteration time: 1.71s\n",
      "                        Total time: 919.30s\n",
      "                               ETA: 472.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 661/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5908 steps/s (collection: 0.800s, learning 0.587s)\n",
      "               Value function loss: 1275.0632\n",
      "                    Surrogate loss: -0.0073\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1739.76\n",
      "               Mean episode length: 467.57\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.78\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5423104\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 920.69s\n",
      "                               ETA: 471.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 662/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6029 steps/s (collection: 0.768s, learning 0.591s)\n",
      "               Value function loss: 1478.7353\n",
      "                    Surrogate loss: -0.0073\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1736.21\n",
      "               Mean episode length: 467.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.83\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5431296\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 922.05s\n",
      "                               ETA: 470.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 663/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5563 steps/s (collection: 0.876s, learning 0.596s)\n",
      "               Value function loss: 1797.2432\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.04\n",
      "                       Mean reward: 1726.23\n",
      "               Mean episode length: 463.08\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.82\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5439488\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 923.52s\n",
      "                               ETA: 468.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 664/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5917 steps/s (collection: 0.808s, learning 0.576s)\n",
      "               Value function loss: 2225.9671\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1734.39\n",
      "               Mean episode length: 463.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.67\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5447680\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 924.90s\n",
      "                               ETA: 467.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 665/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5496 steps/s (collection: 0.863s, learning 0.627s)\n",
      "               Value function loss: 1205.2931\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1745.82\n",
      "               Mean episode length: 468.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.72\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5455872\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 926.39s\n",
      "                               ETA: 466.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 666/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5041 steps/s (collection: 0.788s, learning 0.837s)\n",
      "               Value function loss: 1861.1242\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1753.65\n",
      "               Mean episode length: 469.14\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.72\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5464064\n",
      "                    Iteration time: 1.63s\n",
      "                        Total time: 928.02s\n",
      "                               ETA: 464.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 667/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4962 steps/s (collection: 0.860s, learning 0.791s)\n",
      "               Value function loss: 1575.8213\n",
      "                    Surrogate loss: -0.0076\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1747.36\n",
      "               Mean episode length: 469.58\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.71\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5472256\n",
      "                    Iteration time: 1.65s\n",
      "                        Total time: 929.67s\n",
      "                               ETA: 463.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 668/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5499 steps/s (collection: 0.890s, learning 0.599s)\n",
      "               Value function loss: 2095.5647\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1728.24\n",
      "               Mean episode length: 466.74\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.67\n",
      "       Mean episode length/episode: 28.74\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5480448\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 931.16s\n",
      "                               ETA: 462.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 669/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5807 steps/s (collection: 0.860s, learning 0.550s)\n",
      "               Value function loss: 1443.7060\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1733.35\n",
      "               Mean episode length: 466.74\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.82\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5488640\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 932.57s\n",
      "                               ETA: 460.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 670/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4891 steps/s (collection: 0.805s, learning 0.870s)\n",
      "               Value function loss: 2121.7590\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1741.52\n",
      "               Mean episode length: 465.96\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.80\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5496832\n",
      "                    Iteration time: 1.67s\n",
      "                        Total time: 934.24s\n",
      "                               ETA: 459.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 671/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5831 steps/s (collection: 0.813s, learning 0.591s)\n",
      "               Value function loss: 1693.0109\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1754.59\n",
      "               Mean episode length: 469.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.77\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5505024\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 935.65s\n",
      "                               ETA: 458.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 672/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5715 steps/s (collection: 0.722s, learning 0.711s)\n",
      "               Value function loss: 1498.8368\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1751.27\n",
      "               Mean episode length: 467.45\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.77\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5513216\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 937.08s\n",
      "                               ETA: 456.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 673/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5624 steps/s (collection: 0.866s, learning 0.591s)\n",
      "               Value function loss: 1609.3963\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1725.27\n",
      "               Mean episode length: 464.60\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.85\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5521408\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 938.54s\n",
      "                               ETA: 455.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 674/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5819 steps/s (collection: 0.760s, learning 0.647s)\n",
      "               Value function loss: 1506.1391\n",
      "                    Surrogate loss: -0.0076\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1713.53\n",
      "               Mean episode length: 462.87\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.78\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5529600\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 939.95s\n",
      "                               ETA: 454.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 675/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5648 steps/s (collection: 0.855s, learning 0.595s)\n",
      "               Value function loss: 1279.2873\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1713.95\n",
      "               Mean episode length: 463.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.81\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5537792\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 941.40s\n",
      "                               ETA: 452.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 676/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5256 steps/s (collection: 0.834s, learning 0.725s)\n",
      "               Value function loss: 1227.0873\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1705.29\n",
      "               Mean episode length: 460.20\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.79\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5545984\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 942.96s\n",
      "                               ETA: 451.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 677/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5684 steps/s (collection: 0.772s, learning 0.669s)\n",
      "               Value function loss: 1814.1961\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1700.40\n",
      "               Mean episode length: 459.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.86\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5554176\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 944.40s\n",
      "                               ETA: 449.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 678/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6137 steps/s (collection: 0.778s, learning 0.557s)\n",
      "               Value function loss: 1166.6731\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1704.83\n",
      "               Mean episode length: 458.08\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.85\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5562368\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 945.73s\n",
      "                               ETA: 448.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 679/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5966 steps/s (collection: 0.855s, learning 0.518s)\n",
      "               Value function loss: 1949.1958\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1707.98\n",
      "               Mean episode length: 455.73\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.90\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5570560\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 947.10s\n",
      "                               ETA: 447.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 680/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4805 steps/s (collection: 0.816s, learning 0.889s)\n",
      "               Value function loss: 1875.4017\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1738.40\n",
      "               Mean episode length: 462.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.83\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5578752\n",
      "                    Iteration time: 1.70s\n",
      "                        Total time: 948.81s\n",
      "                               ETA: 445.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 681/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6477 steps/s (collection: 0.773s, learning 0.491s)\n",
      "               Value function loss: 1537.8777\n",
      "                    Surrogate loss: -0.0068\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1698.00\n",
      "               Mean episode length: 455.44\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.87\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5586944\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 950.07s\n",
      "                               ETA: 444.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 682/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5632 steps/s (collection: 0.770s, learning 0.684s)\n",
      "               Value function loss: 1629.2554\n",
      "                    Surrogate loss: -0.0072\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1732.53\n",
      "               Mean episode length: 460.52\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.76\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5595136\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 951.53s\n",
      "                               ETA: 443.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 683/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5379 steps/s (collection: 0.875s, learning 0.648s)\n",
      "               Value function loss: 2014.1697\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1748.61\n",
      "               Mean episode length: 460.35\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.77\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5603328\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 953.05s\n",
      "                               ETA: 441.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 684/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6434 steps/s (collection: 0.802s, learning 0.471s)\n",
      "               Value function loss: 1527.0563\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1786.89\n",
      "               Mean episode length: 466.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.69\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5611520\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 954.32s\n",
      "                               ETA: 440.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 685/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5222 steps/s (collection: 0.869s, learning 0.700s)\n",
      "               Value function loss: 1724.1536\n",
      "                    Surrogate loss: -0.0065\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1800.84\n",
      "               Mean episode length: 467.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.76\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5619712\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 955.89s\n",
      "                               ETA: 438.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 686/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5254 steps/s (collection: 0.844s, learning 0.716s)\n",
      "               Value function loss: 2459.0802\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1780.83\n",
      "               Mean episode length: 461.45\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.62\n",
      "       Mean episode length/episode: 28.54\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5627904\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 957.45s\n",
      "                               ETA: 437.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 687/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5372 steps/s (collection: 0.897s, learning 0.628s)\n",
      "               Value function loss: 1787.4058\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1732.09\n",
      "               Mean episode length: 450.20\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.67\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5636096\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 958.98s\n",
      "                               ETA: 436.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 688/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5215 steps/s (collection: 0.837s, learning 0.734s)\n",
      "               Value function loss: 1420.1960\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1721.25\n",
      "               Mean episode length: 449.08\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.75\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5644288\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 960.55s\n",
      "                               ETA: 435.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 689/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5839 steps/s (collection: 0.847s, learning 0.555s)\n",
      "               Value function loss: 1655.4545\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1718.66\n",
      "               Mean episode length: 449.34\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.69\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5652480\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 961.95s\n",
      "                               ETA: 433.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 690/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5848 steps/s (collection: 0.781s, learning 0.619s)\n",
      "               Value function loss: 1465.6312\n",
      "                    Surrogate loss: -0.0065\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1697.37\n",
      "               Mean episode length: 444.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.57\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5660672\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 963.35s\n",
      "                               ETA: 432.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 691/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5678 steps/s (collection: 0.863s, learning 0.580s)\n",
      "               Value function loss: 1562.6409\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1702.45\n",
      "               Mean episode length: 445.65\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.54\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5668864\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 964.79s\n",
      "                               ETA: 430.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 692/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6264 steps/s (collection: 0.779s, learning 0.528s)\n",
      "               Value function loss: 929.0755\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1694.09\n",
      "               Mean episode length: 445.65\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.58\n",
      "       Mean episode length/episode: 31.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5677056\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 966.10s\n",
      "                               ETA: 429.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 693/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5173 steps/s (collection: 0.868s, learning 0.715s)\n",
      "               Value function loss: 1523.3093\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1670.21\n",
      "               Mean episode length: 445.65\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.67\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5685248\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 967.68s\n",
      "                               ETA: 428.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 694/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5422 steps/s (collection: 0.792s, learning 0.719s)\n",
      "               Value function loss: 1633.9611\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1679.75\n",
      "               Mean episode length: 448.98\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.68\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5693440\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 969.19s\n",
      "                               ETA: 426.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 695/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5511 steps/s (collection: 0.760s, learning 0.726s)\n",
      "               Value function loss: 2015.0122\n",
      "                    Surrogate loss: -0.0070\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1658.25\n",
      "               Mean episode length: 445.35\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.65\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5701632\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 970.68s\n",
      "                               ETA: 425.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 696/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5300 steps/s (collection: 0.745s, learning 0.800s)\n",
      "               Value function loss: 1098.0426\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1639.02\n",
      "               Mean episode length: 444.43\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.75\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5709824\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 972.23s\n",
      "                               ETA: 424.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 697/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5640 steps/s (collection: 0.832s, learning 0.621s)\n",
      "               Value function loss: 1698.0430\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1636.45\n",
      "               Mean episode length: 446.58\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.64\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5718016\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 973.68s\n",
      "                               ETA: 422.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 698/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5351 steps/s (collection: 0.799s, learning 0.732s)\n",
      "               Value function loss: 1281.7926\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1662.73\n",
      "               Mean episode length: 454.11\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.71\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5726208\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 975.21s\n",
      "                               ETA: 421.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 699/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5666 steps/s (collection: 0.823s, learning 0.623s)\n",
      "               Value function loss: 2335.5016\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1716.66\n",
      "               Mean episode length: 465.95\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.73\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5734400\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 976.66s\n",
      "                               ETA: 420.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 700/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4831 steps/s (collection: 0.807s, learning 0.888s)\n",
      "               Value function loss: 1124.3735\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1727.55\n",
      "               Mean episode length: 467.46\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.76\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5742592\n",
      "                    Iteration time: 1.70s\n",
      "                        Total time: 978.35s\n",
      "                               ETA: 418.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 701/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6077 steps/s (collection: 0.803s, learning 0.544s)\n",
      "               Value function loss: 1770.3165\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1729.71\n",
      "               Mean episode length: 469.56\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.81\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5750784\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 979.70s\n",
      "                               ETA: 417.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 702/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6317 steps/s (collection: 0.804s, learning 0.493s)\n",
      "               Value function loss: 1968.6047\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1747.85\n",
      "               Mean episode length: 472.33\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.83\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5758976\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 981.00s\n",
      "                               ETA: 415.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 703/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5354 steps/s (collection: 0.796s, learning 0.734s)\n",
      "               Value function loss: 1913.6480\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1769.64\n",
      "               Mean episode length: 478.83\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.81\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5767168\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 982.53s\n",
      "                               ETA: 414.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 704/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5884 steps/s (collection: 0.788s, learning 0.604s)\n",
      "               Value function loss: 1308.6407\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1748.30\n",
      "               Mean episode length: 474.39\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.77\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5775360\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 983.92s\n",
      "                               ETA: 413.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 705/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5753 steps/s (collection: 0.778s, learning 0.646s)\n",
      "               Value function loss: 1450.7884\n",
      "                    Surrogate loss: -0.0063\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1729.64\n",
      "               Mean episode length: 471.11\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.81\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5783552\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 985.34s\n",
      "                               ETA: 411.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 706/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5251 steps/s (collection: 0.907s, learning 0.653s)\n",
      "               Value function loss: 1621.4314\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1730.85\n",
      "               Mean episode length: 470.33\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.87\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5791744\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 986.90s\n",
      "                               ETA: 410.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 707/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5784 steps/s (collection: 0.779s, learning 0.637s)\n",
      "               Value function loss: 1234.7046\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1750.36\n",
      "               Mean episode length: 473.33\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.00\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5799936\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 988.32s\n",
      "                               ETA: 409.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 708/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5189 steps/s (collection: 0.854s, learning 0.725s)\n",
      "               Value function loss: 1545.8400\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1756.78\n",
      "               Mean episode length: 473.33\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.04\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5808128\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 989.90s\n",
      "                               ETA: 407.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 709/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5424 steps/s (collection: 0.810s, learning 0.701s)\n",
      "               Value function loss: 1379.0022\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1761.26\n",
      "               Mean episode length: 475.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.95\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5816320\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 991.41s\n",
      "                               ETA: 406.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 710/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5548 steps/s (collection: 0.866s, learning 0.611s)\n",
      "               Value function loss: 1980.3738\n",
      "                    Surrogate loss: -0.0068\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1788.21\n",
      "               Mean episode length: 479.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.93\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5824512\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 992.88s\n",
      "                               ETA: 405.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 711/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5044 steps/s (collection: 0.819s, learning 0.805s)\n",
      "               Value function loss: 1880.4155\n",
      "                    Surrogate loss: -0.0068\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1783.59\n",
      "               Mean episode length: 479.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.83\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5832704\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 994.51s\n",
      "                               ETA: 403.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 712/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5911 steps/s (collection: 0.832s, learning 0.554s)\n",
      "               Value function loss: 1350.7704\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1796.47\n",
      "               Mean episode length: 482.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.95\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5840896\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 995.89s\n",
      "                               ETA: 402.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 713/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5779 steps/s (collection: 0.798s, learning 0.619s)\n",
      "               Value function loss: 1974.8109\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1814.47\n",
      "               Mean episode length: 482.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.98\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5849088\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 997.31s\n",
      "                               ETA: 400.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 714/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5105 steps/s (collection: 0.915s, learning 0.690s)\n",
      "               Value function loss: 1880.6477\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1843.35\n",
      "               Mean episode length: 486.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.99\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5857280\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 998.91s\n",
      "                               ETA: 399.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 715/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4797 steps/s (collection: 0.843s, learning 0.865s)\n",
      "               Value function loss: 1751.6053\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1849.05\n",
      "               Mean episode length: 486.62\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.96\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5865472\n",
      "                    Iteration time: 1.71s\n",
      "                        Total time: 1000.62s\n",
      "                               ETA: 398.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 716/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6351 steps/s (collection: 0.745s, learning 0.544s)\n",
      "               Value function loss: 1386.7669\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1863.14\n",
      "               Mean episode length: 491.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.00\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5873664\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 1001.91s\n",
      "                               ETA: 396.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 717/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5988 steps/s (collection: 0.880s, learning 0.488s)\n",
      "               Value function loss: 2038.7809\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1885.89\n",
      "               Mean episode length: 495.59\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.96\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5881856\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 1003.28s\n",
      "                               ETA: 395.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 718/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5439 steps/s (collection: 0.770s, learning 0.736s)\n",
      "               Value function loss: 1801.5581\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1895.74\n",
      "               Mean episode length: 495.59\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.93\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5890048\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 1004.79s\n",
      "                               ETA: 394.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 719/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5533 steps/s (collection: 0.831s, learning 0.650s)\n",
      "               Value function loss: 1897.5382\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1935.56\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.92\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5898240\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 1006.27s\n",
      "                               ETA: 392.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 720/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5463 steps/s (collection: 0.791s, learning 0.708s)\n",
      "               Value function loss: 1595.0996\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1956.06\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.88\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5906432\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1007.77s\n",
      "                               ETA: 391.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 721/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5269 steps/s (collection: 0.902s, learning 0.653s)\n",
      "               Value function loss: 1846.6397\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1966.05\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.97\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5914624\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 1009.32s\n",
      "                               ETA: 390.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 722/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6388 steps/s (collection: 0.720s, learning 0.562s)\n",
      "               Value function loss: 1326.6435\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1974.57\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.96\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5922816\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 1010.60s\n",
      "                               ETA: 388.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 723/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5727 steps/s (collection: 0.871s, learning 0.559s)\n",
      "               Value function loss: 1135.8432\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1981.81\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.99\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5931008\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 1012.03s\n",
      "                               ETA: 387.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 724/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5791 steps/s (collection: 0.747s, learning 0.668s)\n",
      "               Value function loss: 1614.7324\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1964.74\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.06\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5939200\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 1013.45s\n",
      "                               ETA: 385.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 725/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6422 steps/s (collection: 0.753s, learning 0.523s)\n",
      "               Value function loss: 832.7632\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1960.51\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.06\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5947392\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 1014.72s\n",
      "                               ETA: 384.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 726/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5237 steps/s (collection: 0.907s, learning 0.657s)\n",
      "               Value function loss: 2484.3173\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1959.31\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.05\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5955584\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 1016.29s\n",
      "                               ETA: 383.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 727/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5642 steps/s (collection: 0.813s, learning 0.639s)\n",
      "               Value function loss: 1523.5970\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1967.76\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.01\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5963776\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1017.74s\n",
      "                               ETA: 381.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 728/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5611 steps/s (collection: 0.892s, learning 0.567s)\n",
      "               Value function loss: 2044.6647\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1982.54\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.17\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5971968\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1019.20s\n",
      "                               ETA: 380.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 729/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5662 steps/s (collection: 0.821s, learning 0.626s)\n",
      "               Value function loss: 1883.4068\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1988.21\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.06\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5980160\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1020.64s\n",
      "                               ETA: 378.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 730/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5779 steps/s (collection: 0.866s, learning 0.552s)\n",
      "               Value function loss: 2345.2945\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1990.93\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.01\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5988352\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 1022.06s\n",
      "                               ETA: 377.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 731/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6793 steps/s (collection: 0.735s, learning 0.471s)\n",
      "               Value function loss: 1213.0431\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1987.36\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.94\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 5996544\n",
      "                    Iteration time: 1.21s\n",
      "                        Total time: 1023.27s\n",
      "                               ETA: 376.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 732/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5524 steps/s (collection: 0.922s, learning 0.560s)\n",
      "               Value function loss: 1769.8881\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1978.48\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.13\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6004736\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 1024.75s\n",
      "                               ETA: 374.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 733/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5942 steps/s (collection: 0.794s, learning 0.585s)\n",
      "               Value function loss: 2371.2127\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1962.75\n",
      "               Mean episode length: 495.85\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.92\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6012928\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 1026.13s\n",
      "                               ETA: 373.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 734/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5239 steps/s (collection: 0.919s, learning 0.645s)\n",
      "               Value function loss: 1891.2799\n",
      "                    Surrogate loss: -0.0066\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1946.32\n",
      "               Mean episode length: 492.45\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.90\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6021120\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 1027.69s\n",
      "                               ETA: 371.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 735/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6251 steps/s (collection: 0.773s, learning 0.538s)\n",
      "               Value function loss: 1291.7058\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1962.94\n",
      "               Mean episode length: 492.45\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.89\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6029312\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 1029.00s\n",
      "                               ETA: 370.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 736/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5232 steps/s (collection: 0.936s, learning 0.629s)\n",
      "               Value function loss: 2139.7167\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1949.72\n",
      "               Mean episode length: 487.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 3.97\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6037504\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 1030.57s\n",
      "                               ETA: 369.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 737/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5527 steps/s (collection: 0.777s, learning 0.705s)\n",
      "               Value function loss: 1809.3016\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1931.61\n",
      "               Mean episode length: 484.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.02\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6045696\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 1032.05s\n",
      "                               ETA: 367.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 738/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6406 steps/s (collection: 0.745s, learning 0.534s)\n",
      "               Value function loss: 1947.1960\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1942.73\n",
      "               Mean episode length: 484.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.09\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6053888\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 1033.33s\n",
      "                               ETA: 366.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 739/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5322 steps/s (collection: 0.876s, learning 0.663s)\n",
      "               Value function loss: 1375.0330\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1951.75\n",
      "               Mean episode length: 484.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.06\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6062080\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 1034.87s\n",
      "                               ETA: 365.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 740/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6115 steps/s (collection: 0.788s, learning 0.551s)\n",
      "               Value function loss: 1531.9106\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1953.42\n",
      "               Mean episode length: 484.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.14\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6070272\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 1036.21s\n",
      "                               ETA: 363.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 741/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5677 steps/s (collection: 0.846s, learning 0.596s)\n",
      "               Value function loss: 1535.1937\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1937.67\n",
      "               Mean episode length: 481.52\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.22\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6078464\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 1037.65s\n",
      "                               ETA: 362.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 742/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5743 steps/s (collection: 0.775s, learning 0.652s)\n",
      "               Value function loss: 2425.3978\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1943.60\n",
      "               Mean episode length: 481.52\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.17\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6086656\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 1039.08s\n",
      "                               ETA: 360.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 743/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5674 steps/s (collection: 0.857s, learning 0.587s)\n",
      "               Value function loss: 1237.6155\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1944.87\n",
      "               Mean episode length: 481.52\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.09\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6094848\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 1040.52s\n",
      "                               ETA: 359.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 744/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5694 steps/s (collection: 0.776s, learning 0.663s)\n",
      "               Value function loss: 1978.7611\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 1949.08\n",
      "               Mean episode length: 481.52\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.09\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6103040\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 1041.96s\n",
      "                               ETA: 358.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 745/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5620 steps/s (collection: 0.868s, learning 0.590s)\n",
      "               Value function loss: 1800.4416\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1947.25\n",
      "               Mean episode length: 477.78\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.12\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6111232\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1043.42s\n",
      "                               ETA: 356.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 746/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5212 steps/s (collection: 0.828s, learning 0.744s)\n",
      "               Value function loss: 2413.6077\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1956.93\n",
      "               Mean episode length: 481.18\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.06\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6119424\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 1044.99s\n",
      "                               ETA: 355.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 747/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5428 steps/s (collection: 0.829s, learning 0.680s)\n",
      "               Value function loss: 1317.5561\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 1951.66\n",
      "               Mean episode length: 481.18\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.10\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6127616\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 1046.50s\n",
      "                               ETA: 354.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 748/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6057 steps/s (collection: 0.826s, learning 0.526s)\n",
      "               Value function loss: 2313.8220\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 1991.79\n",
      "               Mean episode length: 488.39\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.17\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6135808\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 1047.85s\n",
      "                               ETA: 352.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 749/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4988 steps/s (collection: 0.889s, learning 0.753s)\n",
      "               Value function loss: 2095.4310\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2000.35\n",
      "               Mean episode length: 492.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.03\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6144000\n",
      "                    Iteration time: 1.64s\n",
      "                        Total time: 1049.49s\n",
      "                               ETA: 351.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 750/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5316 steps/s (collection: 0.876s, learning 0.665s)\n",
      "               Value function loss: 2165.8989\n",
      "                    Surrogate loss: -0.0057\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2003.66\n",
      "               Mean episode length: 492.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.09\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6152192\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 1051.03s\n",
      "                               ETA: 349.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 751/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5854 steps/s (collection: 0.793s, learning 0.606s)\n",
      "               Value function loss: 2104.0011\n",
      "                    Surrogate loss: -0.0059\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 1992.74\n",
      "               Mean episode length: 492.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.10\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6160384\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 1052.43s\n",
      "                               ETA: 348.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 752/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5225 steps/s (collection: 0.807s, learning 0.760s)\n",
      "               Value function loss: 1944.2555\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2015.36\n",
      "               Mean episode length: 496.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.01\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6168576\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 1054.00s\n",
      "                               ETA: 347.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 753/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5888 steps/s (collection: 0.849s, learning 0.543s)\n",
      "               Value function loss: 1401.0117\n",
      "                    Surrogate loss: -0.0051\n",
      "             Mean action noise std: 1.05\n",
      "                       Mean reward: 2018.73\n",
      "               Mean episode length: 496.25\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.08\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6176768\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 1055.39s\n",
      "                               ETA: 345.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 754/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6245 steps/s (collection: 0.744s, learning 0.568s)\n",
      "               Value function loss: 1384.9776\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2000.88\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.03\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6184960\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 1056.70s\n",
      "                               ETA: 344.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 755/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5505 steps/s (collection: 0.836s, learning 0.652s)\n",
      "               Value function loss: 1263.2165\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 1995.87\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.15\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6193152\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 1058.19s\n",
      "                               ETA: 342.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 756/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5329 steps/s (collection: 0.875s, learning 0.663s)\n",
      "               Value function loss: 1434.4695\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 1988.37\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.13\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6201344\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 1059.73s\n",
      "                               ETA: 341.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 757/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5303 steps/s (collection: 0.795s, learning 0.749s)\n",
      "               Value function loss: 2205.1212\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 1987.93\n",
      "               Mean episode length: 491.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.07\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6209536\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 1061.27s\n",
      "                               ETA: 340.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 758/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5574 steps/s (collection: 0.852s, learning 0.617s)\n",
      "               Value function loss: 2343.7207\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 1988.01\n",
      "               Mean episode length: 491.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.04\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6217728\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 1062.74s\n",
      "                               ETA: 338.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 759/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6301 steps/s (collection: 0.760s, learning 0.540s)\n",
      "               Value function loss: 1549.5384\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 1998.85\n",
      "               Mean episode length: 491.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.10\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6225920\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 1064.04s\n",
      "                               ETA: 337.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 760/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5216 steps/s (collection: 0.874s, learning 0.697s)\n",
      "               Value function loss: 2045.3823\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 1982.09\n",
      "               Mean episode length: 491.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.12\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6234112\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 1065.61s\n",
      "                               ETA: 336.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 761/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5571 steps/s (collection: 0.834s, learning 0.637s)\n",
      "               Value function loss: 2194.9626\n",
      "                    Surrogate loss: -0.0058\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2001.83\n",
      "               Mean episode length: 491.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.13\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6242304\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 1067.08s\n",
      "                               ETA: 334.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 762/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6202 steps/s (collection: 0.797s, learning 0.524s)\n",
      "               Value function loss: 1609.3614\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 1997.68\n",
      "               Mean episode length: 491.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.14\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6250496\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 1068.40s\n",
      "                               ETA: 333.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 763/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5047 steps/s (collection: 0.910s, learning 0.713s)\n",
      "               Value function loss: 1429.7830\n",
      "                    Surrogate loss: -0.0054\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2000.08\n",
      "               Mean episode length: 491.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.22\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6258688\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 1070.03s\n",
      "                               ETA: 331.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 764/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5308 steps/s (collection: 0.753s, learning 0.790s)\n",
      "               Value function loss: 2861.2280\n",
      "                    Surrogate loss: -0.0050\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2009.63\n",
      "               Mean episode length: 491.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.28\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6266880\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 1071.57s\n",
      "                               ETA: 330.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 765/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6190 steps/s (collection: 0.808s, learning 0.515s)\n",
      "               Value function loss: 2010.5500\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2013.72\n",
      "               Mean episode length: 491.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.23\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6275072\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 1072.89s\n",
      "                               ETA: 329.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 766/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5762 steps/s (collection: 0.888s, learning 0.533s)\n",
      "               Value function loss: 1924.8591\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2033.30\n",
      "               Mean episode length: 495.71\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.14\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6283264\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 1074.32s\n",
      "                               ETA: 327.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 767/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6089 steps/s (collection: 0.753s, learning 0.592s)\n",
      "               Value function loss: 2296.0578\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2041.88\n",
      "               Mean episode length: 495.71\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.10\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6291456\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 1075.66s\n",
      "                               ETA: 326.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 768/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5776 steps/s (collection: 0.853s, learning 0.565s)\n",
      "               Value function loss: 1832.4549\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2056.31\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.15\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6299648\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 1077.08s\n",
      "                               ETA: 324.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 769/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6514 steps/s (collection: 0.711s, learning 0.546s)\n",
      "               Value function loss: 1508.7881\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2061.85\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.38\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6307840\n",
      "                    Iteration time: 1.26s\n",
      "                        Total time: 1078.34s\n",
      "                               ETA: 323.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 770/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5456 steps/s (collection: 0.865s, learning 0.636s)\n",
      "               Value function loss: 1186.4086\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2059.69\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.38\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6316032\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1079.84s\n",
      "                               ETA: 322.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 771/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5631 steps/s (collection: 0.752s, learning 0.702s)\n",
      "               Value function loss: 1623.5815\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2057.62\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.31\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6324224\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1081.29s\n",
      "                               ETA: 320.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 772/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5415 steps/s (collection: 0.773s, learning 0.739s)\n",
      "               Value function loss: 1908.0702\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2073.60\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.39\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6332416\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 1082.81s\n",
      "                               ETA: 319.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 773/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5180 steps/s (collection: 0.867s, learning 0.714s)\n",
      "               Value function loss: 2454.7459\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2081.98\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.28\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6340608\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 1084.39s\n",
      "                               ETA: 318.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 774/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6361 steps/s (collection: 0.811s, learning 0.477s)\n",
      "               Value function loss: 1557.6286\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2077.22\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.25\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6348800\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 1085.67s\n",
      "                               ETA: 316.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 775/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5429 steps/s (collection: 0.866s, learning 0.643s)\n",
      "               Value function loss: 2218.0741\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2083.84\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.35\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6356992\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 1087.18s\n",
      "                               ETA: 315.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 776/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5363 steps/s (collection: 0.726s, learning 0.801s)\n",
      "               Value function loss: 2059.4238\n",
      "                    Surrogate loss: -0.0062\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2089.31\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.24\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6365184\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 1088.71s\n",
      "                               ETA: 313.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 777/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5912 steps/s (collection: 0.766s, learning 0.619s)\n",
      "               Value function loss: 2864.2232\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2103.17\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.34\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6373376\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 1090.10s\n",
      "                               ETA: 312.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 778/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5219 steps/s (collection: 0.829s, learning 0.741s)\n",
      "               Value function loss: 988.5889\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2109.72\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.37\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6381568\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 1091.67s\n",
      "                               ETA: 311.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 779/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5742 steps/s (collection: 0.802s, learning 0.624s)\n",
      "               Value function loss: 2076.6329\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2106.27\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.42\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6389760\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 1093.09s\n",
      "                               ETA: 309.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 780/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5118 steps/s (collection: 0.839s, learning 0.762s)\n",
      "               Value function loss: 2498.1003\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2109.65\n",
      "               Mean episode length: 497.48\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.27\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6397952\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 1094.69s\n",
      "                               ETA: 308.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 781/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5489 steps/s (collection: 0.850s, learning 0.643s)\n",
      "               Value function loss: 2363.7767\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2107.99\n",
      "               Mean episode length: 493.66\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.18\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6406144\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 1096.19s\n",
      "                               ETA: 307.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 782/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6062 steps/s (collection: 0.735s, learning 0.616s)\n",
      "               Value function loss: 1822.5862\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2119.03\n",
      "               Mean episode length: 493.66\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.21\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6414336\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 1097.54s\n",
      "                               ETA: 305.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 783/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5467 steps/s (collection: 0.785s, learning 0.713s)\n",
      "               Value function loss: 2131.3696\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2104.74\n",
      "               Mean episode length: 489.63\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.19\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6422528\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1099.03s\n",
      "                               ETA: 304.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 784/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5284 steps/s (collection: 0.790s, learning 0.760s)\n",
      "               Value function loss: 2197.9269\n",
      "                    Surrogate loss: -0.0060\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2108.73\n",
      "               Mean episode length: 489.63\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.24\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6430720\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 1100.59s\n",
      "                               ETA: 302.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 785/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5241 steps/s (collection: 0.756s, learning 0.807s)\n",
      "               Value function loss: 2063.6918\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2098.10\n",
      "               Mean episode length: 485.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.27\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6438912\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 1102.15s\n",
      "                               ETA: 301.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 786/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5451 steps/s (collection: 0.810s, learning 0.693s)\n",
      "               Value function loss: 1395.9242\n",
      "                    Surrogate loss: -0.0052\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2097.23\n",
      "               Mean episode length: 485.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.23\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6447104\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1103.65s\n",
      "                               ETA: 300.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 787/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5828 steps/s (collection: 0.810s, learning 0.595s)\n",
      "               Value function loss: 1841.8132\n",
      "                    Surrogate loss: -0.0061\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2101.46\n",
      "               Mean episode length: 485.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.24\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6455296\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 1105.06s\n",
      "                               ETA: 298.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 788/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5505 steps/s (collection: 0.774s, learning 0.714s)\n",
      "               Value function loss: 2213.4017\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2106.21\n",
      "               Mean episode length: 485.86\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.28\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6463488\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 1106.54s\n",
      "                               ETA: 297.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 789/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5682 steps/s (collection: 0.866s, learning 0.576s)\n",
      "               Value function loss: 2800.9407\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2074.42\n",
      "               Mean episode length: 483.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.27\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6471680\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 1107.99s\n",
      "                               ETA: 295.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 790/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5929 steps/s (collection: 0.878s, learning 0.504s)\n",
      "               Value function loss: 1409.9669\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2069.68\n",
      "               Mean episode length: 483.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.29\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6479872\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 1109.37s\n",
      "                               ETA: 294.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 791/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5750 steps/s (collection: 0.796s, learning 0.628s)\n",
      "               Value function loss: 2014.0428\n",
      "                    Surrogate loss: -0.0064\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2061.96\n",
      "               Mean episode length: 483.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.32\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6488064\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 1110.79s\n",
      "                               ETA: 293.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 792/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5453 steps/s (collection: 0.801s, learning 0.701s)\n",
      "               Value function loss: 1662.4413\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2052.25\n",
      "               Mean episode length: 484.45\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.40\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6496256\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1112.29s\n",
      "                               ETA: 291.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 793/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5688 steps/s (collection: 0.818s, learning 0.622s)\n",
      "               Value function loss: 2553.2233\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2076.46\n",
      "               Mean episode length: 488.27\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.32\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6504448\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 1113.73s\n",
      "                               ETA: 290.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 794/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6175 steps/s (collection: 0.801s, learning 0.525s)\n",
      "               Value function loss: 1606.2012\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2059.93\n",
      "               Mean episode length: 488.27\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.43\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6512640\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 1115.06s\n",
      "                               ETA: 288.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 795/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4945 steps/s (collection: 0.883s, learning 0.774s)\n",
      "               Value function loss: 2686.2839\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2093.69\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.43\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6520832\n",
      "                    Iteration time: 1.66s\n",
      "                        Total time: 1116.72s\n",
      "                               ETA: 287.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 796/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6849 steps/s (collection: 0.717s, learning 0.479s)\n",
      "               Value function loss: 2277.0691\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2080.67\n",
      "               Mean episode length: 491.75\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.34\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6529024\n",
      "                    Iteration time: 1.20s\n",
      "                        Total time: 1117.91s\n",
      "                               ETA: 286.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 797/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5788 steps/s (collection: 0.758s, learning 0.657s)\n",
      "               Value function loss: 2446.1290\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2082.77\n",
      "               Mean episode length: 490.53\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.30\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6537216\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 1119.33s\n",
      "                               ETA: 284.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 798/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4963 steps/s (collection: 0.882s, learning 0.769s)\n",
      "               Value function loss: 2495.0443\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2102.93\n",
      "               Mean episode length: 493.02\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.43\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6545408\n",
      "                    Iteration time: 1.65s\n",
      "                        Total time: 1120.98s\n",
      "                               ETA: 283.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 799/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5304 steps/s (collection: 0.748s, learning 0.796s)\n",
      "               Value function loss: 2489.4143\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2098.68\n",
      "               Mean episode length: 493.02\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.38\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6553600\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 1122.52s\n",
      "                               ETA: 282.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 800/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5937 steps/s (collection: 0.818s, learning 0.561s)\n",
      "               Value function loss: 1356.4552\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2085.57\n",
      "               Mean episode length: 490.51\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.47\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6561792\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 1123.90s\n",
      "                               ETA: 280.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 801/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5611 steps/s (collection: 0.796s, learning 0.664s)\n",
      "               Value function loss: 1731.2411\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2076.92\n",
      "               Mean episode length: 489.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.53\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6569984\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1125.36s\n",
      "                               ETA: 279.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 802/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5803 steps/s (collection: 0.831s, learning 0.581s)\n",
      "               Value function loss: 2017.9417\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2083.36\n",
      "               Mean episode length: 489.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.65\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6578176\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 1126.77s\n",
      "                               ETA: 277.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 803/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6234 steps/s (collection: 0.765s, learning 0.549s)\n",
      "               Value function loss: 1651.6414\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2097.08\n",
      "               Mean episode length: 489.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.61\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6586368\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 1128.09s\n",
      "                               ETA: 276.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 804/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5247 steps/s (collection: 0.851s, learning 0.710s)\n",
      "               Value function loss: 2651.7687\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2115.73\n",
      "               Mean episode length: 489.82\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.64\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6594560\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 1129.65s\n",
      "                               ETA: 275.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 805/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5229 steps/s (collection: 0.777s, learning 0.789s)\n",
      "               Value function loss: 2643.7566\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2118.37\n",
      "               Mean episode length: 488.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.46\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6602752\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 1131.22s\n",
      "                               ETA: 273.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 806/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5933 steps/s (collection: 0.753s, learning 0.628s)\n",
      "               Value function loss: 2048.4678\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2135.90\n",
      "               Mean episode length: 488.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.58\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6610944\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 1132.60s\n",
      "                               ETA: 272.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 807/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6109 steps/s (collection: 0.811s, learning 0.530s)\n",
      "               Value function loss: 2086.3845\n",
      "                    Surrogate loss: -0.0024\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2130.02\n",
      "               Mean episode length: 488.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.58\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6619136\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 1133.94s\n",
      "                               ETA: 270.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 808/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5877 steps/s (collection: 0.832s, learning 0.562s)\n",
      "               Value function loss: 2354.4968\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2149.02\n",
      "               Mean episode length: 488.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.64\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6627328\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 1135.33s\n",
      "                               ETA: 269.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 809/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5808 steps/s (collection: 0.878s, learning 0.533s)\n",
      "               Value function loss: 2023.3129\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2184.16\n",
      "               Mean episode length: 491.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.51\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6635520\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 1136.74s\n",
      "                               ETA: 268.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 810/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6474 steps/s (collection: 0.741s, learning 0.525s)\n",
      "               Value function loss: 2031.6887\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2195.48\n",
      "               Mean episode length: 491.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.64\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6643712\n",
      "                    Iteration time: 1.27s\n",
      "                        Total time: 1138.01s\n",
      "                               ETA: 266.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 811/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4911 steps/s (collection: 0.850s, learning 0.818s)\n",
      "               Value function loss: 3417.9686\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2206.59\n",
      "               Mean episode length: 491.17\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.57\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6651904\n",
      "                    Iteration time: 1.67s\n",
      "                        Total time: 1139.67s\n",
      "                               ETA: 265.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 812/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6217 steps/s (collection: 0.782s, learning 0.535s)\n",
      "               Value function loss: 3015.8141\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2230.91\n",
      "               Mean episode length: 493.68\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.58\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6660096\n",
      "                    Iteration time: 1.32s\n",
      "                        Total time: 1140.99s\n",
      "                               ETA: 263.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 813/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5735 steps/s (collection: 0.818s, learning 0.611s)\n",
      "               Value function loss: 1903.9239\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2260.74\n",
      "               Mean episode length: 497.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.58\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6668288\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 1142.42s\n",
      "                               ETA: 262.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 814/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4972 steps/s (collection: 0.841s, learning 0.807s)\n",
      "               Value function loss: 2833.8792\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2269.34\n",
      "               Mean episode length: 497.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.68\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6676480\n",
      "                    Iteration time: 1.65s\n",
      "                        Total time: 1144.07s\n",
      "                               ETA: 261.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 815/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5641 steps/s (collection: 0.819s, learning 0.634s)\n",
      "               Value function loss: 1866.0818\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2262.04\n",
      "               Mean episode length: 497.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.61\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6684672\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1145.52s\n",
      "                               ETA: 259.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 816/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5452 steps/s (collection: 0.774s, learning 0.729s)\n",
      "               Value function loss: 2230.0720\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2270.17\n",
      "               Mean episode length: 497.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.73\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6692864\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1147.02s\n",
      "                               ETA: 258.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 817/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5045 steps/s (collection: 0.807s, learning 0.817s)\n",
      "               Value function loss: 1740.9850\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2279.57\n",
      "               Mean episode length: 497.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.73\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6701056\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 1148.65s\n",
      "                               ETA: 257.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 818/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5740 steps/s (collection: 0.802s, learning 0.625s)\n",
      "               Value function loss: 2094.9880\n",
      "                    Surrogate loss: -0.0046\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2287.28\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.79\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6709248\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 1150.07s\n",
      "                               ETA: 255.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 819/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5971 steps/s (collection: 0.776s, learning 0.595s)\n",
      "               Value function loss: 2898.1402\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2292.89\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.91\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6717440\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 1151.44s\n",
      "                               ETA: 254.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 820/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5125 steps/s (collection: 0.849s, learning 0.749s)\n",
      "               Value function loss: 3712.8542\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2274.29\n",
      "               Mean episode length: 496.05\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.69\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6725632\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 1153.04s\n",
      "                               ETA: 252.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 821/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5634 steps/s (collection: 0.789s, learning 0.665s)\n",
      "               Value function loss: 1843.4539\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2273.59\n",
      "               Mean episode length: 496.05\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.52\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6733824\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1154.50s\n",
      "                               ETA: 251.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 822/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5822 steps/s (collection: 0.805s, learning 0.602s)\n",
      "               Value function loss: 2414.6301\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2225.20\n",
      "               Mean episode length: 487.94\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.68\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6742016\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 1155.90s\n",
      "                               ETA: 250.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 823/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5572 steps/s (collection: 0.778s, learning 0.692s)\n",
      "               Value function loss: 2345.5927\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2239.66\n",
      "               Mean episode length: 487.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.73\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6750208\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 1157.37s\n",
      "                               ETA: 248.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 824/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6148 steps/s (collection: 0.805s, learning 0.527s)\n",
      "               Value function loss: 3415.6968\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2243.62\n",
      "               Mean episode length: 487.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.68\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6758400\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 1158.71s\n",
      "                               ETA: 247.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 825/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5975 steps/s (collection: 0.732s, learning 0.639s)\n",
      "               Value function loss: 1288.0152\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2251.03\n",
      "               Mean episode length: 487.19\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.78\n",
      "       Mean episode length/episode: 31.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6766592\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 1160.08s\n",
      "                               ETA: 245.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 826/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4698 steps/s (collection: 0.846s, learning 0.897s)\n",
      "               Value function loss: 2388.5497\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2234.56\n",
      "               Mean episode length: 482.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.76\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6774784\n",
      "                    Iteration time: 1.74s\n",
      "                        Total time: 1161.82s\n",
      "                               ETA: 244.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 827/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6089 steps/s (collection: 0.792s, learning 0.553s)\n",
      "               Value function loss: 2741.3436\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2252.65\n",
      "               Mean episode length: 482.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.75\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6782976\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 1163.17s\n",
      "                               ETA: 243.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 828/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5845 steps/s (collection: 0.769s, learning 0.632s)\n",
      "               Value function loss: 3397.5528\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2246.91\n",
      "               Mean episode length: 478.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.77\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6791168\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 1164.57s\n",
      "                               ETA: 241.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 829/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5036 steps/s (collection: 0.858s, learning 0.768s)\n",
      "               Value function loss: 2539.6333\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2254.76\n",
      "               Mean episode length: 478.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.71\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6799360\n",
      "                    Iteration time: 1.63s\n",
      "                        Total time: 1166.19s\n",
      "                               ETA: 240.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 830/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5592 steps/s (collection: 0.819s, learning 0.646s)\n",
      "               Value function loss: 2323.8656\n",
      "                    Surrogate loss: -0.0056\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2261.46\n",
      "               Mean episode length: 478.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.65\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6807552\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1167.66s\n",
      "                               ETA: 238.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 831/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5170 steps/s (collection: 0.758s, learning 0.827s)\n",
      "               Value function loss: 1967.2667\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2267.89\n",
      "               Mean episode length: 478.84\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.76\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6815744\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 1169.24s\n",
      "                               ETA: 237.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 832/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5615 steps/s (collection: 0.800s, learning 0.659s)\n",
      "               Value function loss: 2107.9691\n",
      "                    Surrogate loss: -0.0047\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2279.78\n",
      "               Mean episode length: 482.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.91\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6823936\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1170.70s\n",
      "                               ETA: 236.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 833/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5271 steps/s (collection: 0.813s, learning 0.741s)\n",
      "               Value function loss: 2289.5422\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2283.40\n",
      "               Mean episode length: 482.80\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.87\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6832128\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 1172.26s\n",
      "                               ETA: 234.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 834/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6289 steps/s (collection: 0.777s, learning 0.526s)\n",
      "               Value function loss: 2392.2993\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2331.76\n",
      "               Mean episode length: 490.90\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.73\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6840320\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 1173.56s\n",
      "                               ETA: 233.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 835/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5034 steps/s (collection: 0.802s, learning 0.825s)\n",
      "               Value function loss: 3014.5973\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2315.81\n",
      "               Mean episode length: 488.03\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.81\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6848512\n",
      "                    Iteration time: 1.63s\n",
      "                        Total time: 1175.19s\n",
      "                               ETA: 231.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 836/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5995 steps/s (collection: 0.767s, learning 0.599s)\n",
      "               Value function loss: 3619.2039\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2326.58\n",
      "               Mean episode length: 488.03\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.63\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6856704\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 1176.55s\n",
      "                               ETA: 230.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 837/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5820 steps/s (collection: 0.727s, learning 0.680s)\n",
      "               Value function loss: 1905.3347\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2327.82\n",
      "               Mean episode length: 488.03\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.62\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6864896\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 1177.96s\n",
      "                               ETA: 229.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 838/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5756 steps/s (collection: 0.781s, learning 0.642s)\n",
      "               Value function loss: 2767.3560\n",
      "                    Surrogate loss: -0.0053\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2356.90\n",
      "               Mean episode length: 492.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.68\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6873088\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 1179.38s\n",
      "                               ETA: 227.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 839/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5649 steps/s (collection: 0.921s, learning 0.528s)\n",
      "               Value function loss: 2561.7522\n",
      "                    Surrogate loss: -0.0048\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2358.95\n",
      "               Mean episode length: 492.32\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.77\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6881280\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1180.83s\n",
      "                               ETA: 226.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 840/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5325 steps/s (collection: 0.774s, learning 0.764s)\n",
      "               Value function loss: 2452.7566\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2371.40\n",
      "               Mean episode length: 496.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.79\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6889472\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 1182.37s\n",
      "                               ETA: 224.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 841/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5507 steps/s (collection: 0.849s, learning 0.639s)\n",
      "               Value function loss: 2546.4045\n",
      "                    Surrogate loss: -0.0049\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2359.78\n",
      "               Mean episode length: 496.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.81\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6897664\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 1183.86s\n",
      "                               ETA: 223.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 842/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4939 steps/s (collection: 0.781s, learning 0.878s)\n",
      "               Value function loss: 3223.9424\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2355.17\n",
      "               Mean episode length: 496.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.87\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6905856\n",
      "                    Iteration time: 1.66s\n",
      "                        Total time: 1185.52s\n",
      "                               ETA: 222.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 843/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5447 steps/s (collection: 0.744s, learning 0.760s)\n",
      "               Value function loss: 2432.6892\n",
      "                    Surrogate loss: -0.0055\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2345.03\n",
      "               Mean episode length: 496.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.88\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6914048\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1187.02s\n",
      "                               ETA: 220.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 844/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5966 steps/s (collection: 0.848s, learning 0.525s)\n",
      "               Value function loss: 2995.0817\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2355.51\n",
      "               Mean episode length: 496.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.71\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6922240\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 1188.39s\n",
      "                               ETA: 219.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 845/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5890 steps/s (collection: 0.767s, learning 0.624s)\n",
      "               Value function loss: 2422.3750\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2344.49\n",
      "               Mean episode length: 496.38\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.71\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6930432\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 1189.78s\n",
      "                               ETA: 218.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 846/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4867 steps/s (collection: 0.864s, learning 0.819s)\n",
      "               Value function loss: 2419.5843\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2357.87\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.91\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6938624\n",
      "                    Iteration time: 1.68s\n",
      "                        Total time: 1191.47s\n",
      "                               ETA: 216.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 847/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5923 steps/s (collection: 0.791s, learning 0.592s)\n",
      "               Value function loss: 1832.5851\n",
      "                    Surrogate loss: -0.0042\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2336.64\n",
      "               Mean episode length: 496.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.95\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6946816\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 1192.85s\n",
      "                               ETA: 215.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 848/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5405 steps/s (collection: 0.787s, learning 0.729s)\n",
      "               Value function loss: 2103.9220\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2336.85\n",
      "               Mean episode length: 496.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.96\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6955008\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 1194.37s\n",
      "                               ETA: 213.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 849/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6170 steps/s (collection: 0.772s, learning 0.556s)\n",
      "               Value function loss: 2501.1873\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2341.63\n",
      "               Mean episode length: 496.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.95\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6963200\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 1195.69s\n",
      "                               ETA: 212.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 850/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5438 steps/s (collection: 0.763s, learning 0.743s)\n",
      "               Value function loss: 1935.5818\n",
      "                    Surrogate loss: -0.0045\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2352.61\n",
      "               Mean episode length: 496.50\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.97\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6971392\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 1197.20s\n",
      "                               ETA: 211.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 851/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5277 steps/s (collection: 0.857s, learning 0.696s)\n",
      "               Value function loss: 3387.5597\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2319.84\n",
      "               Mean episode length: 493.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.89\n",
      "       Mean episode length/episode: 29.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6979584\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 1198.75s\n",
      "                               ETA: 209.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 852/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5909 steps/s (collection: 0.803s, learning 0.583s)\n",
      "               Value function loss: 2662.0787\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2340.96\n",
      "               Mean episode length: 493.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.85\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6987776\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 1200.14s\n",
      "                               ETA: 208.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 853/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5808 steps/s (collection: 0.850s, learning 0.560s)\n",
      "               Value function loss: 3088.6936\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2351.40\n",
      "               Mean episode length: 493.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.87\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 6995968\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 1201.55s\n",
      "                               ETA: 206.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 854/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6238 steps/s (collection: 0.767s, learning 0.546s)\n",
      "               Value function loss: 2916.2244\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2373.51\n",
      "               Mean episode length: 493.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.01\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7004160\n",
      "                    Iteration time: 1.31s\n",
      "                        Total time: 1202.86s\n",
      "                               ETA: 205.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 855/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5455 steps/s (collection: 0.859s, learning 0.643s)\n",
      "               Value function loss: 2970.6457\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2381.40\n",
      "               Mean episode length: 493.06\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.17\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7012352\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1204.36s\n",
      "                               ETA: 204.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 856/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5818 steps/s (collection: 0.772s, learning 0.636s)\n",
      "               Value function loss: 1395.9497\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2371.09\n",
      "               Mean episode length: 490.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.11\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7020544\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 1205.77s\n",
      "                               ETA: 202.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 857/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5439 steps/s (collection: 0.790s, learning 0.716s)\n",
      "               Value function loss: 2646.9578\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2372.44\n",
      "               Mean episode length: 490.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.12\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7028736\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 1207.28s\n",
      "                               ETA: 201.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 858/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5960 steps/s (collection: 0.803s, learning 0.571s)\n",
      "               Value function loss: 3680.2103\n",
      "                    Surrogate loss: -0.0043\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2398.73\n",
      "               Mean episode length: 490.42\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.02\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7036928\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 1208.65s\n",
      "                               ETA: 199.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 859/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5593 steps/s (collection: 0.865s, learning 0.599s)\n",
      "               Value function loss: 3907.4204\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2427.84\n",
      "               Mean episode length: 493.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.93\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7045120\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1210.12s\n",
      "                               ETA: 198.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 860/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5096 steps/s (collection: 0.769s, learning 0.839s)\n",
      "               Value function loss: 1943.7033\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2426.50\n",
      "               Mean episode length: 493.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.96\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7053312\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 1211.72s\n",
      "                               ETA: 197.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 861/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6278 steps/s (collection: 0.724s, learning 0.581s)\n",
      "               Value function loss: 2630.9614\n",
      "                    Surrogate loss: -0.0036\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2422.62\n",
      "               Mean episode length: 493.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.99\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7061504\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 1213.03s\n",
      "                               ETA: 195.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 862/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5587 steps/s (collection: 0.938s, learning 0.528s)\n",
      "               Value function loss: 2580.0938\n",
      "                    Surrogate loss: -0.0044\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2439.28\n",
      "               Mean episode length: 497.36\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.09\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7069696\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 1214.49s\n",
      "                               ETA: 194.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 863/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6379 steps/s (collection: 0.739s, learning 0.546s)\n",
      "               Value function loss: 2949.2991\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2459.19\n",
      "               Mean episode length: 497.36\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.23\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7077888\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 1215.78s\n",
      "                               ETA: 192.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 864/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5091 steps/s (collection: 0.872s, learning 0.738s)\n",
      "               Value function loss: 2065.0404\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2464.57\n",
      "               Mean episode length: 497.36\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.15\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7086080\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 1217.39s\n",
      "                               ETA: 191.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 865/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5887 steps/s (collection: 0.761s, learning 0.631s)\n",
      "               Value function loss: 2257.7940\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2470.66\n",
      "               Mean episode length: 497.36\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.04\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7094272\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 1218.78s\n",
      "                               ETA: 190.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 866/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5625 steps/s (collection: 0.850s, learning 0.606s)\n",
      "               Value function loss: 3316.3020\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2447.23\n",
      "               Mean episode length: 494.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.04\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7102464\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1220.23s\n",
      "                               ETA: 188.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 867/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5178 steps/s (collection: 0.856s, learning 0.726s)\n",
      "               Value function loss: 3679.6807\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2454.51\n",
      "               Mean episode length: 494.12\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.00\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7110656\n",
      "                    Iteration time: 1.58s\n",
      "                        Total time: 1221.82s\n",
      "                               ETA: 187.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 868/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5471 steps/s (collection: 0.808s, learning 0.689s)\n",
      "               Value function loss: 2073.5003\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2471.33\n",
      "               Mean episode length: 496.76\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.02\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7118848\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1223.31s\n",
      "                               ETA: 185.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 869/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6139 steps/s (collection: 0.790s, learning 0.544s)\n",
      "               Value function loss: 2392.4424\n",
      "                    Surrogate loss: -0.0041\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2483.74\n",
      "               Mean episode length: 496.76\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.06\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7127040\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 1224.65s\n",
      "                               ETA: 184.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 870/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5304 steps/s (collection: 0.803s, learning 0.742s)\n",
      "               Value function loss: 2971.3363\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2492.72\n",
      "               Mean episode length: 496.76\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.96\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7135232\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 1226.19s\n",
      "                               ETA: 183.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 871/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6035 steps/s (collection: 0.762s, learning 0.595s)\n",
      "               Value function loss: 4198.6521\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2509.19\n",
      "               Mean episode length: 496.76\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.82\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7143424\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 1227.55s\n",
      "                               ETA: 181.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 872/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5393 steps/s (collection: 0.750s, learning 0.769s)\n",
      "               Value function loss: 1682.4875\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2468.82\n",
      "               Mean episode length: 489.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.93\n",
      "       Mean episode length/episode: 31.03\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7151616\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 1229.07s\n",
      "                               ETA: 180.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 873/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5509 steps/s (collection: 0.821s, learning 0.666s)\n",
      "               Value function loss: 2977.7856\n",
      "                    Surrogate loss: -0.0040\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2460.68\n",
      "               Mean episode length: 489.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.97\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7159808\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 1230.56s\n",
      "                               ETA: 178.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 874/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5278 steps/s (collection: 0.880s, learning 0.672s)\n",
      "               Value function loss: 2848.8134\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2475.81\n",
      "               Mean episode length: 489.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.96\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7168000\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 1232.11s\n",
      "                               ETA: 177.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 875/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6182 steps/s (collection: 0.778s, learning 0.548s)\n",
      "               Value function loss: 3664.0563\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2478.27\n",
      "               Mean episode length: 489.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.97\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7176192\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 1233.43s\n",
      "                               ETA: 176.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 876/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5361 steps/s (collection: 0.893s, learning 0.635s)\n",
      "               Value function loss: 3719.6773\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2471.44\n",
      "               Mean episode length: 489.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.08\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7184384\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 1234.96s\n",
      "                               ETA: 174.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 877/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4862 steps/s (collection: 0.811s, learning 0.874s)\n",
      "               Value function loss: 2467.3274\n",
      "                    Surrogate loss: -0.0038\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2469.11\n",
      "               Mean episode length: 489.37\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.12\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7192576\n",
      "                    Iteration time: 1.68s\n",
      "                        Total time: 1236.65s\n",
      "                               ETA: 173.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 878/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5208 steps/s (collection: 0.736s, learning 0.837s)\n",
      "               Value function loss: 1916.0240\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2488.93\n",
      "               Mean episode length: 492.61\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.07\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7200768\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 1238.22s\n",
      "                               ETA: 171.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 879/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5137 steps/s (collection: 0.801s, learning 0.794s)\n",
      "               Value function loss: 2766.8331\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2488.92\n",
      "               Mean episode length: 492.61\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.15\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7208960\n",
      "                    Iteration time: 1.59s\n",
      "                        Total time: 1239.81s\n",
      "                               ETA: 170.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 880/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6043 steps/s (collection: 0.760s, learning 0.595s)\n",
      "               Value function loss: 1993.9582\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2467.79\n",
      "               Mean episode length: 489.63\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.14\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7217152\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 1241.17s\n",
      "                               ETA: 169.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 881/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4990 steps/s (collection: 0.837s, learning 0.804s)\n",
      "               Value function loss: 2236.8078\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2461.49\n",
      "               Mean episode length: 489.63\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.16\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7225344\n",
      "                    Iteration time: 1.64s\n",
      "                        Total time: 1242.81s\n",
      "                               ETA: 167.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 882/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5697 steps/s (collection: 0.749s, learning 0.689s)\n",
      "               Value function loss: 3964.0212\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2458.02\n",
      "               Mean episode length: 489.63\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.13\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7233536\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 1244.25s\n",
      "                               ETA: 166.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 883/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5834 steps/s (collection: 0.887s, learning 0.517s)\n",
      "               Value function loss: 3528.0879\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2450.73\n",
      "               Mean episode length: 489.63\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.05\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7241728\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 1245.65s\n",
      "                               ETA: 164.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 884/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5492 steps/s (collection: 0.783s, learning 0.709s)\n",
      "               Value function loss: 2568.4088\n",
      "                    Surrogate loss: -0.0039\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2493.17\n",
      "               Mean episode length: 497.03\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.20\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7249920\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 1247.14s\n",
      "                               ETA: 163.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 885/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6085 steps/s (collection: 0.827s, learning 0.519s)\n",
      "               Value function loss: 3218.8076\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2477.08\n",
      "               Mean episode length: 493.92\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.28\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7258112\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 1248.49s\n",
      "                               ETA: 162.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 886/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5277 steps/s (collection: 0.809s, learning 0.744s)\n",
      "               Value function loss: 3216.1736\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2488.54\n",
      "               Mean episode length: 493.92\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.28\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7266304\n",
      "                    Iteration time: 1.55s\n",
      "                        Total time: 1250.04s\n",
      "                               ETA: 160.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 887/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5992 steps/s (collection: 0.776s, learning 0.591s)\n",
      "               Value function loss: 2799.3427\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2483.74\n",
      "               Mean episode length: 493.92\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.23\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7274496\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 1251.41s\n",
      "                               ETA: 159.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 888/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5447 steps/s (collection: 0.783s, learning 0.721s)\n",
      "               Value function loss: 2471.2584\n",
      "                    Surrogate loss: -0.0037\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2485.75\n",
      "               Mean episode length: 493.92\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.21\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7282688\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1252.91s\n",
      "                               ETA: 157.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 889/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6364 steps/s (collection: 0.829s, learning 0.458s)\n",
      "               Value function loss: 3961.7548\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2501.16\n",
      "               Mean episode length: 493.92\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.17\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7290880\n",
      "                    Iteration time: 1.29s\n",
      "                        Total time: 1254.20s\n",
      "                               ETA: 156.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 890/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6826 steps/s (collection: 0.729s, learning 0.471s)\n",
      "               Value function loss: 3272.8702\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2502.27\n",
      "               Mean episode length: 493.92\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.10\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7299072\n",
      "                    Iteration time: 1.20s\n",
      "                        Total time: 1255.40s\n",
      "                               ETA: 155.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 891/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5126 steps/s (collection: 0.940s, learning 0.658s)\n",
      "               Value function loss: 3026.4834\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2535.06\n",
      "               Mean episode length: 496.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.96\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7307264\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 1257.00s\n",
      "                               ETA: 153.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 892/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5449 steps/s (collection: 0.802s, learning 0.702s)\n",
      "               Value function loss: 3581.8553\n",
      "                    Surrogate loss: -0.0023\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2550.86\n",
      "               Mean episode length: 496.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.96\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7315456\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1258.50s\n",
      "                               ETA: 152.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 893/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5384 steps/s (collection: 0.868s, learning 0.654s)\n",
      "               Value function loss: 2971.7823\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2552.58\n",
      "               Mean episode length: 496.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 4.94\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7323648\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 1260.02s\n",
      "                               ETA: 150.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 894/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6044 steps/s (collection: 0.723s, learning 0.632s)\n",
      "               Value function loss: 2308.6642\n",
      "                    Surrogate loss: -0.0027\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2560.68\n",
      "               Mean episode length: 496.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.22\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7331840\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 1261.38s\n",
      "                               ETA: 149.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 895/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5668 steps/s (collection: 0.864s, learning 0.581s)\n",
      "               Value function loss: 1880.0454\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2563.84\n",
      "               Mean episode length: 496.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.37\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7340032\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1262.82s\n",
      "                               ETA: 148.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 896/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5680 steps/s (collection: 0.780s, learning 0.662s)\n",
      "               Value function loss: 2355.7863\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2563.21\n",
      "               Mean episode length: 496.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.30\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7348224\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 1264.26s\n",
      "                               ETA: 146.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 897/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5766 steps/s (collection: 0.728s, learning 0.693s)\n",
      "               Value function loss: 2862.2402\n",
      "                    Surrogate loss: -0.0035\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2590.59\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.20\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7356416\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 1265.69s\n",
      "                               ETA: 145.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 898/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5375 steps/s (collection: 0.914s, learning 0.610s)\n",
      "               Value function loss: 3597.2031\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2573.85\n",
      "               Mean episode length: 499.57\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.02\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7364608\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 1267.21s\n",
      "                               ETA: 143.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 899/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5717 steps/s (collection: 0.845s, learning 0.587s)\n",
      "               Value function loss: 2677.8799\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2581.74\n",
      "               Mean episode length: 499.57\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 4.94\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7372800\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 1268.64s\n",
      "                               ETA: 142.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 900/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5149 steps/s (collection: 0.811s, learning 0.779s)\n",
      "               Value function loss: 3522.7510\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2574.17\n",
      "               Mean episode length: 499.57\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.04\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7380992\n",
      "                    Iteration time: 1.59s\n",
      "                        Total time: 1270.23s\n",
      "                               ETA: 141.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 901/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5373 steps/s (collection: 0.822s, learning 0.703s)\n",
      "               Value function loss: 3094.2111\n",
      "                    Surrogate loss: -0.0034\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2573.69\n",
      "               Mean episode length: 499.57\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.15\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7389184\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 1271.76s\n",
      "                               ETA: 139.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 902/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6010 steps/s (collection: 0.756s, learning 0.607s)\n",
      "               Value function loss: 3686.8251\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2558.58\n",
      "               Mean episode length: 499.57\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.03\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7397376\n",
      "                    Iteration time: 1.36s\n",
      "                        Total time: 1273.12s\n",
      "                               ETA: 138.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 903/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5694 steps/s (collection: 0.866s, learning 0.572s)\n",
      "               Value function loss: 1730.1992\n",
      "                    Surrogate loss: -0.0023\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2534.12\n",
      "               Mean episode length: 496.51\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.13\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7405568\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 1274.56s\n",
      "                               ETA: 136.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 904/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5641 steps/s (collection: 0.767s, learning 0.685s)\n",
      "               Value function loss: 2952.7112\n",
      "                    Surrogate loss: -0.0028\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2534.49\n",
      "               Mean episode length: 496.51\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.26\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7413760\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1276.01s\n",
      "                               ETA: 135.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 905/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5122 steps/s (collection: 0.870s, learning 0.729s)\n",
      "               Value function loss: 3783.1514\n",
      "                    Surrogate loss: -0.0023\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2532.08\n",
      "               Mean episode length: 493.96\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.19\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7421952\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 1277.61s\n",
      "                               ETA: 134.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 906/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5364 steps/s (collection: 0.789s, learning 0.738s)\n",
      "               Value function loss: 4009.5603\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2503.11\n",
      "               Mean episode length: 490.47\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.14\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7430144\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 1279.14s\n",
      "                               ETA: 132.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 907/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5234 steps/s (collection: 0.794s, learning 0.771s)\n",
      "               Value function loss: 2446.1391\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2507.78\n",
      "               Mean episode length: 490.47\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.20\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7438336\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 1280.70s\n",
      "                               ETA: 131.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 908/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5918 steps/s (collection: 0.770s, learning 0.614s)\n",
      "               Value function loss: 2988.0073\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2496.28\n",
      "               Mean episode length: 485.89\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.24\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7446528\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 1282.09s\n",
      "                               ETA: 129.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 909/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6409 steps/s (collection: 0.779s, learning 0.499s)\n",
      "               Value function loss: 2915.0212\n",
      "                    Surrogate loss: -0.0033\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2489.15\n",
      "               Mean episode length: 485.89\n",
      "                 Mean success rate: 0.00\n",
      "                  Mean reward/step: 5.27\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7454720\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 1283.37s\n",
      "                               ETA: 128.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 910/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5385 steps/s (collection: 0.847s, learning 0.674s)\n",
      "               Value function loss: 3000.7448\n",
      "                    Surrogate loss: -0.0028\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2487.76\n",
      "               Mean episode length: 484.46\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.20\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7462912\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 1284.89s\n",
      "                               ETA: 126.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 911/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5871 steps/s (collection: 0.752s, learning 0.644s)\n",
      "               Value function loss: 2162.4989\n",
      "                    Surrogate loss: -0.0032\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2482.50\n",
      "               Mean episode length: 484.46\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.22\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7471104\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 1286.28s\n",
      "                               ETA: 125.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 912/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5588 steps/s (collection: 0.795s, learning 0.670s)\n",
      "               Value function loss: 2708.1366\n",
      "                    Surrogate loss: -0.0030\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2481.82\n",
      "               Mean episode length: 484.46\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.38\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7479296\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 1287.75s\n",
      "                               ETA: 124.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 913/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5907 steps/s (collection: 0.819s, learning 0.568s)\n",
      "               Value function loss: 4056.4078\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2484.32\n",
      "               Mean episode length: 484.46\n",
      "                 Mean success rate: 1.00\n",
      "                  Mean reward/step: 5.38\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7487488\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 1289.13s\n",
      "                               ETA: 122.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 914/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5533 steps/s (collection: 0.879s, learning 0.601s)\n",
      "               Value function loss: 4116.6503\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2480.64\n",
      "               Mean episode length: 481.88\n",
      "                 Mean success rate: 1.00\n",
      "                  Mean reward/step: 5.21\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7495680\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 1290.61s\n",
      "                               ETA: 121.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 915/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6132 steps/s (collection: 0.752s, learning 0.584s)\n",
      "               Value function loss: 2170.6915\n",
      "                    Surrogate loss: -0.0023\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2507.62\n",
      "               Mean episode length: 484.94\n",
      "                 Mean success rate: 1.00\n",
      "                  Mean reward/step: 5.16\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7503872\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 1291.95s\n",
      "                               ETA: 119.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 916/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4982 steps/s (collection: 0.875s, learning 0.769s)\n",
      "               Value function loss: 3234.3339\n",
      "                    Surrogate loss: -0.0031\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2535.78\n",
      "               Mean episode length: 487.49\n",
      "                 Mean success rate: 1.00\n",
      "                  Mean reward/step: 5.11\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7512064\n",
      "                    Iteration time: 1.64s\n",
      "                        Total time: 1293.59s\n",
      "                               ETA: 118.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 917/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5638 steps/s (collection: 0.798s, learning 0.655s)\n",
      "               Value function loss: 2812.8216\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2519.94\n",
      "               Mean episode length: 487.49\n",
      "                 Mean success rate: 1.00\n",
      "                  Mean reward/step: 5.18\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7520256\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1295.05s\n",
      "                               ETA: 117.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 918/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5454 steps/s (collection: 0.811s, learning 0.691s)\n",
      "               Value function loss: 3638.5201\n",
      "                    Surrogate loss: -0.0024\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2537.37\n",
      "               Mean episode length: 490.99\n",
      "                 Mean success rate: 1.50\n",
      "                  Mean reward/step: 5.05\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7528448\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1296.55s\n",
      "                               ETA: 115.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 919/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5375 steps/s (collection: 0.791s, learning 0.733s)\n",
      "               Value function loss: 2387.5358\n",
      "                    Surrogate loss: -0.0025\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2546.68\n",
      "               Mean episode length: 490.99\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.16\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7536640\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 1298.07s\n",
      "                               ETA: 114.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 920/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5051 steps/s (collection: 0.885s, learning 0.736s)\n",
      "               Value function loss: 3049.6079\n",
      "                    Surrogate loss: -0.0029\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2563.41\n",
      "               Mean episode length: 495.56\n",
      "                 Mean success rate: 2.50\n",
      "                  Mean reward/step: 5.21\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7544832\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 1299.69s\n",
      "                               ETA: 112.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 921/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6546 steps/s (collection: 0.777s, learning 0.474s)\n",
      "               Value function loss: 3167.8344\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2544.27\n",
      "               Mean episode length: 491.48\n",
      "                 Mean success rate: 2.50\n",
      "                  Mean reward/step: 5.20\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7553024\n",
      "                    Iteration time: 1.25s\n",
      "                        Total time: 1300.95s\n",
      "                               ETA: 111.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 922/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5569 steps/s (collection: 0.927s, learning 0.544s)\n",
      "               Value function loss: 3357.7058\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2552.00\n",
      "               Mean episode length: 489.72\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.19\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7561216\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 1302.42s\n",
      "                               ETA: 110.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 923/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5976 steps/s (collection: 0.745s, learning 0.626s)\n",
      "               Value function loss: 3489.4828\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2547.85\n",
      "               Mean episode length: 489.72\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.33\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7569408\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 1303.79s\n",
      "                               ETA: 108.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 924/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4891 steps/s (collection: 0.856s, learning 0.818s)\n",
      "               Value function loss: 3379.0538\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2544.54\n",
      "               Mean episode length: 489.72\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.31\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7577600\n",
      "                    Iteration time: 1.67s\n",
      "                        Total time: 1305.46s\n",
      "                               ETA: 107.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 925/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6057 steps/s (collection: 0.788s, learning 0.564s)\n",
      "               Value function loss: 2063.1497\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2576.49\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.27\n",
      "       Mean episode length/episode: 30.91\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7585792\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 1306.81s\n",
      "                               ETA: 105.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 926/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5724 steps/s (collection: 0.740s, learning 0.691s)\n",
      "               Value function loss: 2990.4346\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2575.28\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.34\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7593984\n",
      "                    Iteration time: 1.43s\n",
      "                        Total time: 1308.25s\n",
      "                               ETA: 104.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 927/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5458 steps/s (collection: 0.840s, learning 0.661s)\n",
      "               Value function loss: 2473.4554\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2556.36\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.45\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7602176\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1309.75s\n",
      "                               ETA: 103.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 928/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5550 steps/s (collection: 0.745s, learning 0.730s)\n",
      "               Value function loss: 2424.1491\n",
      "                    Surrogate loss: -0.0024\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2552.80\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.36\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7610368\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 1311.22s\n",
      "                               ETA: 101.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 929/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5600 steps/s (collection: 0.806s, learning 0.657s)\n",
      "               Value function loss: 4599.2133\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2566.07\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.25\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7618560\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1312.69s\n",
      "                               ETA: 100.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 930/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5602 steps/s (collection: 0.843s, learning 0.619s)\n",
      "               Value function loss: 3089.0359\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2565.79\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 1.50\n",
      "                  Mean reward/step: 5.32\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7626752\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1314.15s\n",
      "                               ETA: 98.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 931/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5534 steps/s (collection: 0.865s, learning 0.615s)\n",
      "               Value function loss: 3278.5022\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2563.15\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 1.00\n",
      "                  Mean reward/step: 5.33\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7634944\n",
      "                    Iteration time: 1.48s\n",
      "                        Total time: 1315.63s\n",
      "                               ETA: 97.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 932/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5692 steps/s (collection: 0.874s, learning 0.565s)\n",
      "               Value function loss: 3088.8014\n",
      "                    Surrogate loss: -0.0019\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2580.19\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 0.50\n",
      "                  Mean reward/step: 5.23\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7643136\n",
      "                    Iteration time: 1.44s\n",
      "                        Total time: 1317.07s\n",
      "                               ETA: 96.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 933/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5151 steps/s (collection: 0.775s, learning 0.815s)\n",
      "               Value function loss: 3249.0865\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2588.96\n",
      "               Mean episode length: 492.30\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.19\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7651328\n",
      "                    Iteration time: 1.59s\n",
      "                        Total time: 1318.66s\n",
      "                               ETA: 94.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 934/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5955 steps/s (collection: 0.790s, learning 0.585s)\n",
      "               Value function loss: 3036.9335\n",
      "                    Surrogate loss: -0.0017\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2637.06\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 2.50\n",
      "                  Mean reward/step: 5.02\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7659520\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 1320.03s\n",
      "                               ETA: 93.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 935/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6404 steps/s (collection: 0.734s, learning 0.545s)\n",
      "               Value function loss: 2851.0242\n",
      "                    Surrogate loss: -0.0026\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2643.08\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 2.50\n",
      "                  Mean reward/step: 5.15\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7667712\n",
      "                    Iteration time: 1.28s\n",
      "                        Total time: 1321.31s\n",
      "                               ETA: 91.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 936/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5642 steps/s (collection: 0.877s, learning 0.574s)\n",
      "               Value function loss: 4060.6247\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2626.07\n",
      "               Mean episode length: 495.46\n",
      "                 Mean success rate: 2.50\n",
      "                  Mean reward/step: 5.11\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7675904\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1322.76s\n",
      "                               ETA: 90.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 937/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5125 steps/s (collection: 0.817s, learning 0.781s)\n",
      "               Value function loss: 4743.1394\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2595.48\n",
      "               Mean episode length: 491.43\n",
      "                 Mean success rate: 3.50\n",
      "                  Mean reward/step: 5.14\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7684096\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 1324.36s\n",
      "                               ETA: 88.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 938/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5329 steps/s (collection: 0.812s, learning 0.725s)\n",
      "               Value function loss: 2355.3687\n",
      "                    Surrogate loss: -0.0022\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2587.13\n",
      "               Mean episode length: 491.43\n",
      "                 Mean success rate: 4.00\n",
      "                  Mean reward/step: 5.13\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7692288\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 1325.90s\n",
      "                               ETA: 87.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 939/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5845 steps/s (collection: 0.777s, learning 0.625s)\n",
      "               Value function loss: 3383.1252\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2595.57\n",
      "               Mean episode length: 491.43\n",
      "                 Mean success rate: 4.00\n",
      "                  Mean reward/step: 5.11\n",
      "       Mean episode length/episode: 29.79\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7700480\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 1327.30s\n",
      "                               ETA: 86.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 940/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6290 steps/s (collection: 0.768s, learning 0.535s)\n",
      "               Value function loss: 2843.6716\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2560.03\n",
      "               Mean episode length: 487.43\n",
      "                 Mean success rate: 4.00\n",
      "                  Mean reward/step: 5.15\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7708672\n",
      "                    Iteration time: 1.30s\n",
      "                        Total time: 1328.60s\n",
      "                               ETA: 84.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 941/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5014 steps/s (collection: 0.859s, learning 0.775s)\n",
      "               Value function loss: 3019.2146\n",
      "                    Surrogate loss: -0.0019\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2557.17\n",
      "               Mean episode length: 487.43\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.14\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7716864\n",
      "                    Iteration time: 1.63s\n",
      "                        Total time: 1330.24s\n",
      "                               ETA: 83.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 942/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5979 steps/s (collection: 0.803s, learning 0.567s)\n",
      "               Value function loss: 2152.3226\n",
      "                    Surrogate loss: -0.0013\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2554.27\n",
      "               Mean episode length: 487.43\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.20\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7725056\n",
      "                    Iteration time: 1.37s\n",
      "                        Total time: 1331.61s\n",
      "                               ETA: 81.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 943/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5786 steps/s (collection: 0.821s, learning 0.595s)\n",
      "               Value function loss: 2335.0682\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2527.63\n",
      "               Mean episode length: 484.30\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.32\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7733248\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 1333.02s\n",
      "                               ETA: 80.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 944/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5846 steps/s (collection: 0.841s, learning 0.560s)\n",
      "               Value function loss: 3959.8134\n",
      "                    Surrogate loss: -0.0015\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2523.24\n",
      "               Mean episode length: 484.30\n",
      "                 Mean success rate: 4.50\n",
      "                  Mean reward/step: 5.37\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7741440\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 1334.42s\n",
      "                               ETA: 79.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 945/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5235 steps/s (collection: 0.861s, learning 0.704s)\n",
      "               Value function loss: 4408.9010\n",
      "                    Surrogate loss: -0.0015\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2472.65\n",
      "               Mean episode length: 481.33\n",
      "                 Mean success rate: 3.50\n",
      "                  Mean reward/step: 5.32\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7749632\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 1335.99s\n",
      "                               ETA: 77.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 946/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5321 steps/s (collection: 0.817s, learning 0.723s)\n",
      "               Value function loss: 2478.2519\n",
      "                    Surrogate loss: -0.0019\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2472.55\n",
      "               Mean episode length: 481.33\n",
      "                 Mean success rate: 3.00\n",
      "                  Mean reward/step: 5.30\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7757824\n",
      "                    Iteration time: 1.54s\n",
      "                        Total time: 1337.53s\n",
      "                               ETA: 76.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 947/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5450 steps/s (collection: 0.815s, learning 0.688s)\n",
      "               Value function loss: 2856.6699\n",
      "                    Surrogate loss: -0.0021\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2453.47\n",
      "               Mean episode length: 477.80\n",
      "                 Mean success rate: 3.00\n",
      "                  Mean reward/step: 5.25\n",
      "       Mean episode length/episode: 30.23\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7766016\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1339.03s\n",
      "                               ETA: 74.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 948/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5267 steps/s (collection: 0.838s, learning 0.717s)\n",
      "               Value function loss: 2739.9679\n",
      "                    Surrogate loss: -0.0015\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2450.77\n",
      "               Mean episode length: 479.13\n",
      "                 Mean success rate: 2.50\n",
      "                  Mean reward/step: 5.17\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7774208\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 1340.59s\n",
      "                               ETA: 73.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 949/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4997 steps/s (collection: 0.812s, learning 0.827s)\n",
      "               Value function loss: 4433.9298\n",
      "                    Surrogate loss: -0.0018\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2458.99\n",
      "               Mean episode length: 477.04\n",
      "                 Mean success rate: 2.50\n",
      "                  Mean reward/step: 5.25\n",
      "       Mean episode length/episode: 29.26\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7782400\n",
      "                    Iteration time: 1.64s\n",
      "                        Total time: 1342.22s\n",
      "                               ETA: 72.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 950/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5840 steps/s (collection: 0.808s, learning 0.595s)\n",
      "               Value function loss: 1521.6619\n",
      "                    Surrogate loss: -0.0010\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2464.57\n",
      "               Mean episode length: 477.04\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.32\n",
      "       Mean episode length/episode: 31.15\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7790592\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 1343.63s\n",
      "                               ETA: 70.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 951/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5076 steps/s (collection: 0.842s, learning 0.772s)\n",
      "               Value function loss: 3320.3959\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2457.53\n",
      "               Mean episode length: 477.04\n",
      "                 Mean success rate: 2.00\n",
      "                  Mean reward/step: 5.35\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7798784\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 1345.24s\n",
      "                               ETA: 69.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 952/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4785 steps/s (collection: 0.829s, learning 0.883s)\n",
      "               Value function loss: 3117.7464\n",
      "                    Surrogate loss: -0.0010\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2467.07\n",
      "               Mean episode length: 477.04\n",
      "                 Mean success rate: 2.50\n",
      "                  Mean reward/step: 5.22\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7806976\n",
      "                    Iteration time: 1.71s\n",
      "                        Total time: 1346.95s\n",
      "                               ETA: 67.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 953/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6179 steps/s (collection: 0.745s, learning 0.580s)\n",
      "               Value function loss: 4117.1091\n",
      "                    Surrogate loss: -0.0013\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2504.44\n",
      "               Mean episode length: 481.04\n",
      "                 Mean success rate: 2.50\n",
      "                  Mean reward/step: 5.21\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7815168\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 1348.28s\n",
      "                               ETA: 66.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 954/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5475 steps/s (collection: 0.790s, learning 0.706s)\n",
      "               Value function loss: 3181.9975\n",
      "                    Surrogate loss: -0.0015\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2487.79\n",
      "               Mean episode length: 481.04\n",
      "                 Mean success rate: 3.00\n",
      "                  Mean reward/step: 5.19\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7823360\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1349.77s\n",
      "                               ETA: 65.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 955/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4674 steps/s (collection: 0.825s, learning 0.927s)\n",
      "               Value function loss: 3106.7990\n",
      "                    Surrogate loss: -0.0020\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2518.95\n",
      "               Mean episode length: 484.17\n",
      "                 Mean success rate: 3.50\n",
      "                  Mean reward/step: 5.17\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7831552\n",
      "                    Iteration time: 1.75s\n",
      "                        Total time: 1351.53s\n",
      "                               ETA: 63.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 956/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5905 steps/s (collection: 0.751s, learning 0.636s)\n",
      "               Value function loss: 2918.5491\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2552.01\n",
      "               Mean episode length: 487.13\n",
      "                 Mean success rate: 4.00\n",
      "                  Mean reward/step: 5.24\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7839744\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 1352.91s\n",
      "                               ETA: 62.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 957/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5153 steps/s (collection: 0.777s, learning 0.813s)\n",
      "               Value function loss: 3326.9267\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2552.86\n",
      "               Mean episode length: 484.61\n",
      "                 Mean success rate: 4.00\n",
      "                  Mean reward/step: 5.23\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7847936\n",
      "                    Iteration time: 1.59s\n",
      "                        Total time: 1354.50s\n",
      "                               ETA: 60.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 958/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6059 steps/s (collection: 0.738s, learning 0.614s)\n",
      "               Value function loss: 2798.0916\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2554.92\n",
      "               Mean episode length: 484.61\n",
      "                 Mean success rate: 4.50\n",
      "                  Mean reward/step: 5.27\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7856128\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 1355.86s\n",
      "                               ETA: 59.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 959/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5370 steps/s (collection: 0.734s, learning 0.791s)\n",
      "               Value function loss: 3070.3622\n",
      "                    Surrogate loss: -0.0016\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2584.44\n",
      "               Mean episode length: 488.15\n",
      "                 Mean success rate: 4.50\n",
      "                  Mean reward/step: 5.43\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7864320\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 1357.38s\n",
      "                               ETA: 58.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 960/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6991 steps/s (collection: 0.721s, learning 0.451s)\n",
      "               Value function loss: 4550.4733\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2599.07\n",
      "               Mean episode length: 491.36\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.47\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7872512\n",
      "                    Iteration time: 1.17s\n",
      "                        Total time: 1358.55s\n",
      "                               ETA: 56.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 961/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5426 steps/s (collection: 0.812s, learning 0.698s)\n",
      "               Value function loss: 4500.7997\n",
      "                    Surrogate loss: -0.0006\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2642.30\n",
      "               Mean episode length: 497.48\n",
      "                 Mean success rate: 4.50\n",
      "                  Mean reward/step: 5.25\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7880704\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 1360.06s\n",
      "                               ETA: 55.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 962/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6159 steps/s (collection: 0.752s, learning 0.578s)\n",
      "               Value function loss: 2357.5789\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2641.79\n",
      "               Mean episode length: 497.48\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.18\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7888896\n",
      "                    Iteration time: 1.33s\n",
      "                        Total time: 1361.39s\n",
      "                               ETA: 53.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 963/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5250 steps/s (collection: 0.871s, learning 0.689s)\n",
      "               Value function loss: 2922.4766\n",
      "                    Surrogate loss: -0.0006\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2643.10\n",
      "               Mean episode length: 497.48\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.32\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7897088\n",
      "                    Iteration time: 1.56s\n",
      "                        Total time: 1362.95s\n",
      "                               ETA: 52.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 964/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5881 steps/s (collection: 0.856s, learning 0.537s)\n",
      "               Value function loss: 3442.9855\n",
      "                    Surrogate loss: -0.0008\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2636.45\n",
      "               Mean episode length: 497.48\n",
      "                 Mean success rate: 7.00\n",
      "                  Mean reward/step: 5.34\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7905280\n",
      "                    Iteration time: 1.39s\n",
      "                        Total time: 1364.35s\n",
      "                               ETA: 50.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 965/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5343 steps/s (collection: 0.869s, learning 0.664s)\n",
      "               Value function loss: 3598.9949\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2641.21\n",
      "               Mean episode length: 497.48\n",
      "                 Mean success rate: 6.00\n",
      "                  Mean reward/step: 5.21\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7913472\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 1365.88s\n",
      "                               ETA: 49.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 966/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5514 steps/s (collection: 0.829s, learning 0.656s)\n",
      "               Value function loss: 3087.0622\n",
      "                    Surrogate loss: -0.0014\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2650.58\n",
      "               Mean episode length: 497.48\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.30\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7921664\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 1367.36s\n",
      "                               ETA: 48.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 967/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5831 steps/s (collection: 0.809s, learning 0.596s)\n",
      "               Value function loss: 4102.7469\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2637.52\n",
      "               Mean episode length: 494.18\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.26\n",
      "       Mean episode length/episode: 29.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7929856\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 1368.77s\n",
      "                               ETA: 46.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 968/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4885 steps/s (collection: 0.855s, learning 0.821s)\n",
      "               Value function loss: 3516.8739\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2623.60\n",
      "               Mean episode length: 494.18\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.16\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7938048\n",
      "                    Iteration time: 1.68s\n",
      "                        Total time: 1370.45s\n",
      "                               ETA: 45.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 969/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5469 steps/s (collection: 0.825s, learning 0.673s)\n",
      "               Value function loss: 3309.6674\n",
      "                    Surrogate loss: -0.0009\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2629.84\n",
      "               Mean episode length: 496.70\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.17\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7946240\n",
      "                    Iteration time: 1.50s\n",
      "                        Total time: 1371.94s\n",
      "                               ETA: 43.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 970/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4900 steps/s (collection: 0.904s, learning 0.768s)\n",
      "               Value function loss: 4056.7208\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2616.17\n",
      "               Mean episode length: 496.70\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.25\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7954432\n",
      "                    Iteration time: 1.67s\n",
      "                        Total time: 1373.62s\n",
      "                               ETA: 42.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 971/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5349 steps/s (collection: 0.739s, learning 0.792s)\n",
      "               Value function loss: 3441.4806\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2618.43\n",
      "               Mean episode length: 496.70\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.18\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7962624\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 1375.15s\n",
      "                               ETA: 41.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 972/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5381 steps/s (collection: 0.807s, learning 0.715s)\n",
      "               Value function loss: 2306.7978\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2613.85\n",
      "               Mean episode length: 496.70\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.11\n",
      "       Mean episode length/episode: 30.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7970816\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 1376.67s\n",
      "                               ETA: 39.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 973/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5809 steps/s (collection: 0.855s, learning 0.556s)\n",
      "               Value function loss: 2779.5683\n",
      "                    Surrogate loss: -0.0006\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2612.24\n",
      "               Mean episode length: 496.70\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.30\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7979008\n",
      "                    Iteration time: 1.41s\n",
      "                        Total time: 1378.08s\n",
      "                               ETA: 38.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 974/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5768 steps/s (collection: 0.906s, learning 0.514s)\n",
      "               Value function loss: 2499.7091\n",
      "                    Surrogate loss: -0.0006\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2603.21\n",
      "               Mean episode length: 496.70\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.39\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7987200\n",
      "                    Iteration time: 1.42s\n",
      "                        Total time: 1379.50s\n",
      "                               ETA: 36.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 975/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4913 steps/s (collection: 0.749s, learning 0.918s)\n",
      "               Value function loss: 2633.1399\n",
      "                    Surrogate loss: -0.0007\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2603.09\n",
      "               Mean episode length: 496.70\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.45\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 7995392\n",
      "                    Iteration time: 1.67s\n",
      "                        Total time: 1381.17s\n",
      "                               ETA: 35.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 976/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6127 steps/s (collection: 0.836s, learning 0.501s)\n",
      "               Value function loss: 4998.6147\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2617.11\n",
      "               Mean episode length: 496.70\n",
      "                 Mean success rate: 4.00\n",
      "                  Mean reward/step: 5.41\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8003584\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 1382.50s\n",
      "                               ETA: 34.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 977/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5213 steps/s (collection: 0.908s, learning 0.663s)\n",
      "               Value function loss: 2843.2142\n",
      "                    Surrogate loss: -0.0005\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2605.42\n",
      "               Mean episode length: 496.70\n",
      "                 Mean success rate: 4.00\n",
      "                  Mean reward/step: 5.25\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8011776\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 1384.07s\n",
      "                               ETA: 32.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 978/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5953 steps/s (collection: 0.777s, learning 0.599s)\n",
      "               Value function loss: 3412.0833\n",
      "                    Surrogate loss: -0.0003\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2607.60\n",
      "               Mean episode length: 496.70\n",
      "                 Mean success rate: 4.50\n",
      "                  Mean reward/step: 5.26\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8019968\n",
      "                    Iteration time: 1.38s\n",
      "                        Total time: 1385.45s\n",
      "                               ETA: 31.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 979/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5858 steps/s (collection: 0.875s, learning 0.523s)\n",
      "               Value function loss: 3102.4513\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2617.24\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.31\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8028160\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 1386.85s\n",
      "                               ETA: 29.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 980/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5866 steps/s (collection: 0.748s, learning 0.648s)\n",
      "               Value function loss: 4034.7107\n",
      "                    Surrogate loss: -0.0004\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2634.12\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.39\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8036352\n",
      "                    Iteration time: 1.40s\n",
      "                        Total time: 1388.25s\n",
      "                               ETA: 28.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 981/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5079 steps/s (collection: 0.853s, learning 0.760s)\n",
      "               Value function loss: 1965.0050\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2633.58\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 6.00\n",
      "                  Mean reward/step: 5.41\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8044544\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 1389.86s\n",
      "                               ETA: 26.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 982/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5508 steps/s (collection: 0.821s, learning 0.667s)\n",
      "               Value function loss: 3055.2066\n",
      "                    Surrogate loss: -0.0003\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2631.81\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 6.00\n",
      "                  Mean reward/step: 5.44\n",
      "       Mean episode length/episode: 30.34\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8052736\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 1391.35s\n",
      "                               ETA: 25.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 983/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5646 steps/s (collection: 0.883s, learning 0.568s)\n",
      "               Value function loss: 4480.2053\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2648.47\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 6.00\n",
      "                  Mean reward/step: 5.28\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8060928\n",
      "                    Iteration time: 1.45s\n",
      "                        Total time: 1392.80s\n",
      "                               ETA: 24.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 984/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5348 steps/s (collection: 0.788s, learning 0.744s)\n",
      "               Value function loss: 4640.8800\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2647.32\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.32\n",
      "       Mean episode length/episode: 29.47\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8069120\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 1394.33s\n",
      "                               ETA: 22.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 985/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6107 steps/s (collection: 0.818s, learning 0.523s)\n",
      "               Value function loss: 2221.1588\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2644.75\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 5.50\n",
      "                  Mean reward/step: 5.17\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8077312\n",
      "                    Iteration time: 1.34s\n",
      "                        Total time: 1395.67s\n",
      "                               ETA: 21.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 986/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5113 steps/s (collection: 0.901s, learning 0.701s)\n",
      "               Value function loss: 3132.8023\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2650.56\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.12\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8085504\n",
      "                    Iteration time: 1.60s\n",
      "                        Total time: 1397.27s\n",
      "                               ETA: 19.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 987/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5504 steps/s (collection: 0.756s, learning 0.733s)\n",
      "               Value function loss: 3276.6616\n",
      "                    Surrogate loss: -0.0002\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2649.26\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 4.50\n",
      "                  Mean reward/step: 5.23\n",
      "       Mean episode length/episode: 30.12\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8093696\n",
      "                    Iteration time: 1.49s\n",
      "                        Total time: 1398.76s\n",
      "                               ETA: 18.4s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 988/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5612 steps/s (collection: 0.861s, learning 0.599s)\n",
      "               Value function loss: 3246.7482\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2645.93\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 4.00\n",
      "                  Mean reward/step: 5.38\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8101888\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1400.22s\n",
      "                               ETA: 17.0s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 989/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5365 steps/s (collection: 0.806s, learning 0.721s)\n",
      "               Value function loss: 2272.1291\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2651.67\n",
      "               Mean episode length: 500.00\n",
      "                 Mean success rate: 4.00\n",
      "                  Mean reward/step: 5.48\n",
      "       Mean episode length/episode: 30.80\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8110080\n",
      "                    Iteration time: 1.53s\n",
      "                        Total time: 1401.75s\n",
      "                               ETA: 15.6s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 990/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 4802 steps/s (collection: 0.830s, learning 0.876s)\n",
      "               Value function loss: 2517.4695\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2644.17\n",
      "               Mean episode length: 498.64\n",
      "                 Mean success rate: 4.50\n",
      "                  Mean reward/step: 5.49\n",
      "       Mean episode length/episode: 30.45\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8118272\n",
      "                    Iteration time: 1.71s\n",
      "                        Total time: 1403.45s\n",
      "                               ETA: 14.2s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 991/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5424 steps/s (collection: 0.804s, learning 0.707s)\n",
      "               Value function loss: 3585.4080\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2622.80\n",
      "               Mean episode length: 494.80\n",
      "                 Mean success rate: 6.00\n",
      "                  Mean reward/step: 5.33\n",
      "       Mean episode length/episode: 29.68\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8126464\n",
      "                    Iteration time: 1.51s\n",
      "                        Total time: 1404.96s\n",
      "                               ETA: 12.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 992/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5233 steps/s (collection: 0.919s, learning 0.646s)\n",
      "               Value function loss: 4131.2210\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2583.04\n",
      "               Mean episode length: 491.90\n",
      "                 Mean success rate: 4.50\n",
      "                  Mean reward/step: 5.21\n",
      "       Mean episode length/episode: 29.05\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8134656\n",
      "                    Iteration time: 1.57s\n",
      "                        Total time: 1406.53s\n",
      "                               ETA: 11.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 993/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5071 steps/s (collection: 0.785s, learning 0.830s)\n",
      "               Value function loss: 2849.3904\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2573.89\n",
      "               Mean episode length: 488.97\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.27\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8142848\n",
      "                    Iteration time: 1.62s\n",
      "                        Total time: 1408.14s\n",
      "                               ETA: 9.9s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 994/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5598 steps/s (collection: 0.822s, learning 0.641s)\n",
      "               Value function loss: 2735.5721\n",
      "                    Surrogate loss: -0.0001\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2577.00\n",
      "               Mean episode length: 488.97\n",
      "                 Mean success rate: 5.00\n",
      "                  Mean reward/step: 5.33\n",
      "       Mean episode length/episode: 30.57\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8151040\n",
      "                    Iteration time: 1.46s\n",
      "                        Total time: 1409.61s\n",
      "                               ETA: 8.5s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 995/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5378 steps/s (collection: 0.862s, learning 0.661s)\n",
      "               Value function loss: 3050.2246\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2543.22\n",
      "               Mean episode length: 484.53\n",
      "                 Mean success rate: 6.00\n",
      "                  Mean reward/step: 5.22\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8159232\n",
      "                    Iteration time: 1.52s\n",
      "                        Total time: 1411.13s\n",
      "                               ETA: 7.1s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 996/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5567 steps/s (collection: 0.800s, learning 0.672s)\n",
      "               Value function loss: 4173.7837\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2558.21\n",
      "               Mean episode length: 484.53\n",
      "                 Mean success rate: 6.50\n",
      "                  Mean reward/step: 5.17\n",
      "       Mean episode length/episode: 29.36\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8167424\n",
      "                    Iteration time: 1.47s\n",
      "                        Total time: 1412.60s\n",
      "                               ETA: 5.7s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 997/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 6086 steps/s (collection: 0.821s, learning 0.525s)\n",
      "               Value function loss: 1944.2568\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2553.55\n",
      "               Mean episode length: 484.53\n",
      "                 Mean success rate: 7.00\n",
      "                  Mean reward/step: 5.39\n",
      "       Mean episode length/episode: 31.27\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8175616\n",
      "                    Iteration time: 1.35s\n",
      "                        Total time: 1413.95s\n",
      "                               ETA: 4.3s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 998/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5090 steps/s (collection: 0.885s, learning 0.724s)\n",
      "               Value function loss: 3787.2465\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2563.51\n",
      "               Mean episode length: 484.53\n",
      "                 Mean success rate: 6.50\n",
      "                  Mean reward/step: 5.32\n",
      "       Mean episode length/episode: 29.90\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8183808\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 1415.56s\n",
      "                               ETA: 2.8s\n",
      "\n",
      "################################################################################\n",
      "                     \u001b[1m Learning iteration 999/1000 \u001b[0m                      \n",
      "\n",
      "                       Computation: 5099 steps/s (collection: 0.753s, learning 0.853s)\n",
      "               Value function loss: 3100.1386\n",
      "                    Surrogate loss: -0.0000\n",
      "             Mean action noise std: 1.06\n",
      "                       Mean reward: 2565.61\n",
      "               Mean episode length: 484.53\n",
      "                 Mean success rate: 7.50\n",
      "                  Mean reward/step: 5.36\n",
      "       Mean episode length/episode: 30.01\n",
      "--------------------------------------------------------------------------------\n",
      "                   Total timesteps: 8192000\n",
      "                    Iteration time: 1.61s\n",
      "                        Total time: 1417.16s\n",
      "                               ETA: 1.4s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform training\n",
    "ppo = process_ppo(env, cfg, cfg_dict, cfg.logdir, cfg.cptdir)\n",
    "ppo.run(num_learning_iterations=cfg.train.learn.max_iterations, log_interval=cfg.train.learn.save_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "num_resume=cfg_dict['train']['learn']['max_iterations']\n",
    "print(num_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python tools/train_ppo.py test=True headless=False logdir=./ckpts/FrankaCabinet resume=1000\n"
     ]
    }
   ],
   "source": [
    "# run the test code in terminal as the output of the following line\n",
    "print(f\"python tools/train_ppo.py test=True headless=False logdir={cfg_dict['logdir']} resume={num_resume}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
